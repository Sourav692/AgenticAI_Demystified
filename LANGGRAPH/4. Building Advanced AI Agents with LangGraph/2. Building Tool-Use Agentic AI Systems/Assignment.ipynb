{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f761bf4c-1320-4093-8a3a-37735d87cf64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.3.14\n",
    "!pip install langchain-openai==0.3.0\n",
    "!pip install langchain-community==0.3.14\n",
    "!pip install langgraph==0.2.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d49f3d-9f85-4a48-8fb4-b316e26f5534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e39134c-a42a-46a3-b3e1-fb020808adfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f4d66e-5468-4149-ae04-0d83f0ea6bda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6409993-9bc2-4bf1-9820-0090cc154009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explanation: LangGraph Mathematical Agent\n",
    "\n",
    "The agent is built using LangGraph and LangChain to combine a large language model (LLM) with custom mathematical tools.\n",
    "\n",
    "- **LLM Integration**: The agent uses OpenAI's GPT-4o model via `ChatOpenAI`. This LLM is responsible for understanding user queries and generating responses.\n",
    "- **Tool Definition**: Four mathematical functions (`plus`, `substract`, `multiply`, `divide`) are defined and decorated with `@tool`. These allow the agent to perform addition, subtraction, multiplication, and division.\n",
    "- **System Prompt**: A system prompt instructs the LLM to use these tools for math queries, ensuring that mathematical operations are handled by the dedicated functions rather than the LLM's internal logic.\n",
    "- **Graph Construction**: The agent is structured as a state graph:\n",
    "  - The conversation state is managed and trimmed to avoid token overflow.\n",
    "  - The graph has nodes for the chatbot (LLM) and tool execution.\n",
    "  - Conditional logic routes requests: if a tool is needed, the agent calls the appropriate function; otherwise, it ends the conversation.\n",
    "- **Workflow**:\n",
    "  1. The user sends a message.\n",
    "  2. The LLM interprets the query and decides if a tool call is required.\n",
    "  3. If so, the relevant tool is invoked and the result is returned.\n",
    "  4. The agent continues the conversation or ends it based on the context.\n",
    "\n",
    "This design ensures that general queries are answered by the LLM, while mathematical operations are handled by reliable, predefined functions, providing a seamless conversational experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0e3735-5133-4908-86b5-5882910aa04a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain_core.tools import tool\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, RemoveMessage\n",
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "@tool\n",
    "def plus(num1: int, num2: int):\n",
    "    \"\"\"Add 2 Numbers\"\"\"\n",
    "    results = num1 + num2\n",
    "    return results\n",
    "\n",
    "@tool\n",
    "def substract(num1: int, num2: int):\n",
    "    \"\"\"Substract 2 Numbers\"\"\"\n",
    "    results = num1 - num2\n",
    "    return results\n",
    "\n",
    "@tool\n",
    "def multiply(num1: int, num2: int):\n",
    "    \"\"\"Add 2 Numbers\"\"\"\n",
    "    results = num1 * num2\n",
    "    return results\n",
    "\n",
    "@tool\n",
    "def divide(num1: int, num2: int):\n",
    "    \"\"\"Divide 2 numbers, with error handling division by zero\"\"\"\n",
    "    if num2 == 0:\n",
    "        return \"Error: Division by zero\"\n",
    "    results = num1 / num2\n",
    "    return results\n",
    "\n",
    "System_Prompt = \"\"\"You are a helpful assistant that answers general questions using the LLM, and when asked to perform mathematical operations (addition, subtraction, multiplication, and division), it calls four predefined functions (plus, divide, substract, multiply) for answering. The agent should handle both general and math-related queries seamlessly. For any math related query please call the tools, Dont use LLM inbuild capability to answer any math related query.\n",
    "\n",
    "Example:\n",
    "Question: What is 5 plus 3?\n",
    "Use plus tool.\n",
    "\n",
    "Question: How much is 8 divided by 2?\n",
    "Use Divide tool.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Define the state of the graph, which holds the conversation messages\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# Initialize the graph builder with the defined state\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# List of tools the agent can use (e.g., fetching stock data, searching the web)\n",
    "tools = [\n",
    "    plus,\n",
    "    substract,\n",
    "    multiply,\n",
    "    divide\n",
    "]\n",
    "\n",
    "# Initialize the LLM (GPT-4) and bind the tools to it\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# System message to guide the agent's behavior\n",
    "SYS_MSG = SystemMessage(content=System_Prompt)\n",
    "\n",
    "# Define the chatbot node, which processes user input and generates responses\n",
    "def chatbot(state: State):\n",
    "    # Trim messages to avoid exceeding token limits\n",
    "    messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        max_tokens=127000,\n",
    "        strategy=\"last\", # keep last 127K tokens in messages\n",
    "        token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "        include_system=True, # keep system message always\n",
    "        allow_partial=True, # trim messages to partial content if needed\n",
    "\n",
    "    )\n",
    "    # Invoke the LLM with the system message and trimmed conversation history\n",
    "    return {\"messages\": [llm_with_tools.invoke([SYS_MSG] + messages)]}\n",
    "\n",
    "# Add the chatbot node to the graph\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Add a node for executing tools (e.g., fetching data, searching the web)\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add conditional edges: the chatbot decides whether to use tools or end the conversation\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    "    ['tools', END]\n",
    ")\n",
    "\n",
    "# After using a tool, return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "# Set the chatbot as the entry point of the graph\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "# Compile the graph into a runnable agent\n",
    "agent = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6905f740-618e-4c03-a80e-b695715c6346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "Image(agent.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2311791f-342a-45dc-8abd-36948487bd04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "For th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f08211-d171-46bd-ae5f-44e5d215d83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def call_agent(agent, prompt, user_config={\"configurable\": {\"thread_id\": \"any\"}}):\n",
    "    events = agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
    "        user_config,\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "\n",
    "    for event in events:\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    print()\n",
    "    print('Final Response:\\n')\n",
    "    display(Markdown(event[\"messages\"][-1].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec93a5d7-fbbc-4a4d-9491-c7f4f1a57f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "call_agent(agent, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f3f820-5b5e-4cea-a643-c037b98c2537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cac36e59-b13b-4e44-b170-68dc3b06eb96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Alternate Approach. Using should_continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c19050c1-0372-42ac-82cd-331eab07fb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# LangGraph Mathematical Agent\n",
    "# A conversational agent that handles general questions using an LLM and\n",
    "# performs mathematical operations using predefined functions.\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import operator\n",
    "# from typing import Annotated, Literal, TypedDict\n",
    "# from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "# from langchain_core.tools import tool\n",
    "# from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "# from langgraph.prebuilt import ToolNode\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# # Choose your LLM provider - uncomment the one you want to use\n",
    "# # Option 1: Using Groq API\n",
    "# # from langchain_groq import ChatGroq\n",
    "# # llm = ChatGroq(\n",
    "# #     model=\"llama-3.1-70b-versatile\",\n",
    "# #     api_key=os.getenv(\"GROQ_API_KEY\"),  # Set your API key as environment variable\n",
    "# #     temperature=0\n",
    "# # )\n",
    "\n",
    "# # Option 2: Using Google Gemini API\n",
    "# # from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# # llm = ChatGoogleGenerativeAI(\n",
    "# #     model=\"gemini-1.5-pro\",\n",
    "# #     google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "# #     temperature=0\n",
    "# # )\n",
    "\n",
    "# # Option 3: Using Ollama (Local LLM)\n",
    "# # from langchain_ollama import ChatOllama\n",
    "# # llm = ChatOllama(\n",
    "# #     model=\"llama3.1\",\n",
    "# #     temperature=0\n",
    "# # )\n",
    "\n",
    "# # Option 4: Using OpenAI\n",
    "# # from langchain_openai import ChatOpenAI\n",
    "# # llm = ChatOpenAI(\n",
    "# #     model=\"gpt-4o-mini\",\n",
    "# #     api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "# #     temperature=0\n",
    "# # )\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# # ============================================================================\n",
    "# # MATHEMATICAL OPERATION TOOLS\n",
    "# # ============================================================================\n",
    "\n",
    "# @tool\n",
    "# def plus(a: float, b: float) -> float:\n",
    "#     \"\"\"Add two numbers together.\n",
    "    \n",
    "#     Args:\n",
    "#         a: The first number\n",
    "#         b: The second number\n",
    "        \n",
    "#     Returns:\n",
    "#         The sum of a and b\n",
    "#     \"\"\"\n",
    "#     result = a + b\n",
    "#     print(f\"[TOOL CALL] plus({a}, {b}) = {result}\")\n",
    "#     return result\n",
    "\n",
    "\n",
    "# @tool\n",
    "# def subtract(a: float, b: float) -> float:\n",
    "#     \"\"\"Subtract the second number from the first number.\n",
    "    \n",
    "#     Args:\n",
    "#         a: The number to subtract from\n",
    "#         b: The number to subtract\n",
    "        \n",
    "#     Returns:\n",
    "#         The difference between a and b\n",
    "#     \"\"\"\n",
    "#     result = a - b\n",
    "#     print(f\"[TOOL CALL] subtract({a}, {b}) = {result}\")\n",
    "#     return result\n",
    "\n",
    "\n",
    "# @tool\n",
    "# def multiply(a: float, b: float) -> float:\n",
    "#     \"\"\"Multiply two numbers together.\n",
    "    \n",
    "#     Args:\n",
    "#         a: The first number\n",
    "#         b: The second number\n",
    "        \n",
    "#     Returns:\n",
    "#         The product of a and b\n",
    "#     \"\"\"\n",
    "#     result = a * b\n",
    "#     print(f\"[TOOL CALL] multiply({a}, {b}) = {result}\")\n",
    "#     return result\n",
    "\n",
    "\n",
    "# @tool\n",
    "# def divide(a: float, b: float) -> float:\n",
    "#     \"\"\"Divide the first number by the second number.\n",
    "    \n",
    "#     Args:\n",
    "#         a: The dividend (number to be divided)\n",
    "#         b: The divisor (number to divide by)\n",
    "        \n",
    "#     Returns:\n",
    "#         The quotient of a divided by b\n",
    "        \n",
    "#     Raises:\n",
    "#         ValueError: If attempting to divide by zero\n",
    "#     \"\"\"\n",
    "#     if b == 0:\n",
    "#         error_msg = \"Error: Division by zero is not allowed\"\n",
    "#         print(f\"[TOOL CALL] divide({a}, {b}) - {error_msg}\")\n",
    "#         raise ValueError(error_msg)\n",
    "    \n",
    "#     result = a / b\n",
    "#     print(f\"[TOOL CALL] divide({a}, {b}) = {result}\")\n",
    "#     return result\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # AGENT STATE AND GRAPH SETUP\n",
    "# # ============================================================================\n",
    "\n",
    "# # Define the tools available to the agent\n",
    "# tools = [plus, subtract, multiply, divide]\n",
    "\n",
    "# # Bind tools to the LLM\n",
    "# llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "# def should_continue(state: MessagesState) -> Literal[\"tools\", \"end\"]:\n",
    "#     \"\"\"Determine whether to continue to tools or end the conversation.\n",
    "    \n",
    "#     This function checks if the last message from the LLM contains tool calls.\n",
    "#     If it does, we route to the tools node. Otherwise, we end the conversation.\n",
    "    \n",
    "#     Args:\n",
    "#         state: The current state containing all messages\n",
    "        \n",
    "#     Returns:\n",
    "#         \"tools\" if tool calls are present, \"end\" otherwise\n",
    "#     \"\"\"\n",
    "#     messages = state[\"messages\"]\n",
    "#     last_message = messages[-1]\n",
    "    \n",
    "#     # Check if the last message has tool calls\n",
    "#     if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "#         print(f\"[ROUTING] Tool calls detected: {len(last_message.tool_calls)} call(s)\")\n",
    "#         return \"tools\"\n",
    "    \n",
    "#     print(\"[ROUTING] No tool calls - ending conversation\")\n",
    "#     return \"end\"\n",
    "\n",
    "\n",
    "# def call_model(state: MessagesState) -> dict:\n",
    "#     \"\"\"Call the LLM with the current state.\n",
    "    \n",
    "#     This function invokes the language model with the conversation history\n",
    "#     and returns the response, which may include tool calls for mathematical operations.\n",
    "    \n",
    "#     Args:\n",
    "#         state: The current state containing all messages\n",
    "        \n",
    "#     Returns:\n",
    "#         A dictionary with the new message to add to state\n",
    "#     \"\"\"\n",
    "#     messages = state[\"messages\"]\n",
    "#     print(f\"[LLM] Processing {len(messages)} message(s)\")\n",
    "    \n",
    "#     response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "#     # Return the response to be added to the state\n",
    "#     return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # BUILD THE GRAPH\n",
    "# # ============================================================================\n",
    "\n",
    "# def create_agent_graph():\n",
    "#     \"\"\"Create and compile the LangGraph agent.\n",
    "    \n",
    "#     The graph structure:\n",
    "#     1. START -> agent: Initial entry point\n",
    "#     2. agent -> should_continue: Decision point\n",
    "#     3. should_continue -> tools: If tool calls are needed\n",
    "#     4. should_continue -> END: If no tool calls needed\n",
    "#     5. tools -> agent: Return to agent after tool execution\n",
    "    \n",
    "#     Returns:\n",
    "#         A compiled StateGraph ready for execution\n",
    "#     \"\"\"\n",
    "#     # Initialize the graph with MessagesState\n",
    "#     workflow = StateGraph(MessagesState)\n",
    "    \n",
    "#     # Add nodes to the graph\n",
    "#     workflow.add_node(\"agent\", call_model)\n",
    "#     workflow.add_node(\"tools\", ToolNode(tools))\n",
    "    \n",
    "#     # Set the entry point\n",
    "#     workflow.add_edge(START, \"agent\")\n",
    "    \n",
    "#     # Add conditional edges\n",
    "#     workflow.add_conditional_edges(\n",
    "#         \"agent\",\n",
    "#         should_continue,\n",
    "#         {\n",
    "#             \"tools\": \"tools\",\n",
    "#             \"end\": END\n",
    "#         }\n",
    "#     )\n",
    "    \n",
    "#     # Add edge from tools back to agent\n",
    "#     workflow.add_edge(\"tools\", \"agent\")\n",
    "    \n",
    "#     # Compile the graph with memory\n",
    "#     memory = MemorySaver()\n",
    "#     app = workflow.compile(checkpointer=memory)\n",
    "    \n",
    "#     return app\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # MAIN EXECUTION\n",
    "# # ============================================================================\n",
    "\n",
    "# def run_agent(query: str, app, thread_id: str = \"default\"):\n",
    "#     \"\"\"Run the agent with a given query.\n",
    "    \n",
    "#     Args:\n",
    "#         query: The user's question or request\n",
    "#         app: The compiled LangGraph application\n",
    "#         thread_id: Unique identifier for the conversation thread\n",
    "#     \"\"\"\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(f\"USER QUERY: {query}\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     # Configure the thread\n",
    "#     config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "#     # Create the input message\n",
    "#     input_messages = [HumanMessage(content=query)]\n",
    "    \n",
    "#     # Stream the response\n",
    "#     for event in app.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "#         # Get the last message in the current state\n",
    "#         last_message = event[\"messages\"][-1]\n",
    "        \n",
    "#         # Only print AI messages (final responses)\n",
    "#         if isinstance(last_message, AIMessage) and not hasattr(last_message, \"tool_calls\"):\n",
    "#             print(\"\\n\" + \"-\"*80)\n",
    "#             print(\"AGENT RESPONSE:\")\n",
    "#             print(\"-\"*80)\n",
    "#             print(last_message.content)\n",
    "#             print(\"-\"*80 + \"\\n\")\n",
    "#         elif isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "#             print(f\"[AGENT] Preparing to call {len(last_message.tool_calls)} tool(s)\")\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main function to demonstrate the mathematical agent.\"\"\"\n",
    "    \n",
    "#     print(\"\"\"\n",
    "#     ╔══════════════════════════════════════════════════════════════════════╗\n",
    "#     ║         LangGraph Mathematical Agent - Demonstration                 ║\n",
    "#     ║                                                                      ║\n",
    "#     ║  This agent can:                                                     ║\n",
    "#     ║  1. Answer general questions using an LLM                           ║\n",
    "#     ║  2. Perform mathematical operations using specialized tools         ║\n",
    "#     ║     - Addition (plus)                                               ║\n",
    "#     ║     - Subtraction (subtract)                                        ║\n",
    "#     ║     - Multiplication (multiply)                                     ║\n",
    "#     ║     - Division (divide)                                             ║\n",
    "#     ╚══════════════════════════════════════════════════════════════════════╝\n",
    "#     \"\"\")\n",
    "    \n",
    "#     # Create the agent\n",
    "#     print(\"Initializing agent...\")\n",
    "#     app = create_agent_graph()\n",
    "#     print(\"Agent initialized successfully!\\n\")\n",
    "    \n",
    "#     # Test cases demonstrating different capabilities\n",
    "#     test_queries = [\n",
    "#         # Mathematical operations\n",
    "#         \"What is 5 plus 3?\",\n",
    "#         \"Calculate 100 minus 45\",\n",
    "#         \"How much is 7 multiplied by 8?\",\n",
    "#         \"What's 144 divided by 12?\",\n",
    "        \n",
    "#         # Complex mathematical queries\n",
    "#         \"I need to add 25.5 and 30.75\",\n",
    "#         \"Can you multiply 15 by 4 for me?\",\n",
    "        \n",
    "#         # Edge case - division by zero\n",
    "#         \"What happens if I divide 10 by 0?\",\n",
    "        \n",
    "#         # General knowledge questions\n",
    "#         \"What is the capital of France?\",\n",
    "#         \"Explain what artificial intelligence is in simple terms\",\n",
    "#         \"Who wrote Romeo and Juliet?\",\n",
    "        \n",
    "#         # Mixed context\n",
    "#         \"I'm planning a dinner for 8 people. If each pizza serves 4 people, how many pizzas do I need?\"\n",
    "#     ]\n",
    "    \n",
    "#     # Run each test query\n",
    "#     for i, query in enumerate(test_queries, 1):\n",
    "#         run_agent(query, app, thread_id=f\"test_{i}\")\n",
    "        \n",
    "#         # Add a small pause between queries for readability\n",
    "#         if i < len(test_queries):\n",
    "#             input(\"\\nPress Enter to continue to next query...\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"DEMONSTRATION COMPLETE\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     # Interactive mode\n",
    "#     print(\"\\n\\nEntering interactive mode. Type 'quit' or 'exit' to end.\\n\")\n",
    "    \n",
    "#     while True:\n",
    "#         try:\n",
    "#             user_input = input(\"You: \").strip()\n",
    "            \n",
    "#             if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "#                 print(\"Goodbye!\")\n",
    "#                 break\n",
    "            \n",
    "#             if not user_input:\n",
    "#                 continue\n",
    "            \n",
    "#             run_agent(user_input, app, thread_id=\"interactive\")\n",
    "            \n",
    "#         except KeyboardInterrupt:\n",
    "#             print(\"\\n\\nGoodbye!\")\n",
    "#             break\n",
    "#         except Exception as e:\n",
    "#             print(f\"\\nError: {str(e)}\\n\")\n",
    "\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3336ad-5cf7-4b48-af34-477dd214b332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# query = \"What is 5 plus 3\"\n",
    "# app = create_agent_graph()\n",
    "# run_agent(query, app, thread_id=f\"u01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c75a41-ce16-472f-a8b8-00d38f05c467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f17dca6d-874e-4a32-8761-5de4217c6fa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
