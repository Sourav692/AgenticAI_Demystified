{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97636eb7-680e-40c9-a977-c0cd170cfc27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "f4169bfb-769a-4db3-833e-c827f19024b2"
   },
   "source": [
    "# Dynamic Parallel Execution with  Map-Reduce using Send in LangGraph\n",
    "\n",
    "\n",
    "Map-reduce operations are essential for efficient task decomposition and parallel processing.\n",
    "\n",
    "It has two phases:\n",
    "\n",
    "(1) `Map` - Break a task into smaller sub-tasks, processing each sub-task in parallel.\n",
    "\n",
    "(2) `Reduce` - Aggregate the results across all of the completed, parallelized sub-tasks.\n",
    "\n",
    "We will design a system that will do two things:\n",
    "\n",
    "(1) `Map` - Create a set of questions about a topic. Then answer them in parallel (using `Send(...)`)\n",
    "\n",
    "(2) `Reduce` - Compile a comprehensive report based on the QAs on the topic.\n",
    "\n",
    "![](https://i.imgur.com/SN7KifO.png)\n",
    "\n",
    "\n",
    "LangGraph's Map-Reduce pattern enables efficient task decomposition and parallel processing, enhancing performance in complex workflows. The Send function plays a pivotal role in this mechanism.\n",
    "\n",
    "Map-Reduce in LangGraph:\n",
    "- Task Decomposition: Breaks down a large task into smaller, manageable sub-tasks (planning or complex question decomposition)\n",
    "- Parallel Processing: Executes sub-tasks concurrently, significantly reducing overall processing time.\n",
    "- Result Aggregation: Combines outcomes from all sub-tasks to form a comprehensive response.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4592874-d7ce-430d-9670-51e6db5bdb9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9hEI3WL328vZ"
   },
   "source": [
    "## Install OpenAI, LangGraph and LangChain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fac2355d-fdf1-4c19-aa39-b7005c443cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "618eab5c-4ef7-4273-8e0b-a9c847897ed7",
    "outputId": "bbafb70d-1639-4d23-b7da-0a1798358d6a"
   },
   "outputs": [],
   "source": [
    "!pip install langchain==0.3.14\n",
    "!pip install langchain-openai==0.3.0\n",
    "!pip install langchain-community==0.3.14\n",
    "!pip install langgraph==0.2.64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56956199-c895-4a3f-abc2-18ba3ad485c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "H9c37cLnSrbg"
   },
   "source": [
    "## Enter Open AI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b0f1f33-45eb-4232-aefc-cce517f02461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cv3JzCEx_PAd",
    "outputId": "e57fac95-ca07-4f63-8b7a-efe97036e33e"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60dcb12b-fb6b-4f9d-8535-aefa5e7c0b52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1T0s0um5Svfa"
   },
   "source": [
    "## Setup Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13eb84c7-6766-4545-b2ce-8ea1f1b2eeb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "x1YSuHNF_lbh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d4e3b8a-e9eb-4d5d-82ee-79ceb69cf792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "E1OZCIkyYYzt"
   },
   "source": [
    "## Define Agent State Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aadae43-7641-4304-936f-0a2c5d6ad357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "o7EnucYkRb6f"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel\n",
    "import operator\n",
    "from typing import Annotated\n",
    "\n",
    "# Define state\n",
    "class Questions(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class Report(BaseModel):\n",
    "    report: str\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    topic: str\n",
    "    questions: list\n",
    "    answers: Annotated[list, operator.add]\n",
    "    report: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e38549bf-1b90-43b9-b448-b101ffce5984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8UUj4MMIYfuH"
   },
   "source": [
    "## Define Agent Node Functions\n",
    "\n",
    "Role of the Send Function:\n",
    "- Dynamic Task Distribution: Utilizes the Send function to dispatch different states to multiple instances of a node, facilitating parallel execution.\n",
    "- Flexible Workflow Management: Very useful when you do not have a fixed number of static edges to parallelize like in router agent. A simple example would be generating a random number of questions or steps to solve a problem and parallelizing the generation process for each of those questions or steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dcdc3e9-19ad-4a86-902a-9cb8abacb3b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hZ-LxJ5YRb82"
   },
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Node to generate questions\n",
    "def generate_questions(state: OverallState):\n",
    "    # sometimes gpt-4o just generates 5 questions always so feel free to play around with the following prompt\n",
    "    # or you can also use gpt-4o-mini or other LLMs or just randomly select a subselt of questions from the response\n",
    "    # just to demonstrate and see how Send works with a variable number of questions\n",
    "    questions_prompt = \"\"\"Generate a list of concise sub-questions related to this overall topic: {topic}\n",
    "                          which would help build a good report.\n",
    "                          Follow these rules for question generation:\n",
    "                            - Do not create very long questions.\n",
    "                            - Number of questions should always be 3 for simple topics (Birds, Animals, AI)\n",
    "                              and 5 for more complex topics (Outlook for ..., Impact of ...)\n",
    "                       \"\"\"\n",
    "    prompt = questions_prompt.format(topic=state[\"topic\"])\n",
    "    response = llm.with_structured_output(Questions).invoke(prompt)\n",
    "    return {\"questions\": response.questions}\n",
    "\n",
    "# Node to generate answer to one question\n",
    "def generate_answer(state: Answer):\n",
    "    answer_prompt = \"\"\"Generate the answer about {question}.\"\"\"\n",
    "    prompt = answer_prompt.format(question=state[\"question\"])\n",
    "    response = llm.with_structured_output(Answer).invoke(prompt)\n",
    "    return {\"answers\": [{\"question\": state[\"question\"], \"answer\": response.answer}]}\n",
    "\n",
    "# Node to continue to answers\n",
    "def continue_to_answers(state: OverallState):\n",
    "    return [Send(\"generate_answer\", {\"question\": q}) for q in state[\"questions\"]] # does the parallel execution\n",
    "\n",
    "# Node to compile the report\n",
    "def compile_report(state: OverallState):\n",
    "    q_and_a = \"\\n\\n\".join(\n",
    "        [f\"Q: {qa['question']}\\nA: {qa['answer']}\" for qa in state[\"answers\"]]\n",
    "    )\n",
    "    report_prompt = \"\"\"Below are a bunch of questions and answers about topic:\n",
    "                       {topic}.\n",
    "                       Generate a detailed report from this about the topic.\n",
    "                       {q_and_a}\"\"\"\n",
    "    prompt = report_prompt.format(topic=state[\"topic\"], q_and_a=q_and_a)\n",
    "    response = llm.with_structured_output(Report).invoke(prompt)\n",
    "    return {\"report\": response.report}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e89489f-9124-469c-a858-6e6378edce2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yuRbxOMMYjMS"
   },
   "source": [
    "## Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "112e2a0c-0bef-429c-bbef-1da7313ed179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1a1p_mpwRb_C"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Compile the graph\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_questions\", generate_questions)\n",
    "graph.add_node(\"generate_answer\", generate_answer)\n",
    "graph.add_node(\"compile_report\", compile_report)\n",
    "\n",
    "graph.add_edge(START, \"generate_questions\")\n",
    "graph.add_conditional_edges(\"generate_questions\",\n",
    "                            continue_to_answers,\n",
    "                            [\"generate_answer\"])\n",
    "graph.add_edge(\"generate_answer\", \"compile_report\")\n",
    "graph.add_edge(\"compile_report\", END)\n",
    "\n",
    "# Compile the app\n",
    "agent = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1ce249f-f602-4bf5-b773-13c23681e188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "Te1lHnkqRcBH",
    "outputId": "e563c981-b295-43ec-bc47-1673b57b960b"
   },
   "outputs": [],
   "source": [
    "# Display the graph\n",
    "from IPython.display import display, Image\n",
    "Image(agent.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0caeeca1-ccd0-47d6-8997-ffe5ec258874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sUfJrGBAYv64"
   },
   "source": [
    "## Run and Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96ebdeec-efc5-416e-bd24-07e134a2a65f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UYB9crmZRcDD",
    "outputId": "69be16e4-f61d-4aea-b924-e6d9b80267a4"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "for state in agent.stream({\"topic\": \"Artificial Intelligence\"}):\n",
    "    print(state)\n",
    "    if 'compile_report' in state:\n",
    "        display(Markdown(state['compile_report']['report']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "177993c9-a82f-4327-9c83-84fa235fb498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kkGDMBRTRcFx",
    "outputId": "8ed5f2c1-ab44-41c5-ecde-6ae0091d0cf8"
   },
   "outputs": [],
   "source": [
    "for state in agent.stream({\"topic\": \"Animals\"}):\n",
    "    print(state)\n",
    "    if 'compile_report' in state:\n",
    "        display(Markdown(state['compile_report']['report']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f522d663-a209-451f-9cc4-44a5683c6b4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lbaRJSh9RcH_",
    "outputId": "1dc10cfc-3679-4f25-82ef-8d0423ed4eea"
   },
   "outputs": [],
   "source": [
    "for state in agent.stream({\"topic\": \"Impact of AI on jobs\"}):\n",
    "    print(state)\n",
    "    if 'compile_report' in state:\n",
    "        display(Markdown(state['compile_report']['report']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "025e1551-30de-48e3-9b31-531ef4ce4cbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ddra7VuHbiwn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. The_Send_construct_in_LangGraph_Dynamic_Parallel_Execution_with_Map_Reduce",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
