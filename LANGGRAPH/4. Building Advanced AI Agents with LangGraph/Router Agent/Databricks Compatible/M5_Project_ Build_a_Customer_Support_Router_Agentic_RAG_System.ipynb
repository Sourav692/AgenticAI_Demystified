{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf2674d-7fa2-4f8a-836c-f778b3503fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Databricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04bd62c9-4f3d-438c-8a28-6fba71c6e984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NffLMDtsJFsY"
   },
   "source": [
    "# Build a Customer Support Router Agentic RAG System\n",
    "\n",
    "In this project, we will leverage the power of AI Agents and RAG Systems to build an intelligent Router Agentic RAG System to handle customer support queries using a custom knowledgebase.\n",
    "\n",
    "![](https://i.imgur.com/bLCdxCI.png)\n",
    "\n",
    "### Intelligent Router Agentic RAG System\n",
    "\n",
    "This project focuses on building an **Intelligent Router Agentic RAG System** that combines intelligent query analysis, sentiment detection, and dynamic routing with Retrieval-Augmented Generation (RAG) to handle diverse user inquiries efficiently. The workflow includes the following components:\n",
    "\n",
    "1. **Query Categorization and Sentiment Analysis**:\n",
    "   - The system uses **OpenAI GPT-4o** to analyze the user's query and determine:\n",
    "     - **Query Category**: Identifies the type of problem, such as billing, technical issues, or general queries.\n",
    "     - **User Sentiment**: Evaluates the user's sentiment (positive, neutral, or negative) to determine if escalation is needed.\n",
    "\n",
    "2. **Intelligent Routing**:\n",
    "   - Based on the **query_category** and **query_sentiment**, the system routes the query to the appropriate handling node:\n",
    "     - **Escalate to Human**: If the sentiment is negative, the query is escalated to a human for resolution.\n",
    "     - **Generate Billing Response**: Queries related to billing are routed to generate an appropriate response.\n",
    "     - **Generate Technical Response**: Technical queries are routed for a specialized technical response.\n",
    "     - **Generate General Response**: General queries are handled with context-aware responses.\n",
    "\n",
    "3. **Knowledge Base Integration (RAG)**:\n",
    "   - The system integrates with a **Knowledge Base (Vector Database)** to augment responses with relevant and accurate information.\n",
    "   - Retrieval-Augmented Generation (RAG) ensures that responses are grounded in the latest and most reliable data.\n",
    "\n",
    "4. **Escalation Mechanism**:\n",
    "   - Negative sentiment triggers an **escalation to a human**, ensuring the user receives empathetic and personalized support for critical issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd7f264-e3f9-49a0-a95d-9a9fea12bd4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9hEI3WL328vZ"
   },
   "source": [
    "## Install OpenAI, LangGraph and LangChain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f4ae8b-26d5-460d-b253-5ce3b960ca47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dzjE7G_8KOUM"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain==0.3.14\n",
    "!pip install -q langchain-openai==0.3.0\n",
    "!pip install -q langchain-community==0.3.14\n",
    "!pip install -q langgraph==0.2.64\n",
    "!pip install -U -qqqq databricks-langchain uv databricks-agents mlflow-skinny[databricks]\n",
    "!pip install -q databricks-vectorsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6074cef-43a4-4c5a-b3e0-d3d613776535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e243b2-48d6-4ee1-8655-ad8d42da787b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "H9c37cLnSrbg"
   },
   "source": [
    "## Enter Open AI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4da614-ed2b-4d74-838f-bfe0ab4fd51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "cv3JzCEx_PAd"
   },
   "outputs": [],
   "source": [
    "# from getpass import getpass\n",
    "\n",
    "# OPENAI_KEY = getpass('Enter Open AI API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dd8d91d-991d-4796-a155-124c397ff854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1T0s0um5Svfa"
   },
   "source": [
    "## Setup Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f4215c-01b2-4ba2-939f-e6f2251eb71a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "x1YSuHNF_lbh"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1072bfbf-aab2-4cd8-88e6-3ce0650223b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %%writefile agent.py\n",
    "# ##########################################################\n",
    "# # Initial Imports\n",
    "# ##########################################################\n",
    "# import os\n",
    "# from databricks.vector_search.client import VectorSearchClient\n",
    "# from langchain_core.documents import Document\n",
    "# from typing import List,Dict,TypedDict, Literal\n",
    "# from pydantic import BaseModel\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langgraph.graph import StateGraph, END\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from databricks_langchain import ChatDatabricks, UCFunctionToolkit, VectorSearchRetrieverTool\n",
    "# from langgraph.graph import StateGraph, END\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from mlflow.pyfunc import ResponsesAgent\n",
    "# from mlflow.types.responses import (\n",
    "#     ResponsesAgentRequest,\n",
    "#     ResponsesAgentResponse,\n",
    "#     ResponsesAgentStreamEvent,\n",
    "# )\n",
    "# from typing import Annotated, Any, Generator, Optional, Sequence, Union\n",
    "# import mlflow\n",
    "\n",
    "# ###########################################################\n",
    "# # Initial Variables Declaration\n",
    "# ###########################################################\n",
    "# endpoint_name=\"router_agent_endpoint\"\n",
    "# source_table_name=\"agentic_ai.langgraph.router_agent_chunks\"\n",
    "# index_name=\"agentic_ai.langgraph.router_agent_index\"\n",
    "\n",
    "# ###########################################################\n",
    "# # Instantiate the VectorSearchClient to interact with Databricks Vector Search endpoints.\n",
    "# ###########################################################\n",
    "# client = VectorSearchClient()\n",
    "# index = client.get_index(endpoint_name=endpoint_name, index_name=index_name)\n",
    "\n",
    "# ################################################################################\n",
    "# # Define your LLM endpoint\n",
    "# ################################################################################\n",
    "# # llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "# # LLM_ENDPOINT_NAME = \"databricks-gpt-oss-120b\"\n",
    "# LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "# llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# ######################################################################################################################\n",
    "# # Define the Customer Inquiry State\n",
    "# # We create a CustomerSupportState typed dictionary to keep track of each interaction:\n",
    "\n",
    "# # customer_query: The text of the customer's question\n",
    "# # query_category: Technical, Billing, or General (used for routing)\n",
    "# # query_sentiment: Positive, Neutral, or Negative (used for routing)\n",
    "# # final_response: The system's response to the customer\n",
    "# ######################################################################################################################\n",
    "# class CustomerSupportState(TypedDict):\n",
    "#     \"\"\"\n",
    "#     customer_query: the original query from the customer.\n",
    "#     query_category: the topic of the query (e.g., Technical, Billing).\n",
    "#     query_sentiment: the emotional tone (e.g., Positive, Negative).\n",
    "#     final_response: the system-generated response.\n",
    "#     \"\"\"\n",
    "#     customer_query: str\n",
    "#     query_category: str\n",
    "#     query_sentiment: str\n",
    "#     final_response: str\n",
    "\n",
    "# class QueryCategory(BaseModel):\n",
    "#     categorized_topic: Literal['Technical', 'Billing', 'General']\n",
    "\n",
    "# class QuerySentiment(BaseModel):\n",
    "#     sentiment: Literal['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "# ######################################################################################################################\n",
    "# # Create Node Functions\n",
    "# # Each function below represents a stage in processing a customer inquiry:\n",
    "\n",
    "# # categorize_inquiry: Classifies the query into Technical, Billing, or General.\n",
    "# # analyze_inquiry_sentiment: Determines if the sentiment is Positive, Neutral, or Negative.\n",
    "# # generate_technical_response: Produces a response for technical issues.\n",
    "# # generate_billing_response: Produces a response for billing questions.\n",
    "# # generate_general_response: Produces a response for general queries.\n",
    "# # escalate_to_human_agent: Escalates the query to a human if sentiment is negative.\n",
    "# # determine_route: Routes the inquiry to the appropriate response node based on category and sentiment.\n",
    "# ######################################################################################################################\n",
    "# def categorize_inquiry(support_state: CustomerSupportState) -> CustomerSupportState:\n",
    "#     \"\"\"\n",
    "#     Classify the customer query into Technical, Billing, or General.\n",
    "#     \"\"\"\n",
    "\n",
    "#     query = support_state[\"customer_query\"]\n",
    "#     ROUTE_CATEGORY_PROMPT = \"\"\"Act as a customer support agent trying to best categorize the customer query.\n",
    "#                                You are an agent for an AI products and hardware company.\n",
    "\n",
    "#                                Please read the customer query below and\n",
    "#                                determine the best category from the following list:\n",
    "\n",
    "#                                'Technical', 'Billing', or 'General'.\n",
    "\n",
    "#                                Remember:\n",
    "#                                 - Technical queries will focus more on technical aspects like AI models, hardware, software related queries etc.\n",
    "#                                 - General queries will focus more on general aspects like contacting support, finding things, policies etc.\n",
    "#                                 - Billing queries will focus more on payment and purchase related aspects\n",
    "\n",
    "#                                 Return just the category name (from one of the above)\n",
    "\n",
    "#                                 Query:\n",
    "#                                 {customer_query}\n",
    "#                             \"\"\"\n",
    "#     prompt = ROUTE_CATEGORY_PROMPT.format(customer_query=query)\n",
    "#     route_category = llm.with_structured_output(QueryCategory).invoke(prompt)\n",
    "\n",
    "#     return {\n",
    "#         \"query_category\": route_category.categorized_topic\n",
    "#     }\n",
    "\n",
    "# def analyze_inquiry_sentiment(support_state: CustomerSupportState) -> CustomerSupportState:\n",
    "#     \"\"\"\n",
    "#     Analyze the sentiment of the customer query as Positive, Neutral, or Negative.\n",
    "#     \"\"\"\n",
    "\n",
    "#     query = support_state[\"customer_query\"]\n",
    "#     SENTIMENT_CATEGORY_PROMPT = \"\"\"Act as a customer support agent trying to best categorize the customer query's sentiment.\n",
    "#                                    You are an agent for an AI products and hardware company.\n",
    "\n",
    "#                                    Please read the customer query below,\n",
    "#                                    analyze its sentiment which should be one from the following list:\n",
    "\n",
    "#                                    'Positive', 'Neutral', or 'Negative'.\n",
    "\n",
    "#                                    Return just the sentiment (from one of the above)\n",
    "\n",
    "#                                    Query:\n",
    "#                                    {customer_query}\n",
    "#                                 \"\"\"\n",
    "#     prompt = SENTIMENT_CATEGORY_PROMPT.format(customer_query=query)\n",
    "#     sentiment_category = llm.with_structured_output(QuerySentiment).invoke(prompt)\n",
    "\n",
    "#     return {\n",
    "#         \"query_sentiment\": sentiment_category.sentiment\n",
    "#     }\n",
    "\n",
    "# def generate_technical_response(support_state: CustomerSupportState) -> CustomerSupportState:\n",
    "#     \"\"\"\n",
    "#     Provide a technical support response by combining knowledge from the vector store and LLM.\n",
    "#     \"\"\"\n",
    "#     # Retrieve category and ensure it is lowercase for metadata filtering\n",
    "\n",
    "#     categorized_topic = support_state[\"query_category\"]\n",
    "#     query = support_state[\"customer_query\"]\n",
    "\n",
    "#     # Use metadata filter for 'technical' queries\n",
    "#     if categorized_topic.lower() == \"technical\":\n",
    "#         # Perform retrieval from VectorDB\n",
    "#         results = index.similarity_search(\n",
    "#                 query_text=query,\n",
    "#                 columns=[\"content\",\"category\"],  # Ensure only columns present in the index are listed\n",
    "#                 num_results=3,\n",
    "#                 filters={\"category\": [\"technical\"]},\n",
    "#                 query_type=\"hybrid\"\n",
    "#             )\n",
    "#         relevant_docs = convert_vector_search_to_documents(results)\n",
    "#         retrieved_content = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "#         # Combine retrieved information into the prompt\n",
    "#         prompt = ChatPromptTemplate.from_template(\n",
    "#             \"\"\"\n",
    "#             Craft a clear and detailed technical support response for the following customer query.\n",
    "#             Use the provided knowledge base information to enrich your response.\n",
    "#             In case there is no knowledge base information or you do not know the answer just say:\n",
    "\n",
    "#             Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\n",
    "\n",
    "#             Customer Query:\n",
    "#             {customer_query}\n",
    "\n",
    "#             Relevant Knowledge Base Information:\n",
    "#             {retrieved_content}\n",
    "#             \"\"\"\n",
    "#         )\n",
    "\n",
    "#         # Generate the final response using the LLM\n",
    "#         chain = prompt | llm\n",
    "#         tech_reply = chain.invoke({\n",
    "#             \"customer_query\": query,\n",
    "#             \"retrieved_content\": retrieved_content\n",
    "#         }).content\n",
    "#     else:\n",
    "#         # For non-technical queries, provide a default response or a general handling\n",
    "#         tech_reply = \"Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\"\n",
    "\n",
    "#     # Update and return the modified support state\n",
    "#     return {\n",
    "#         \"final_response\": tech_reply\n",
    "#     }\n",
    "    \n",
    "# def generate_billing_response(support_state: CustomerSupportState) -> CustomerSupportState:\n",
    "#     \"\"\"\n",
    "#     Provide a billing support response by combining knowledge from the vector store and LLM.\n",
    "#     \"\"\"\n",
    "#     # Retrieve category and ensure it is lowercase for metadata filtering\n",
    "#     categorized_topic = support_state[\"query_category\"]\n",
    "#     query = support_state[\"customer_query\"]\n",
    "\n",
    "#     # Use metadata filter for 'billing' queries\n",
    "#     if categorized_topic.lower() == \"billing\":\n",
    "#         # Perform retrieval from VectorDB\n",
    "#         results = index.similarity_search(\n",
    "#                 query_text=query,\n",
    "#                 columns=[\"content\",\"category\"],  # Ensure only columns present in the index are listed\n",
    "#                 num_results=3,\n",
    "#                 filters={\"category\": [\"billing\"]},\n",
    "#                 query_type=\"hybrid\"\n",
    "#             )\n",
    "#         relevant_docs = convert_vector_search_to_documents(results)\n",
    "#         retrieved_content = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "#         # Combine retrieved information into the prompt\n",
    "#         prompt = ChatPromptTemplate.from_template(\n",
    "#             \"\"\"\n",
    "#             Craft a clear and detailed billing support response for the following customer query.\n",
    "#             Use the provided knowledge base information to enrich your response.\n",
    "#             In case there is no knowledge base information or you do not know the answer just say:\n",
    "\n",
    "#             Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\n",
    "\n",
    "#             Customer Query:\n",
    "#             {customer_query}\n",
    "\n",
    "#             Relevant Knowledge Base Information:\n",
    "#             {retrieved_content}\n",
    "#             \"\"\"\n",
    "#         )\n",
    "\n",
    "#         # Generate the final response using the LLM\n",
    "#         chain = prompt | llm\n",
    "#         billing_reply = chain.invoke({\n",
    "#             \"customer_query\": query,\n",
    "#             \"retrieved_content\": retrieved_content\n",
    "#         }).content\n",
    "#     else:\n",
    "#         # For non-billing queries, provide a default response or a general handling\n",
    "#         billing_reply = \"Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\"\n",
    "\n",
    "#     # Update and return the modified support state\n",
    "#     return {\n",
    "#         \"final_response\": billing_reply\n",
    "#     }\n",
    "\n",
    "# def generate_general_response(support_state: CustomerSupportState) -> CustomerSupportState:\n",
    "#     \"\"\"\n",
    "#     Provide a general support response by combining knowledge from the vector store and LLM.\n",
    "#     \"\"\"\n",
    "#     # Retrieve category and ensure it is lowercase for metadata filtering\n",
    "#     categorized_topic = support_state[\"query_category\"]\n",
    "#     query = support_state[\"customer_query\"]\n",
    "\n",
    "#     # Use metadata filter for 'general' queries\n",
    "#     if categorized_topic.lower() == \"general\":\n",
    "#         # Perform retrieval from VectorDB\n",
    "#         results = index.similarity_search(\n",
    "#                 query_text=query,\n",
    "#                 columns=[\"content\",\"category\"],  # Ensure only columns present in the index are listed\n",
    "#                 num_results=3,\n",
    "#                 filters={\"category\": [\"general\"]},\n",
    "#                 query_type=\"hybrid\"\n",
    "#             )\n",
    "#         relevant_docs = convert_vector_search_to_documents(results)\n",
    "#         retrieved_content = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "#         print(retrieved_content)\n",
    "\n",
    "#         # Combine retrieved information into the prompt\n",
    "#         prompt = ChatPromptTemplate.from_template(\n",
    "#             \"\"\"\n",
    "#             Craft a clear and detailed general support response for the following customer query.\n",
    "#             Use the provided knowledge base information to enrich your response.\n",
    "#             In case there is no knowledge base information or you do not know the answer just say:\n",
    "\n",
    "#             Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\n",
    "\n",
    "#             Customer Query:\n",
    "#             {customer_query}\n",
    "\n",
    "#             Relevant Knowledge Base Information:\n",
    "#             {retrieved_content}\n",
    "#             \"\"\"\n",
    "#         )\n",
    "\n",
    "#         # Generate the final response using the LLM\n",
    "#         chain = prompt | llm\n",
    "#         general_reply = chain.invoke({\n",
    "#             \"customer_query\": query,\n",
    "#             \"retrieved_content\": retrieved_content\n",
    "#         }).content\n",
    "#     else:\n",
    "#         # For non-general queries, provide a default response or a general handling\n",
    "#         general_reply = \"Apologies I was not able to answer your question, please reach out to +1-xxx-xxxx\"\n",
    "\n",
    "#     # Update and return the modified support state\n",
    "#     return {\n",
    "#         \"final_response\": general_reply\n",
    "#     }\n",
    "\n",
    "# def escalate_to_human_agent(support_state: CustomerSupportState) -> CustomerSupportState:\n",
    "#     \"\"\"\n",
    "#     Escalate the query to a human agent if sentiment is negative.\n",
    "#     \"\"\"\n",
    "\n",
    "#     return {\n",
    "#         \"final_response\": \"Apologies, we are really sorry! Someone from our team will be reaching out to your shortly!\"\n",
    "#     }\n",
    "\n",
    "# def determine_route(support_state: CustomerSupportState) -> str:\n",
    "#     \"\"\"\n",
    "#     Route the inquiry based on sentiment and category.\n",
    "#     \"\"\"\n",
    "#     if support_state[\"query_sentiment\"] == \"Negative\":\n",
    "#         return \"escalate_to_human_agent\"\n",
    "#     elif support_state[\"query_category\"] == \"Technical\":\n",
    "#         return \"generate_technical_response\"\n",
    "#     elif support_state[\"query_category\"] == \"Billing\":\n",
    "#         return \"generate_billing_response\"\n",
    "#     else:\n",
    "#         return \"generate_general_response\"\n",
    "    \n",
    "# ###########################################################\n",
    "# # Convert the vector search result to LangChain Document\n",
    "# ###########################################################\n",
    "# def convert_vector_search_to_documents(results) -> List[Document]:\n",
    "#   column_names = []\n",
    "#   for column in results[\"manifest\"][\"columns\"]:\n",
    "#       column_names.append(column)\n",
    "\n",
    "#   langchain_docs = []\n",
    "#   for item in results[\"result\"][\"data_array\"]:\n",
    "#       metadata = {}\n",
    "#       score = item[-1]\n",
    "#       # print(score)\n",
    "#       i = 1\n",
    "#       for field in item[1:-1]:\n",
    "#           # print(field + \"--\")\n",
    "#           metadata[column_names[i][\"name\"]] = field\n",
    "#           i = i + 1\n",
    "#       doc = Document(page_content=item[0], metadata=metadata)  # , 9)\n",
    "#       langchain_docs.append(doc)\n",
    "#   return langchain_docs\n",
    "\n",
    "# ###############################################################################\n",
    "# # Create Tool-Calling Agent with Language Model and Tools\n",
    "# ###############################################################################\n",
    "# def create_router_agent():\n",
    "#     \"\"\"\n",
    "#     Create a chatbot agent with the given language model and tools.\n",
    "#     \"\"\"\n",
    "#     # Create the graph with our typed state\n",
    "#     customer_support_graph = StateGraph(CustomerSupportState)\n",
    "\n",
    "#     # Add nodes for each function\n",
    "#     customer_support_graph.add_node(\"categorize_inquiry\", categorize_inquiry)\n",
    "#     customer_support_graph.add_node(\"analyze_inquiry_sentiment\", analyze_inquiry_sentiment)\n",
    "#     customer_support_graph.add_node(\"generate_technical_response\", generate_technical_response)\n",
    "#     customer_support_graph.add_node(\"generate_billing_response\", generate_billing_response)\n",
    "#     customer_support_graph.add_node(\"generate_general_response\", generate_general_response)\n",
    "#     customer_support_graph.add_node(\"escalate_to_human_agent\", escalate_to_human_agent)\n",
    "\n",
    "#     # Add edges to represent the processing flow\n",
    "#     customer_support_graph.add_edge(\"categorize_inquiry\", \"analyze_inquiry_sentiment\")\n",
    "#     customer_support_graph.add_conditional_edges(\n",
    "#         \"analyze_inquiry_sentiment\",\n",
    "#         determine_route,\n",
    "#         [\n",
    "#             \"generate_technical_response\",\n",
    "#             \"generate_billing_response\",\n",
    "#             \"generate_general_response\",\n",
    "#             \"escalate_to_human_agent\"\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # All terminal nodes lead to the END\n",
    "#     customer_support_graph.add_edge(\"generate_technical_response\", END)\n",
    "#     customer_support_graph.add_edge(\"generate_billing_response\", END)\n",
    "#     customer_support_graph.add_edge(\"generate_general_response\", END)\n",
    "#     customer_support_graph.add_edge(\"escalate_to_human_agent\", END)\n",
    "\n",
    "#     # Set the entry point for the workflow\n",
    "#     customer_support_graph.set_entry_point(\"categorize_inquiry\")\n",
    "\n",
    "#     # Compile the graph into a runnable agent\n",
    "#     memory = MemorySaver()\n",
    "#     compiled_support_agent = customer_support_graph.compile()\n",
    "#     return compiled_support_agent\n",
    "\n",
    "# ###############################################################################\n",
    "# # Convert Responses to ChatCompletion Messages\n",
    "# ###############################################################################\n",
    "# class LangGraphResponsesAgent(ResponsesAgent):\n",
    "#     def __init__(self, agent):\n",
    "#         self.agent = agent\n",
    "\n",
    "#     def _responses_to_cc(self, message: dict[str, Any]) -> list[dict[str, Any]]:\n",
    "#         \"\"\"Convert from a Responses API output item to ChatCompletion messages.\"\"\"\n",
    "#         msg_type = message.get(\"type\")\n",
    "#         print(f\"Message type: {msg_type}\")\n",
    "#         if msg_type == \"function_call\":\n",
    "#             return [\n",
    "#                 {\n",
    "#                     \"role\": \"assistant\",\n",
    "#                     \"content\": \"tool call\",\n",
    "#                     \"tool_calls\": [\n",
    "#                         {\n",
    "#                             \"id\": message[\"call_id\"],\n",
    "#                             \"type\": \"function\",\n",
    "#                             \"function\": {\n",
    "#                                 \"arguments\": message[\"arguments\"],\n",
    "#                                 \"name\": message[\"name\"],\n",
    "#                             },\n",
    "#                         }\n",
    "#                     ],\n",
    "#                 }\n",
    "#             ]\n",
    "#         elif msg_type == \"message\" and isinstance(message[\"content\"], list):\n",
    "#             return [\n",
    "#                 {\"role\": message[\"role\"], \"content\": content[\"text\"]}\n",
    "#                 for content in message[\"content\"]\n",
    "#             ]\n",
    "#         elif msg_type == \"reasoning\":\n",
    "#             return [{\"role\": \"assistant\", \"content\": json.dumps(message[\"summary\"])}]\n",
    "#         elif msg_type == \"function_call_output\":\n",
    "#             return [\n",
    "#                 {\n",
    "#                     \"role\": \"tool\",\n",
    "#                     \"content\": message[\"output\"],\n",
    "#                     \"tool_call_id\": message[\"call_id\"],\n",
    "#                 }\n",
    "#             ]\n",
    "#         compatible_keys = [\"role\", \"content\", \"name\", \"tool_calls\", \"tool_call_id\"]\n",
    "#         filtered = {k: v for k, v in message.items() if k in compatible_keys}\n",
    "#         return [filtered] if filtered else []\n",
    "\n",
    "\n",
    "#     def _langchain_to_responses(self, messages: list[dict[str, Any]]) -> list[dict[str, Any]]:\n",
    "#         \"Convert from ChatCompletion dict to Responses output item dictionaries\"\n",
    "#         for message in messages:\n",
    "#             message = message.model_dump()\n",
    "#             role = message[\"type\"]\n",
    "#             if role == \"ai\":\n",
    "#                 if tool_calls := message.get(\"tool_calls\"):\n",
    "#                     return [\n",
    "#                         self.create_function_call_item(\n",
    "#                             id=message.get(\"id\") or str(uuid4()),\n",
    "#                             call_id=tool_call[\"id\"],\n",
    "#                             name=tool_call[\"name\"],\n",
    "#                             arguments=json.dumps(tool_call[\"args\"]),\n",
    "#                         )\n",
    "#                         for tool_call in tool_calls\n",
    "#                     ]\n",
    "#                 else:\n",
    "#                     return [\n",
    "#                         self.create_text_output_item(\n",
    "#                             text=message[\"content\"],\n",
    "#                             id=message.get(\"id\") or str(uuid4()),\n",
    "#                         )\n",
    "#                     ]\n",
    "#             elif role == \"tool\":\n",
    "#                 return [\n",
    "#                     self.create_function_call_output_item(\n",
    "#                         call_id=message[\"tool_call_id\"],\n",
    "#                         output=message[\"content\"],\n",
    "#                     )\n",
    "#                 ]\n",
    "#             elif role == \"user\":\n",
    "#                 return [message]\n",
    "#             else:\n",
    "#                 return [message]\n",
    "\n",
    "#     def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "#         outputs = [\n",
    "#             event.item\n",
    "#             for event in self.predict_stream(request)\n",
    "#             if event.type == \"response.output_item.done\"\n",
    "#         ]\n",
    "#         return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "#     def predict_stream(\n",
    "#         self,\n",
    "#         request: ResponsesAgentRequest,\n",
    "#     ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "#         cc_msgs = []\n",
    "#         for msg in request.input:\n",
    "#             cc_msgs.extend(self._responses_to_cc(msg.model_dump()))\n",
    "\n",
    "#         for event in self.agent.stream({\"customer_query\": cc_msgs}, stream_mode=[\"values\"]):\n",
    "#             if event[0] == \"updates\":\n",
    "#                 for node_data in event[1].values():\n",
    "#                     print(node_data)\n",
    "#                     for item in self._langchain_to_responses(node_data.get('final_response', [])):\n",
    "#                         yield ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=item)\n",
    "#             # filter the streamed messages to just the generated text messages\n",
    "#             elif event[0] == \"messages\":\n",
    "#                 try:\n",
    "#                     chunk = event[1][0]\n",
    "#                     if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "#                         yield ResponsesAgentStreamEvent(\n",
    "#                             **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "#                         )\n",
    "#                 except Exception as e:\n",
    "#                     print(e)\n",
    "\n",
    "# # Create the agent object, and specify it as the agent object to use when\n",
    "# # loading the agent back for inference via mlflow.models.set_model()\n",
    "# mlflow.langchain.autolog()\n",
    "# agent = create_router_agent()\n",
    "# AGENT = LangGraphResponsesAgent(agent)\n",
    "# mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d8ba709-2792-4669-9349-15c5300b1236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from agent import AGENT,agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386adcb6-3f42-4d74-89b4-bf5609c398b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DEBUG predict_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c41cd538-e822-4217-bce3-1eb721123d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from mlflow.types.responses import ResponsesAgentRequest\n",
    "\n",
    "# request_data = {\n",
    "#     \"input\": [{\"role\": \"user\", \"content\": \"Can you tell me about your shipping policy?\"}]\n",
    "# }\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"STEP 1: Input Request\")\n",
    "# print(\"=\" * 80)\n",
    "# print(request_data)\n",
    "# print()\n",
    "\n",
    "# # Step 2: Convert Responses format to ChatCompletion format\n",
    "# cc_msgs = []\n",
    "# for msg in request_data[\"input\"]:\n",
    "#     print(f\"Converting: {msg}\")\n",
    "#     converted = AGENT._responses_to_cc(msg)\n",
    "#     cc_msgs.extend(converted)\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"STEP 2: Converted to ChatCompletion Format\")\n",
    "# print(\"=\" * 80)\n",
    "# print(cc_msgs)\n",
    "# print()\n",
    "\n",
    "# # Step 3: Start streaming the agent\n",
    "# print(\"=\" * 80)\n",
    "# print(\"STEP 3: Streaming Agent Events\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# event_count = 0\n",
    "# for event in AGENT.agent.stream(\n",
    "#     {\"customer_query\": cc_msgs}, \n",
    "#     stream_mode=[\"values\"]\n",
    "# ):\n",
    "#     event_count += 1\n",
    "#     print(f\"\\n--- Event #{event_count} ---\")\n",
    "#     print(f\"Event Type: {event[0]}\")\n",
    "\n",
    "#     if event[0] == \"updates\":\n",
    "#         print(\"Mode: STATE UPDATES (complete items)\")\n",
    "#         for node_name, node_data in event[1].items():\n",
    "#             print(f\"  Node: {node_name}\")\n",
    "#             print(f\"node_data: {node_data}\")\n",
    "#             print(type(node_data.get('final_response', [])))\n",
    "#             print(f\"  Messages in update: {len(node_data.get('final_response', []))}\")\n",
    "#             # print(f\"Messages in update: {len(node_data['message'])}\")\n",
    "\n",
    "#             for msg in node_data.get('final_response', []):\n",
    "#                 print(f\"    - {type(msg).__name__}: {str(msg)[:100]}...\")\n",
    "#                 print(\"----------------\")\n",
    "#                 print(msg)\n",
    "      \n",
    "#                 # Convert to Responses format\n",
    "#                 responses_items = AGENT._langchain_to_responses([msg])\n",
    "#                 if responses_items:\n",
    "#                     for item in responses_items:\n",
    "#                         print(f\"      Converted to: {item}\")\n",
    "              \n",
    "#     # elif event[0] == \"messages\":\n",
    "#     #     print(\"Mode: MESSAGE STREAMING (text chunks)\")\n",
    "#     #     chunk = event[1][0]\n",
    "#     #     print(f\"  Chunk Type: {type(chunk).__name__}\")\n",
    "#     #     if hasattr(chunk, 'content') and chunk.content:\n",
    "#     #         print(f\"  Content: '{chunk.content}'\")\n",
    "\n",
    "#     # # Pause to inspect (optional - remove in production)\n",
    "#     # if event_count >= 20:  # Limit for testing\n",
    "#     #     print(\"\\n[Showing first 20 events only... Remove limit to see all]\")\n",
    "#     #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4069f3f2-d044-4c50-97f7-e7d9923ede89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image,Markdown\n",
    "\n",
    "Image(agent.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8de4be-1b01-4603-83e1-b406d1643967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def call_support_agent(agent, prompt, user_session_id, verbose=False):\n",
    "    events = agent.stream(\n",
    "        {\"customer_query\": prompt}, # initial state of the agent\n",
    "        {\"configurable\": {\"thread_id\": user_session_id}},\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "\n",
    "    print('Running Agent. Please wait...')\n",
    "    for event in events:\n",
    "        if verbose:\n",
    "                print(event)\n",
    "\n",
    "    display(Markdown(event['final_response']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a563b4-d1ee-450c-9014-92af8fb39bd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Testing the Customer Support Workflow\n",
    "\n",
    "Let's test the workflow with some sample queries to verify categorization, sentiment analysis, and response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "687eaa9f-452a-4976-8b4d-56a862fbbbf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for chunk in AGENT.predict_stream(\n",
    "    {\"input\": [{\"role\": \"user\", \"content\": \"Can you tell me about your shipping policy?\"}]}\n",
    "):\n",
    "    print(chunk.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d23bcd-9f7d-40a8-85b4-220afc48670d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = AGENT.predict({\"input\": [{\"role\": \"user\", \"content\": \"Can you tell me about your shipping policy?\"}]})\n",
    "print(result.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8209aabc-1f7b-47a7-848c-2e40818a1cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# uid = 'jim001'\n",
    "# query = \"do you support pre-trained models?\"\n",
    "# call_support_agent(agent=agent,\n",
    "#                    prompt=query,\n",
    "#                    user_session_id=uid,\n",
    "#                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de62cf84-6c45-4280-871e-96a591802d53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# uid = 'jim002'\n",
    "# query = \"how do I get my invoice?\"\n",
    "# call_support_agent(agent=agent,\n",
    "#                    prompt=query,\n",
    "#                    user_session_id=uid,\n",
    "#                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b48ec9-d357-4921-bd72-575ab3f04608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain_core.documents import Document\n",
    "from typing import List,Dict,TypedDict, Literal\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from databricks_langchain import ChatDatabricks, UCFunctionToolkit, VectorSearchRetrieverTool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    ")\n",
    "from typing import Annotated, Any, Generator, Optional, Sequence, Union\n",
    "import mlflow\n",
    "from pkg_resources import get_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bf8e14-d21a-4e7f-9cdb-e3db3ce2d0b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"agentic_ai\"\n",
    "schema = \"langgraph\"\n",
    "model_name = \"customer_support_agent\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"customer_support_assistant\",\n",
    "        python_model=AGENT, \n",
    "        pip_requirements=[\n",
    "            \"langchain==0.3.14\",\n",
    "            \"langchain-openai==0.3.0\",\n",
    "            \"langchain-community==0.3.14\",\n",
    "            \"langgraph==0.2.64\",\n",
    "            \"databricks-langchain\",\n",
    "            \"databricks-agents\",\n",
    "            \"mlflow-skinny[databricks]\",\n",
    "            \"databricks-vectorsearch\"\n",
    "        ],\n",
    "        registered_model_name=UC_MODEL_NAME,\n",
    "            )\n",
    "# The error was due to incorrect usage of the 'python_model' argument and 'name' instead of 'artifact_path'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6020e62b-93b6-4f8e-8ffe-7862e1840c44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"Can you tell me about your shipping policy?\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb58f8a-9890-47c0-9fd3-cdadef4b2055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"agents\"\n",
    "schema = \"main\"\n",
    "model_name = \"langgraph-responses-agent\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "M5_Project_ Build_a_Customer_Support_Router_Agentic_RAG_System",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
