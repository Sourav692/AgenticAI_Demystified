{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40593191-0b62-4413-86cd-8899710603f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reference Link:** [RAG Systems Essentials (Analytics Vidhya)](https://courses.analyticsvidhya.com/courses/take/rag-systems-essentials/lessons/60148017-hands-on-deep-dive-into-rag-evaluation-metrics-generator-metrics-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Evaluation Metrics\n",
    "\n",
    "## Overview\n",
    "- This notebook demonstrates how to evaluate RAG system generators using DeepEval metrics\n",
    "- It focuses on four key evaluation metrics for assessing generation quality in RAG pipelines\n",
    "\n",
    "## Key Evaluation Metrics\n",
    "\n",
    "### **Answer Relevancy**\n",
    "- **Purpose**: Measures how relevant the generated answer is to the input query\n",
    "- **Implementation Options**:\n",
    "  - **DeepEval**: LLM-based evaluation using GPT-4o as judge\n",
    "  - **RAGAS**: Semantic similarity-based evaluation using cosine similarity\n",
    "- **Input Requirements**: Query and generated response\n",
    "- **Use Case**: Assesses if the answer directly addresses the user's question\n",
    "\n",
    "### **Faithfulness**\n",
    "- **Purpose**: Measures whether the generated output factually aligns with retrieved context\n",
    "- **Input Requirements**: Query, generated response, and retrieval context\n",
    "- **Scoring**: Evaluates factual consistency between response and source documents\n",
    "- **Use Case**: Ensures generated answers are grounded in retrieved information\n",
    "\n",
    "### **Hallucination Check**\n",
    "- **Purpose**: Determines if the LLM generates factually correct information\n",
    "- **Input Requirements**: Query, generated response, and human ground truth context\n",
    "- **Scoring**: Evaluates factual accuracy against verified information\n",
    "- **Use Case**: Detects when AI generates false or unsupported claims\n",
    "\n",
    "### **Custom LLM as Judge (G-Eval)**\n",
    "- **Purpose**: Uses advanced LLM models to evaluate response quality\n",
    "- **Implementation**: Leverages GPT-4o for sophisticated evaluation reasoning\n",
    "- **Benefits**: Provides detailed explanations for metric scores\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "- **DeepEval Framework**: Comprehensive evaluation suite for RAG systems\n",
    "- **LLM Judge**: GPT-4o model evaluates quality and provides reasoning\n",
    "- **Test Cases**: Creates LLMTestCase objects for systematic evaluation\n",
    "- **Configurable Thresholds**: Adjustable success criteria for each metric\n",
    "- **Verbose Mode**: Detailed reasoning and evaluation breakdowns\n",
    "\n",
    "## Evaluation Process\n",
    "\n",
    "1. **Setup**: Run existing RAG pipeline to get generation results\n",
    "2. **Context Preparation**: Extract retrieved documents and ground truth\n",
    "3. **Metric Configuration**: Set up evaluation parameters and thresholds\n",
    "4. **Testing**: Run evaluation on test cases with different scenarios\n",
    "5. **Analysis**: Review scores, reasons, and pass/fail results\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- **Quality Assurance**: Systematic evaluation of generation performance\n",
    "- **Hallucination Detection**: Identifies when AI generates false information\n",
    "- **Relevance Assessment**: Ensures answers directly address user queries\n",
    "- **Factual Verification**: Confirms generated content aligns with source material\n",
    "- **Continuous Improvement**: Provides metrics to optimize generation quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0e8dc7-036f-4de3-88a3-55b5ed757180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZlekplN_WC5A"
   },
   "source": [
    "# Generator Evaluation Metrics\n",
    "\n",
    "![](https://i.imgur.com/GaMHy7w.png)\n",
    "\n",
    "The generation step, which comes after retrieval, generally includes:\n",
    "\n",
    "- Building a prompt that combines the initial input with the context retrieved in the previous step.\n",
    "- Feeding this prompt to your LLM, which produces the final generated response.\n",
    "\n",
    "\n",
    "Key Metrics to Evaluate here include:\n",
    "\n",
    "- Answer Relevancy\n",
    "- Faithfulness\n",
    "- Hallucination Check\n",
    "- Custom LLM as a Judge (G-Eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33bf149b-34b7-46b2-b476-224600d3191b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "bCsfBjF5l_sz"
   },
   "source": [
    "## LLM-based Answer Relevancy - DeepEval\n",
    "\n",
    "The answer relevancy metric measures the quality of your RAG pipeline's generator by evaluating how relevant the `actual_output` of your LLM application is compared to the provided `input`.\n",
    "\n",
    "`deepeval`'s answer relevancy metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score using an LLM as a Judge.\n",
    "\n",
    "In `deepeval`, to use the AnswerRelevancyMetric, you'll have to provide the following arguments when creating an `LLMTestCase`:\n",
    "\n",
    "- `input` : Input Query\n",
    "- `actual_output` : Actual LLM Response\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/GbNSCFC.png)\n",
    "\n",
    "\n",
    "\n",
    "## Semantic Similarity based Answer Relevancy - RAGAS\n",
    "\n",
    "DeepEval has bindings to Ragas which enables us to use the RAGASAnswerRelevancyMetric which focuses on assessing how pertinent the generated answer is to the given query using cosine similarity. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy.\n",
    "\n",
    "![](https://i.imgur.com/vq1ytZ3.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.3.11 requires langchain<0.4.0,>=0.3.11, but you have langchain 0.3.10 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mMetadata: {'id': 10, 'title': 'Artificial Intelligence'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': 10, 'title': 'Artificial Intelligence'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': 3, 'title': 'Natural Language Processing (NLP)'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': 5, 'title': 'Photosynthesis'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Photosynthesis is the process plants use to convert sunlight into energy. This process produces glucose and releases oxygen as a byproduct. It is crucial for sustaining life on Earth by providing food and oxygen."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'id': 5, 'title': 'Photosynthesis'}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Photosynthesis is the process plants use to convert sunlight into energy. This process produces glucose and releases oxygen as a byproduct. It is crucial for sustaining life on Earth by providing food and oxygen."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run Build_RAG_Pipeline_with_Source.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65587888-f81f-4dfc-a45c-c63e3c838e4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1210,
     "status": "ok",
     "timestamp": 1734095873218,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "2nWBbl_3l_s0",
    "outputId": "e32d6c8f-6484-4ba6-b9ee-2be7da70d804"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'id': 10, 'title': 'Artificial Intelligence'}, page_content=\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\"),\n",
       "  Document(metadata={'id': 10, 'title': 'Artificial Intelligence'}, page_content=\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\"),\n",
       "  Document(metadata={'id': 3, 'title': 'Natural Language Processing (NLP)'}, page_content='NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.')],\n",
       " 'question': 'What is AI?',\n",
       " 'response': 'AI refers to machines mimicking human intelligence, such as problem-solving and learning, and includes applications like virtual assistants, robotics, and autonomous vehicles.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is AI?\"\n",
    "response = rag_chain_w_sources.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2773deca-bf0d-43c9-81b7-8e95f7d153ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "81EpPklYl_s1"
   },
   "source": [
    "### Example - DeepEval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e9305c8-5399-48ec-9228-694c12ed98e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3675,
     "status": "ok",
     "timestamp": 1734095878632,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "2klPvY0Al_s2",
    "outputId": "78f646f0-bf75-4b67-b458-33346528fd59"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:04,  4.31s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Answer Relevancy Verbose Logs\n",
      "**************************************************\n",
      "\n",
      "Statements:\n",
      "[\n",
      "    \"AI refers to machines mimicking human intelligence, such as problem-solving and learning, and includes applications like virtual assistants, robotics, and autonomous vehicles.\"\n",
      "] \n",
      " \n",
      "Verdicts:\n",
      "[\n",
      "    {\n",
      "        \"verdict\": \"yes\",\n",
      "        \"reason\": null\n",
      "    }\n",
      "]\n",
      " \n",
      "Score: 1.0\n",
      "Reason: The score is 1.00 because the answer is perfectly relevant and directly addresses the question 'What is AI?' without any irrelevant information. Great job!\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the answer is perfectly relevant and directly addresses the question 'What is AI?' without any irrelevant information. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is AI?\n",
      "  - actual output: AI refers to machines mimicking human intelligence, such as problem-solving and learning, and includes applications like virtual assistants, robotics, and autonomous vehicles.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=response['response'],\n",
    ")\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c738912-7a6f-487f-9c59-f38738f3e7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1734095878632,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "JR5A3ZhPp6Dz",
    "outputId": "552510ec-1894-4a35-bcca-e830f48b6fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess: True\n",
      "Score: 1.0\n",
      "Reason: The score is 1.00 because the response was perfectly relevant with no irrelevant statements. Great job!\n"
     ]
    }
   ],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15a8585d-09cc-45c3-9a49-ce13fcf3a26f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9vZNbqkwx-Sj"
   },
   "source": [
    "### Example - RAGAS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb797f46-1f5c-487c-990e-c985edfc4c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620,
     "referenced_widgets": [
      "8890484b6c3e4018907b96b026510712",
      "0a1982c87e18425a97c43a61c18f928b",
      "d7e48662099b409baeb15177af124bb5",
      "d3d8515a3b194fb3b1fdc820607a00bf",
      "ce0f9b8f431246da93ebbe3c5cb8b026",
      "28c6583c344341599988c3dad3070cc8",
      "9d4be022d51847e8a1ccd0a9eb885de2",
      "7614ca4667f948ccba8f980a1ae4722d",
      "ab7499442d7648168ca9415bda596e05",
      "ad467cc9f9884950ada71fbb95cfccbd",
      "c5bd0a78f28749e39a63421f0da20446"
     ]
    },
    "executionInfo": {
     "elapsed": 5490,
     "status": "ok",
     "timestamp": 1734095893121,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "VyssN8E4pv-U",
    "outputId": "449d537e-ac37-47a7-cbbc-84b673781abc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">ragas</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mragas\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |          |  0% (0/1) [Time Taken: 00:00, ?test case/s]/Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Evaluating: 100%|██████████| 1/1 [00:04<00:00,  4.36s/it]\n",
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:10, 10.53s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (ragas) (score: 0.9208820444994655, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: None, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is AI?\n",
      "  - actual output: AI refers to machines mimicking human intelligence, such as problem-solving and learning, and includes applications like virtual assistants, robotics, and autonomous vehicles.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy (ragas): 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.metrics.ragas import RAGASAnswerRelevancyMetric\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=response['response'],\n",
    ")\n",
    "\n",
    "metric = RAGASAnswerRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    embeddings=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efc93360-ab76-4316-86e9-acf19e722268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1734095897061,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "WAubWCbzl_s3",
    "outputId": "2e878c34-3b16-4214-d5e8-d65dd47b5184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess: True\n",
      "Score: 0.9208820444994655\n",
      "Reason: None\n"
     ]
    }
   ],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e627de7f-feed-4e6a-914f-5bcfe6f3c5ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JmdnS3rw1EhX"
   },
   "source": [
    "## Faithfulness\n",
    "\n",
    "The faithfulness metric measures the quality of your RAG pipeline's generator by evaluating whether the `actual_output` factually aligns with the contents of your `retrieval_context`.\n",
    "\n",
    "`deepeval`'s faithfulness metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score using an LLM as a Judge.\n",
    "\n",
    "In `deepeval`, to use the FaithfulnessMetric, you'll have to provide the following arguments when creating an `LLMTestCase`:\n",
    "\n",
    "- `input` : Input Query (not used in the computation)\n",
    "- `actual_output` : Actual LLM Response\n",
    "- `retrieval_context` : Top-N retrieved document chunks (nodes) from Vector DB\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/OCSFPTb.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6e7c4d9-707f-4311-8da1-ff24cc34f9eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YPm0hd3m1EhY",
    "outputId": "1fc21ab4-c554-4b6b-a732-a1e45c1503ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'id': 10, 'title': 'Artificial Intelligence'}, page_content=\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\"),\n",
       "  Document(metadata={'id': 10, 'title': 'Artificial Intelligence'}, page_content=\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\"),\n",
       "  Document(metadata={'id': 3, 'title': 'Natural Language Processing (NLP)'}, page_content='NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.')],\n",
       " 'question': 'What is AI?',\n",
       " 'response': 'AI refers to machines mimicking human intelligence, such as problem-solving and learning, and includes applications like virtual assistants, robotics, and autonomous vehicles.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is AI?\"\n",
    "response = rag_chain_w_sources.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e09101f-84cf-4fdf-a690-dc14d7cfe6b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tzoXGIqh1EhZ"
   },
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5115b8db-b9f4-4929-a02a-0884ab1c6098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-p0SPQqW166X",
    "outputId": "60e3f71c-95c1-48d6-a1a8-26a6f17501b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\",\n",
       " \"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\",\n",
       " 'NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_context = [doc.page_content for doc in response['context']]\n",
    "retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56089ffb-74c7-4c60-a088-70056c1b7848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3hw-CRSG1EhZ",
    "outputId": "fd8d4b8a-df2f-462d-bbe7-f7e584519602"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |          |  0% (0/1) [Time Taken: 00:00, ?test case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:08,  8.24s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Faithfulness Verbose Logs\n",
      "**************************************************\n",
      "\n",
      "Truths (limit=None):\n",
      "[\n",
      "    \"Artificial intelligence refers to machines mimicking human intelligence.\",\n",
      "    \"AI includes applications like virtual assistants, robotics, and autonomous vehicles.\",\n",
      "    \"AI is evolving rapidly with advancements in machine learning and deep learning.\",\n",
      "    \"NLP is a branch of AI that enables computers to understand, interpret, and generate human language.\",\n",
      "    \"NLP techniques include tokenization, stemming, and sentiment analysis.\",\n",
      "    \"Applications of NLP range from chatbots to language translation services.\"\n",
      "] \n",
      " \n",
      "Claims:\n",
      "[\n",
      "    \"AI refers to machines mimicking human intelligence, such as problem-solving and learning.\",\n",
      "    \"AI includes applications like virtual assistants, robotics, and autonomous vehicles.\"\n",
      "] \n",
      " \n",
      "Verdicts:\n",
      "[\n",
      "    {\n",
      "        \"verdict\": \"idk\",\n",
      "        \"reason\": null\n",
      "    },\n",
      "    {\n",
      "        \"verdict\": \"yes\",\n",
      "        \"reason\": null\n",
      "    }\n",
      "]\n",
      " \n",
      "Score: 1.0\n",
      "Reason: The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context. Great job maintaining consistency!\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context. Great job maintaining consistency!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is AI?\n",
      "  - actual output: AI refers to machines mimicking human intelligence, such as problem-solving and learning, and includes applications like virtual assistants, robotics, and autonomous vehicles.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: [\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\", \"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\", 'NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Faithfulness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=response['response'],\n",
    "    retrieval_context=retrieved_context\n",
    ")\n",
    "\n",
    "metric = FaithfulnessMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daf6944a-0c99-4824-bf9c-981524ab938b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kDPBfo71EhZ",
    "outputId": "699f306e-344e-4ac3-ea64-b845d6e9bc82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess: True\n",
      "Score: 1.0\n",
      "Reason: The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context. Great job maintaining consistency!\n"
     ]
    }
   ],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1a93d4c-56f6-481b-a298-1573314e0603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3OEaZBlf5Ijm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "808dc093-90bd-4cb3-a4bd-c14e763d1891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "LrIyB4rA5I36"
   },
   "source": [
    "## Hallucination Check\n",
    "\n",
    "The hallucination metric determines whether your LLM generates factually correct information by comparing the `actual_output` to the provided (human ground truth) `context`.\n",
    "\n",
    "`deepeval`'s hallucination metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score using an LLM as a Judge.\n",
    "\n",
    "In `deepeval`, to use the HallucinationMetric, you'll have to provide the following arguments when creating an `LLMTestCase`:\n",
    "\n",
    "- `input` : Input Query (not used in the computation)\n",
    "- `actual_output` : Actual LLM Response\n",
    "- `context` : Human Ground Truth Context Document Chunks (Nodes)\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/qyVBKU2.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd24b98d-07a8-4859-953c-7774580edc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGeeQFHy5I37",
    "outputId": "85afd6d4-bfda-4e1f-bbd8-07266bce8ebc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'id': 10, 'title': 'Artificial Intelligence'}, page_content=\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\"),\n",
       "  Document(metadata={'id': 10, 'title': 'Artificial Intelligence'}, page_content=\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\"),\n",
       "  Document(metadata={'id': 3, 'title': 'Natural Language Processing (NLP)'}, page_content='NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.')],\n",
       " 'question': 'What is AI?',\n",
       " 'response': 'AI refers to machines mimicking human intelligence, such as problem-solving and learning, and includes applications like virtual assistants, robotics, and autonomous vehicles. It is evolving rapidly with advancements in machine learning and deep learning.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is AI?\"\n",
    "response = rag_chain_w_sources.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f61af9c3-d654-4157-b228-6d4918a7916d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "L3uQFN3I5I37"
   },
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6b20bd1-bf36-497d-b3bd-e4ae2b81d30f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VGxjvNZM5I38",
    "outputId": "199dcfe7-6521-42d3-e711-416f67f3b26d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\",\n",
       " \"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\",\n",
       " 'NLP is a branch of AI that enables computers to understand, interpret, and generate human language. Techniques include tokenization, stemming, and sentiment analysis. Applications range from chatbots to language translation services.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_context = [doc.page_content for doc in response['context']]\n",
    "retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f577dd7-d22d-4194-af44-e08802745116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgjaKdKR6O4_",
    "outputId": "addeede2-fd07-421e-83d1-4475e38b9c69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\",\n",
       " 'Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_ground_truth_context = [\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\",\n",
    "                              \"Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.\"]\n",
    "human_ground_truth_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "869fb096-d869-4f22-a53b-60094d3d2139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 909
    },
    "id": "XXl6z6Cq5I38",
    "outputId": "85693856-dd93-46bb-87d5-bcc5abf0933a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |          |  0% (0/1) [Time Taken: 00:00, ?test case/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:03,  3.47s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Hallucination Verbose Logs\n",
      "**************************************************\n",
      "\n",
      "Verdicts:\n",
      "[\n",
      "    {\n",
      "        \"verdict\": \"yes\",\n",
      "        \"reason\": \"The actual output agrees with the provided context which states that AI refers to machines mimicking human intelligence, like problem-solving and learning, and includes applications like virtual assistants, robotics, and autonomous vehicles. It also mentions the rapid evolution of AI with advancements in machine learning and deep learning.\"\n",
      "    },\n",
      "    {\n",
      "        \"verdict\": \"yes\",\n",
      "        \"reason\": \"The actual output does not contradict the provided context about machine learning being a field of artificial intelligence focused on enabling systems to learn patterns from data. The detail about machine learning and its applications does not conflict with the general description of AI in the actual output.\"\n",
      "    }\n",
      "]\n",
      " \n",
      "Score: 0.0\n",
      "Reason: The score is 0.00 because the actual output fully aligns with the provided context and there are no contradictions.\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context and there are no contradictions., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is AI?\n",
      "  - actual output: AI refers to machines mimicking human intelligence, such as problem-solving and learning, and includes applications like virtual assistants, robotics, and autonomous vehicles. It is evolving rapidly with advancements in machine learning and deep learning.\n",
      "  - expected output: None\n",
      "  - context: [\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\", 'Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.']\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Hallucination: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval import evaluate\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=response['response'],\n",
    "    context=human_ground_truth_context\n",
    ")\n",
    "\n",
    "metric = HallucinationMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdd69775-3b7b-4390-8953-e7174b96189a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNV-zh4f5I38",
    "outputId": "e12a69ed-48e4-49b7-fe0b-69572ab4b52d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess: True\n",
      "Score: 0.0\n",
      "Reason: The score is 0.00 because the actual output fully aligns with the provided context and there are no contradictions.\n"
     ]
    }
   ],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc5fe577-75a2-4544-998a-268faeaf2129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "W6VU09PE6jOj"
   },
   "outputs": [],
   "source": [
    "ai_response = 'AI refers to machines mimicking human intelligence to produce cyborgs and electric sheep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3e6aace-05db-4339-98b7-a5a7c9917798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 909
    },
    "id": "8jBVsZui6k_S",
    "outputId": "a232645d-c59c-4264-da67-58283c77595e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |██████████|100% (1/1) [Time Taken: 00:04,  4.68s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Hallucination Verbose Logs\n",
      "**************************************************\n",
      "\n",
      "Verdicts:\n",
      "[\n",
      "    {\n",
      "        \"verdict\": \"no\",\n",
      "        \"reason\": \"The actual output does not agree with the provided context. The context states that AI includes applications like virtual assistants, robotics, and autonomous vehicles, not the production of cyborgs and electric sheep.\"\n",
      "    },\n",
      "    {\n",
      "        \"verdict\": \"no\",\n",
      "        \"reason\": \"The actual output contradicts the context which defines machine learning as enabling systems to learn patterns from data and does not mention producing cyborgs and electric sheep.\"\n",
      "    }\n",
      "]\n",
      " \n",
      "Score: 1.0\n",
      "Reason: The score is 1.00 because there are multiple contradictions between the actual output and the provided context, with no factual alignments present. The output incorrectly associates AI and machine learning with the production of cyborgs and electric sheep, which is not supported by the context.\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Hallucination (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are multiple contradictions between the actual output and the provided context, with no factual alignments present. The output incorrectly associates AI and machine learning with the production of cyborgs and electric sheep, which is not supported by the context., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is AI?\n",
      "  - actual output: AI refers to machines mimicking human intelligence to produce cyborgs and electric sheep\n",
      "  - expected output: None\n",
      "  - context: [\"Artificial intelligence refers to machines mimicking human intelligence, like problem-solving and learning. AI includes applications like virtual assistants, robotics, and autonomous vehicles. It's evolving rapidly with advancements in machine learning and deep learning.\", 'Machine learning is a field of artificial intelligence focused on enabling systems to learn patterns from data. Algorithms analyze past data to make predictions or classify information. Popular applications include recommendation systems and image recognition.']\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Hallucination: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=response['question'],\n",
    "    actual_output=ai_response,\n",
    "    context=human_ground_truth_context\n",
    ")\n",
    "\n",
    "metric = HallucinationMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    "    verbose_mode=True\n",
    ")\n",
    "\n",
    "result = evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "921ea2dd-fbeb-4d41-adfb-22887ce1a0b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sthkQ2_F6wwW",
    "outputId": "bf71ad6e-ee28-485a-8bea-614cb9b279eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess: False\n",
      "Score: 1.0\n",
      "Reason: The score is 1.00 because there are multiple contradictions between the actual output and the provided context, with no factual alignments present. The output incorrectly associates AI and machine learning with the production of cyborgs and electric sheep, which is not supported by the context.\n"
     ]
    }
   ],
   "source": [
    "print('Sucess:', result.test_results[0].metrics_data[0].success)\n",
    "print('Score:', result.test_results[0].metrics_data[0].score)\n",
    "print('Reason:', result.test_results[0].metrics_data[0].reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f51651c-9209-4a52-9bba-b6c0889caeb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "h0-E1OjJF3ak"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "M7_Deep_Dive_into_RAG_Evaluation_Metrics",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a1982c87e18425a97c43a61c18f928b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28c6583c344341599988c3dad3070cc8",
      "placeholder": "​",
      "style": "IPY_MODEL_9d4be022d51847e8a1ccd0a9eb885de2",
      "value": "Evaluating: 100%"
     }
    },
    "28c6583c344341599988c3dad3070cc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7614ca4667f948ccba8f980a1ae4722d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8890484b6c3e4018907b96b026510712": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0a1982c87e18425a97c43a61c18f928b",
       "IPY_MODEL_d7e48662099b409baeb15177af124bb5",
       "IPY_MODEL_d3d8515a3b194fb3b1fdc820607a00bf"
      ],
      "layout": "IPY_MODEL_ce0f9b8f431246da93ebbe3c5cb8b026"
     }
    },
    "9d4be022d51847e8a1ccd0a9eb885de2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab7499442d7648168ca9415bda596e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ad467cc9f9884950ada71fbb95cfccbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5bd0a78f28749e39a63421f0da20446": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce0f9b8f431246da93ebbe3c5cb8b026": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3d8515a3b194fb3b1fdc820607a00bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad467cc9f9884950ada71fbb95cfccbd",
      "placeholder": "​",
      "style": "IPY_MODEL_c5bd0a78f28749e39a63421f0da20446",
      "value": " 1/1 [00:03&lt;00:00,  3.12s/it]"
     }
    },
    "d7e48662099b409baeb15177af124bb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7614ca4667f948ccba8f980a1ae4722d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab7499442d7648168ca9415bda596e05",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
