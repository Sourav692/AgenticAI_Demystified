{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de864c8f-ed5f-4dc2-8a18-b3809f3cb58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reference Link:** [RAG Systems Essentials (Analytics Vidhya)](https://courses.analyticsvidhya.com/courses/take/rag-systems-essentials/lessons/60148017-hands-on-deep-dive-into-rag-evaluation-metrics-generator-metrics-i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c8c2cbf-5b2b-4ff4-8d78-3a80c37fdcae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.3.10\n",
      "  Using cached langchain-0.3.10-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain==0.3.10) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain==0.3.10) (2.0.43)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain==0.3.10) (3.12.15)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.22 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain==0.3.10) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain==0.3.10) (0.3.9)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.3.10)\n",
      "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy<2,>=1.22.4 (from langchain==0.3.10)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain==0.3.10) (2.11.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain==0.3.10) (2.32.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain==0.3.10) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.20.1)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-core<0.4.0,>=0.3.22 (from langchain==0.3.10)\n",
      "  Downloading langchain_core-0.3.73-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Downloading langchain_core-0.3.71-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Downloading langchain_core-0.3.70-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached langchain_core-0.3.69-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached langchain_core-0.3.67-py3-none-any.whl.metadata (5.8 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_core-0.3.66-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached langchain_core-0.3.64-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (1.33)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (4.14.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (3.0.0)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.10)\n",
      "  Using cached langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.0)\n",
      "Requirement already satisfied: anyio in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.16.0)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.10) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.10) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.3.1)\n",
      "Using cached langchain-0.3.10-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.3.63-py3-none-any.whl (438 kB)\n",
      "Using cached langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: packaging, numpy, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "\u001b[2K  Attempting uninstall: packaging\n",
      "\u001b[2K    Found existing installation: packaging 25.0\n",
      "\u001b[2K    Uninstalling packaging-25.0:\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.3.2\n",
      "\u001b[2K    Uninstalling numpy-2.3.2:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.2\n",
      "\u001b[2K  Attempting uninstall: langsmith━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: langsmith 0.3.45━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling langsmith-0.3.45:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled langsmith-0.3.45━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: langchain-core0m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [langsmith]\n",
      "\u001b[2K    Found existing installation: langchain-core 0.3.74━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [langsmith]\n",
      "\u001b[2K    Uninstalling langchain-core-0.3.74:━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [langsmith]\n",
      "\u001b[2K      Successfully uninstalled langchain-core-0.3.74━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [langsmith]\n",
      "\u001b[2K  Attempting uninstall: langchain-text-splitters━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [langchain-core]\n",
      "\u001b[2K    Found existing installation: langchain-text-splitters 0.3.9[0m \u001b[32m3/6\u001b[0m [langchain-core]\n",
      "\u001b[2K    Uninstalling langchain-text-splitters-0.3.9:━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [langchain-core]\n",
      "\u001b[2K      Successfully uninstalled langchain-text-splitters-0.3.9━\u001b[0m \u001b[32m3/6\u001b[0m [langchain-core]\n",
      "\u001b[2K  Attempting uninstall: langchain╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [langchain-core]\n",
      "\u001b[2K    Found existing installation: langchain 0.3.27━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [langchain-core]\n",
      "\u001b[2K    Uninstalling langchain-0.3.27:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [langchain-core]\n",
      "\u001b[2K      Successfully uninstalled langchain-0.3.2790m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m5/6\u001b[0m [langchain]]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [langchain]/6\u001b[0m [langchain]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.3.25 requires langchain<1.0.0,>=0.3.25, but you have langchain 0.3.10 which is incompatible.\n",
      "langchain-community 0.3.25 requires langchain-core<1.0.0,>=0.3.65, but you have langchain-core 0.3.63 which is incompatible.\n",
      "langchain-openai 0.3.23 requires langchain-core<1.0.0,>=0.3.65, but you have langchain-core 0.3.63 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-0.3.10 langchain-core-0.3.63 langchain-text-splitters-0.3.8 langsmith-0.1.147 numpy-1.26.4 packaging-24.2\n",
      "Collecting langchain-openai==0.2.12\n",
      "  Using cached langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-openai==0.2.12) (0.3.63)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.55.3 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-openai==0.2.12) (1.100.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-openai==0.2.12) (0.11.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.1.147)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (4.14.1)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.11.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.32.5)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.0.0)\n",
      "Requirement already satisfied: anyio in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2025.7.34)\n",
      "Using cached langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n",
      "Installing collected packages: langchain-openai\n",
      "  Attempting uninstall: langchain-openai\n",
      "    Found existing installation: langchain-openai 0.3.23\n",
      "    Uninstalling langchain-openai-0.3.23:\n",
      "      Successfully uninstalled langchain-openai-0.3.23\n",
      "Successfully installed langchain-openai-0.2.12\n",
      "Collecting langchain-community==0.3.11\n",
      "  Using cached langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-community==0.3.11) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-community==0.3.11) (2.0.43)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-community==0.3.11) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-community==0.3.11) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-community==0.3.11) (0.4.1)\n",
      "Collecting langchain<0.4.0,>=0.3.11 (from langchain-community==0.3.11)\n",
      "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-community==0.3.11) (0.3.63)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-community==0.3.11) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-community==0.3.11) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-community==0.3.11) (2.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-community==0.3.11) (2.32.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-community==0.3.11) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11) (0.9.0)\n",
      "Collecting langchain-core<0.4.0,>=0.3.24 (from langchain-community==0.3.11)\n",
      "  Using cached langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11)\n",
      "  Using cached langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.11.7)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-core<0.4.0,>=0.3.24 (from langchain-community==0.3.11)\n",
      "  Using cached langchain_core-0.3.73-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain<0.4.0,>=0.3.11 (from langchain-community==0.3.11)\n",
      "  Using cached langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.3.8)\n",
      "Collecting langchain-core<0.4.0,>=0.3.24 (from langchain-community==0.3.11)\n",
      "  Using cached langchain_core-0.3.71-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached langchain_core-0.3.70-py3-none-any.whl.metadata (5.8 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_core-0.3.69-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached langchain_core-0.3.67-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached langchain_core-0.3.66-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain<0.4.0,>=0.3.11 (from langchain-community==0.3.11)\n",
      "  Using cached langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (4.14.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.0)\n",
      "Requirement already satisfied: anyio in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.11) (1.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community==0.3.11) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-community==0.3.11) (2.5.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.3.1)\n",
      "Using cached langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n",
      "Using cached langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "Installing collected packages: langchain, langchain-community\n",
      "\u001b[2K  Attempting uninstall: langchain\n",
      "\u001b[2K    Found existing installation: langchain 0.3.10\n",
      "\u001b[2K    Uninstalling langchain-0.3.10:\n",
      "\u001b[2K      Successfully uninstalled langchain-0.3.10\n",
      "\u001b[2K  Attempting uninstall: langchain-community━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [langchain]\n",
      "\u001b[2K    Found existing installation: langchain-community 0.3.250m [langchain]\n",
      "\u001b[2K    Uninstalling langchain-community-0.3.25:90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [langchain-community]\n",
      "\u001b[2K      Successfully uninstalled langchain-community-0.3.25━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [langchain-community]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [langchain-community]ngchain-community]\n",
      "\u001b[1A\u001b[2KSuccessfully installed langchain-0.3.25 langchain-community-0.3.11\n",
      "Collecting langchain-huggingface==0.1.2\n",
      "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from langchain-huggingface==0.1.2)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-huggingface==0.1.2) (0.3.63)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface==0.1.2)\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting tokenizers>=0.19.1 (from langchain-huggingface==0.1.2)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting transformers>=4.39.0 (from langchain-huggingface==0.1.2)\n",
      "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.1.147)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (4.14.1)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.11.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.32.5)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.0)\n",
      "Requirement already satisfied: anyio in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (2.5.0)\n",
      "Collecting filelock (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2) (4.67.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.23.0->langchain-huggingface==0.1.2)\n",
      "  Using cached hf_xet-1.1.8-cp37-abi3-macosx_11_0_arm64.whl.metadata (703 bytes)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
      "  Downloading torch-2.8.0-cp311-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
      "  Using cached scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
      "  Using cached scipy-1.16.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting Pillow (from sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
      "  Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from transformers>=4.39.0->langchain-huggingface==0.1.2) (2025.7.34)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.39.0->langchain-huggingface==0.1.2)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.15->langchain-huggingface==0.1.2) (1.3.1)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface==0.1.2)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m36m-:--:--\u001b[0m\n",
      "\u001b[?25hUsing cached hf_xet-1.1.8-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Downloading torch-2.8.0-cp311-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl (8.7 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.16.1-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, Pillow, networkx, MarkupSafe, joblib, hf-xet, fsspec, filelock, scikit-learn, jinja2, huggingface-hub, torch, tokenizers, transformers, sentence-transformers, langchain-huggingface\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20\u001b[0m [langchain-huggingface]ence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 Pillow-11.3.0 filelock-3.19.1 fsspec-2025.7.0 hf-xet-1.1.8 huggingface-hub-0.34.4 jinja2-3.1.6 joblib-1.5.1 langchain-huggingface-0.1.2 mpmath-1.3.0 networkx-3.5 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.4 torch-2.8.0 transformers-4.55.2\n",
      "Collecting jq==1.8.0\n",
      "  Downloading jq-1.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.0 kB)\n",
      "Downloading jq-1.8.0-cp311-cp311-macosx_11_0_arm64.whl (422 kB)\n",
      "Installing collected packages: jq\n",
      "Successfully installed jq-1.8.0\n",
      "Collecting pymupdf==1.25.1\n",
      "  Downloading pymupdf-1.25.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.1-cp39-abi3-macosx_11_0_arm64.whl (18.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.6/18.6 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "  Attempting uninstall: pymupdf\n",
      "    Found existing installation: PyMuPDF 1.26.1\n",
      "    Uninstalling PyMuPDF-1.26.1:\n",
      "      Successfully uninstalled PyMuPDF-1.26.1\n",
      "Successfully installed pymupdf-1.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.3.10\n",
    "!pip install langchain-openai==0.2.12\n",
    "!pip install langchain-community==0.3.11\n",
    "!pip install langchain-huggingface==0.1.2\n",
    "!pip install jq==1.8.0\n",
    "!pip install pymupdf==1.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1e783c9-6b6f-4574-82aa-e4666dfa7770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-chroma==0.1.4\n",
      "  Using cached langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma==0.1.4)\n",
      "  Using cached chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fastapi<1,>=0.95.2 (from langchain-chroma==0.1.4)\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1.40 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-chroma==0.1.4) (0.3.63)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-chroma==0.1.4) (1.26.4)\n",
      "Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.11.7)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached chroma_hnswlib-0.7.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (252 bytes)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.25.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.14.1)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached onnxruntime-1.22.1-cp311-cp311-macosx_13_0_universal2.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.36.0)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.57b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.36.0)\n",
      "Collecting tokenizers<=0.20.3,>=0.13.2 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached tokenizers-0.20.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached pypika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.74.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.16.1)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading mmh3-5.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.11.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (13.9.4)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi<1,>=0.95.2->langchain-chroma==0.1.4)\n",
      "  Using cached starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (0.1.147)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (2.32.5)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (1.0.0)\n",
      "Requirement already satisfied: anyio in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<0.4,>=0.1.40->langchain-chroma==0.1.4) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.34.4)\n",
      "Requirement already satisfied: filelock in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2025.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.1.8)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.40.3)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.6.1)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (6.32.0)\n",
      "Requirement already satisfied: sympy in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from opentelemetry-sdk>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.57b0)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.57b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.57b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.57b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_instrumentation-0.57b0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting opentelemetry-util-http==0.57b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading opentelemetry_util_http-0.57b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.57b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Downloading wrapt-1.17.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.57b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached asgiref-3.9.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (0.1.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.5.4)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached httptools-0.6.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.1.0)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached uvloop-0.21.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached watchfiles-1.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (15.0.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/sourav.banerjee/Documents/My Codebases/RAG_Demystified/.venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma==0.1.4) (1.3.0)\n",
      "Using cached langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
      "Using cached chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "Using cached chroma_hnswlib-0.7.6-cp311-cp311-macosx_11_0_arm64.whl (185 kB)\n",
      "Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Using cached starlette-0.47.2-py3-none-any.whl (72 kB)\n",
      "Using cached tokenizers-0.20.3-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl (498 kB)\n",
      "Downloading build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Using cached kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading mmh3-5.2.0-cp311-cp311-macosx_11_0_arm64.whl (40 kB)\n",
      "Using cached oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading onnxruntime-1.22.1-cp311-cp311-macosx_13_0_universal2.whl (34.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.57b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.57b0-py3-none-any.whl (32 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.57b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_util_http-0.57b0-py3-none-any.whl (7.6 kB)\n",
      "Using cached asgiref-3.9.1-py3-none-any.whl (23 kB)\n",
      "Downloading wrapt-1.17.3-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Using cached httptools-0.6.4-cp311-cp311-macosx_11_0_arm64.whl (103 kB)\n",
      "Using cached uvloop-0.21.0-cp311-cp311-macosx_10_9_universal2.whl (1.4 MB)\n",
      "Using cached watchfiles-1.1.0-cp311-cp311-macosx_11_0_arm64.whl (397 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pypika, flatbuffers, durationpy, wrapt, websocket-client, uvloop, uvicorn, pyproject_hooks, overrides, opentelemetry-util-http, oauthlib, mmh3, importlib-resources, humanfriendly, httptools, chroma-hnswlib, bcrypt, asgiref, watchfiles, starlette, requests-oauthlib, coloredlogs, build, tokenizers, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, chromadb, langchain-chroma\n",
      "\u001b[2K  Attempting uninstall: tokenizers\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m21/32\u001b[0m [coloredlogs]sources]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.21.4━━━━━━━━━━━━\u001b[0m \u001b[32m21/32\u001b[0m [coloredlogs]\n",
      "\u001b[2K    Uninstalling tokenizers-0.21.4:[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m21/32\u001b[0m [coloredlogs]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.21.4m━━━━━━━━━━━━━\u001b[0m \u001b[32m21/32\u001b[0m [coloredlogs]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32/32\u001b[0m [langchain-chroma][chromadb]try-instrumentation]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.55.2 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asgiref-3.9.1 bcrypt-4.3.0 build-1.3.0 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.10 fastapi-0.116.1 flatbuffers-25.2.10 httptools-0.6.4 humanfriendly-10.0 importlib-resources-6.5.2 kubernetes-33.1.0 langchain-chroma-0.1.4 mmh3-5.2.0 oauthlib-3.3.1 onnxruntime-1.22.1 opentelemetry-instrumentation-0.57b0 opentelemetry-instrumentation-asgi-0.57b0 opentelemetry-instrumentation-fastapi-0.57b0 opentelemetry-util-http-0.57b0 overrides-7.7.0 pypika-0.48.9 pyproject_hooks-1.2.0 requests-oauthlib-2.0.0 starlette-0.47.2 tokenizers-0.20.3 uvicorn-0.35.0 uvloop-0.21.0 watchfiles-1.1.0 websocket-client-1.8.0 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-chroma==0.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9921df9e-d29d-4058-a48b-2c81fb3babde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb7e5933-d075-428c-a82c-4b680b468d56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21252d26-eab7-410f-b4a2-5f5a3ca711f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56f97cc5-0857-4179-8ff1-507dfa9a95fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cfba87c-6d53-4c60-a193-4f8e802a1a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a chat prompt\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "def generate_chunk_context(document, chunk):\n",
    "\n",
    "    chunk_process_prompt = \"\"\"You are an AI assistant specializing in research paper analysis.\n",
    "                            Your task is to provide brief, relevant context for a chunk of text\n",
    "                            based on the following research paper.\n",
    "\n",
    "                            Here is the research paper:\n",
    "                            <paper>\n",
    "                            {paper}\n",
    "                            </paper>\n",
    "\n",
    "                            Here is the chunk we want to situate within the whole document:\n",
    "                            <chunk>\n",
    "                            {chunk}\n",
    "                            </chunk>\n",
    "\n",
    "                            Provide a concise context (3-4 sentences max) for this chunk,\n",
    "                            considering the following guidelines:\n",
    "\n",
    "                            - Give a short succinct context to situate this chunk within the overall document\n",
    "                            for the purposes of improving search retrieval of the chunk.\n",
    "                            - Answer only with the succinct context and nothing else.\n",
    "                            - Context should be mentioned like 'Focuses on ....'\n",
    "                            do not mention 'this chunk or section focuses on...'\n",
    "\n",
    "                            Context:\n",
    "                        \"\"\"\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(chunk_process_prompt)\n",
    "\n",
    "    agentic_chunk_chain = (prompt_template\n",
    "                                |\n",
    "                            chatgpt\n",
    "                                |\n",
    "                            StrOutputParser())\n",
    "\n",
    "    context = agentic_chunk_chain.invoke({'paper': document, 'chunk': chunk})\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dea4251d-9fd1-4bfb-ba9d-2280cfdc4385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "def create_contextual_chunks(file_path):\n",
    "\n",
    "    print('Loading pages:', file_path)\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    doc_pages = loader.load()\n",
    "\n",
    "    print('Chunking pages:', file_path)\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=3500,\n",
    "                                              chunk_overlap=0)\n",
    "    doc_chunks = splitter.split_documents(doc_pages)\n",
    "\n",
    "    print('Generating contextual chunks:', file_path)\n",
    "    original_doc = '\\n'.join([doc.page_content for doc in doc_chunks])\n",
    "    contextual_chunks = []\n",
    "    for chunk in doc_chunks:\n",
    "        context = generate_chunk_context(original_doc, chunk.page_content)\n",
    "        contextual_chunks.append(Document(page_content=context+'\\n'+chunk.page_content,\n",
    "                                          metadata=chunk.metadata))\n",
    "    print('Finished processing:', file_path)\n",
    "    print()\n",
    "    return contextual_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def create_simple_chunks(file_path, chunk_size=3500, chunk_overlap=0):\n",
    "\n",
    "    print('Loading pages:', file_path)\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    doc_pages = loader.load()\n",
    "\n",
    "    print('Chunking pages:', file_path)\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                              chunk_overlap=chunk_overlap)\n",
    "    doc_chunks = splitter.split_documents(doc_pages)\n",
    "    print('Finished processing:', file_path)\n",
    "    print()\n",
    "    return doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01e5bb5a-dfc7-41df-9177-3e138ef4795c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/2005.11401v4.pdf',\n",
       " './data/2005.14165v4.pdf',\n",
       " './data/1706.03762v7.pdf']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "pdf_files = glob('./data/*.pdf')\n",
    "pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pages: ./data/2005.11401v4.pdf\n",
      "Chunking pages: ./data/2005.11401v4.pdf\n",
      "Finished processing: ./data/2005.11401v4.pdf\n",
      "\n",
      "Loading pages: ./data/2005.14165v4.pdf\n",
      "Chunking pages: ./data/2005.14165v4.pdf\n",
      "Finished processing: ./data/2005.14165v4.pdf\n",
      "\n",
      "Loading pages: ./data/1706.03762v7.pdf\n",
      "Chunking pages: ./data/1706.03762v7.pdf\n",
      "Finished processing: ./data/1706.03762v7.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paper_docs = []\n",
    "for fp in pdf_files:\n",
    "    paper_docs.extend(create_simple_chunks(file_path=fp,\n",
    "                                           chunk_size=3500,\n",
    "                                           chunk_overlap=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f05da8f9-d50f-471f-8582-72432cdb0db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# paper_docs = []\n",
    "# for fp in pdf_files:\n",
    "#     paper_docs.extend(create_contextual_chunks(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7af3273-75a8-46d7-ae88-cdd89b11a886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# create vector DB of docs and embeddings - takes < 30s on Colab\n",
    "chroma_db = Chroma.from_documents(documents=paper_docs,\n",
    "                                  collection_name='final_project',\n",
    "                                  embedding=openai_embed_model,\n",
    "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
    "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
    "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                                  persist_directory=\"./final_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "005d0083-3551-4c79-8f74-1a871f14e759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load from disk\n",
    "chroma_db = Chroma(persist_directory=\"./final_project\",\n",
    "                   collection_name='final_project',\n",
    "                   embedding_function=openai_embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6c76bc4-52e7-453a-882f-f8f5f4837300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "480731ed-a7d3-47ce-a7a1-42d2d0bf7dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def display_docs(docs):\n",
    "    for doc in docs:\n",
    "        print('Metadata:', doc.metadata)\n",
    "        print('Content Brief:')\n",
    "        display(Markdown(doc.page_content[:1000]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbc66ccb-2129-4e02-a156-18538240edce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 3, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "loop of meta-learning. We further specialize the description to “zero-shot”, “one-shot”, or “few-shot” depending on how many\n",
       "demonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model\n",
       "learns new tasks from scratch at inference time or simply recognizes patterns seen during training – this is an important issue which\n",
       "we discuss later in the paper, but “meta-learning” is intended to encompass both possibilities, and simply describes the inner-outer\n",
       "loop structure.\n",
       "4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 3, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Figure 1.2: Larger models make increasingly efﬁcient use of in-context information. We show in-context learning\n",
       "performance on a simple task requiring the model to remove random symbols from a word, both with and without a\n",
       "natural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate\n",
       "improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range\n",
       "of tasks.\n",
       "sufﬁcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing\n",
       "to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans\n",
       "to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy\n",
       "dialogue. To be broadly useful, we would someday like our NLP systems to have this same ﬂuidity and generality.\n",
       "One potential route towards addressing these issues is meta"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 2, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1\n",
       "Introduction\n",
       "Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly\n",
       "ﬂexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word\n",
       "vectors [MCCD13, PSM14] and fed to task-speciﬁc architectures, then RNNs with multiple layers of representations\n",
       "and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to\n",
       "task-speciﬁc architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have\n",
       "been directly ﬁne-tuned, entirely removing the need for task-speciﬁc architectures [RNSS18, DCLT18, HR18].\n",
       "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension,\n",
       "question answering, textual entailment, and many others, and has continued to advance based on new architectures\n",
       "and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 2, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "the desired task. We use the term “in-context learning” to describe the inner loop of this process, which occurs within\n",
       "the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a\n",
       "model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded\n",
       "within a single sequence.\n",
       "3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 39, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "task-speciﬁc [SDCW19, JYS+19, KR16] approaches to distillation of language models. These architectures and\n",
       "techniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint\n",
       "of giant models.\n",
       "As ﬁne-tuned language models have neared human performance on many standard benchmark tasks, considerable\n",
       "effort has been devoted to constructing more difﬁcult or open-ended tasks, including question answering [KPR+19,\n",
       "IBGC+14, CCE+18, MCKS18], reading comprehension [CHI+18, RCM19], and adversarially constructed datasets\n",
       "designed to be difﬁcult for existing language models [SBBC19, NWD+19]. In this work we test our models on many\n",
       "of these datasets.\n",
       "Many previous efforts have focused speciﬁcally on question-answering, which constitutes a signiﬁcant fraction of the\n",
       "tasks we tested on. Recent efforts include [RSR+19, RRS20], which ﬁne-tuned an 11 billion parameter language model,\n",
       "and [GLT+20], which focused on attending over a large corpus of data at te"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is machine learning?\"\n",
    "top_docs = similarity_retriever.invoke(query)\n",
    "display_docs(top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8843c863-a304-4073-9670-aa5ceeb909ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b40d364-0505-4169-8635-f4b0397509ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
    "                                              search_kwargs={\"k\": 5})\n",
    "\n",
    "mq_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=similarity_retriever, llm=chatgpt\n",
    ")\n",
    "\n",
    "logging.basicConfig()\n",
    "# so we can see what queries are generated by the LLM\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82d55f3c-c180-4dc6-914d-cb8aa3d2d3fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What does CNN stand for and what are its main functions?  ', 'Can you explain the concept and applications of convolutional neural networks?  ', 'What are the key features and uses of CNNs in machine learning?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'author': '', 'creationDate': 'D:20240410211143Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/1706.03762v7.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240410211143Z', 'page': 5, 'producer': 'pdfTeX-1.40.25', 'source': './data/1706.03762v7.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
       "for different layer types. n is the sequence length, d is the representation dimension, k is the kernel\n",
       "size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
       "Layer Type\n",
       "Complexity per Layer\n",
       "Sequential\n",
       "Maximum Path Length\n",
       "Operations\n",
       "Self-Attention\n",
       "O(n2 · d)\n",
       "O(1)\n",
       "O(1)\n",
       "Recurrent\n",
       "O(n · d2)\n",
       "O(n)\n",
       "O(n)\n",
       "Convolutional\n",
       "O(k · n · d2)\n",
       "O(1)\n",
       "O(logk(n))\n",
       "Self-Attention (restricted)\n",
       "O(r · n · d)\n",
       "O(1)\n",
       "O(n/r)\n",
       "3.5\n",
       "Positional Encoding\n",
       "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
       "order of the sequence, we must inject some information about the relative or absolute position of the\n",
       "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
       "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
       "as the embeddings, so that the two can be summed. Ther"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20240410211143Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/1706.03762v7.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240410211143Z', 'page': 2, 'producer': 'pdfTeX-1.40.25', 'source': './data/1706.03762v7.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Figure 1: The Transformer - model architecture.\n",
       "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
       "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
       "respectively.\n",
       "3.1\n",
       "Encoder and Decoder Stacks\n",
       "Encoder:\n",
       "The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
       "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
       "wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
       "the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\n",
       "LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
       "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
       "layers, produce outputs of dimension dmodel = 512.\n",
       "Decoder:\n",
       "The decoder is also composed of a stack of N = 6 identical layers. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20240410211143Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/1706.03762v7.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240410211143Z', 'page': 1, 'producer': 'pdfTeX-1.40.25', 'source': './data/1706.03762v7.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1\n",
       "Introduction\n",
       "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
       "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
       "transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\n",
       "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
       "architectures [38, 24, 15].\n",
       "Recurrent models typically factor computation along the symbol positions of the input and output\n",
       "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
       "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
       "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
       "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
       "significant improvements in computational efficiency through factor"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20240410211143Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/1706.03762v7.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240410211143Z', 'page': 4, 'producer': 'pdfTeX-1.40.25', 'source': './data/1706.03762v7.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "output values. These are concatenated and once again projected, resulting in the final values, as\n",
       "depicted in Figure 2.\n",
       "Multi-head attention allows the model to jointly attend to information from different representation\n",
       "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
       "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\n",
       "where headi = Attention(QW Q\n",
       "i , KW K\n",
       "i , V W V\n",
       "i )\n",
       "Where the projections are parameter matrices W Q\n",
       "i\n",
       "∈Rdmodel×dk, W K\n",
       "i\n",
       "∈Rdmodel×dk, W V\n",
       "i\n",
       "∈Rdmodel×dv\n",
       "and W O ∈Rhdv×dmodel.\n",
       "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
       "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
       "is similar to that of single-head attention with full dimensionality.\n",
       "3.2.3\n",
       "Applications of Attention in our Model\n",
       "The Transformer uses multi-head attention in three different ways:\n",
       "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
       "and"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20240410211143Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/1706.03762v7.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240410211143Z', 'page': 1, 'producer': 'pdfTeX-1.40.25', 'source': './data/1706.03762v7.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "language modeling tasks [34].\n",
       "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
       "entirely on self-attention to compute representations of its input and output without using sequence-\n",
       "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
       "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
       "3\n",
       "Model Architecture\n",
       "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
       "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
       "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
       "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
       "[10], consuming the previously generated symbols as additional input when generating the next.\n",
       "2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 3, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Figure 1.2: Larger models make increasingly efﬁcient use of in-context information. We show in-context learning\n",
       "performance on a simple task requiring the model to remove random symbols from a word, both with and without a\n",
       "natural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate\n",
       "improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range\n",
       "of tasks.\n",
       "sufﬁcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing\n",
       "to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans\n",
       "to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy\n",
       "dialogue. To be broadly useful, we would someday like our NLP systems to have this same ﬂuidity and generality.\n",
       "One potential route towards addressing these issues is meta"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 2, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1\n",
       "Introduction\n",
       "Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly\n",
       "ﬂexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word\n",
       "vectors [MCCD13, PSM14] and fed to task-speciﬁc architectures, then RNNs with multiple layers of representations\n",
       "and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to\n",
       "task-speciﬁc architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have\n",
       "been directly ﬁne-tuned, entirely removing the need for task-speciﬁc architectures [RNSS18, DCLT18, HR18].\n",
       "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension,\n",
       "question answering, textual entailment, and many others, and has continued to advance based on new architectures\n",
       "and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is a cnn?\"\n",
    "top_docs = mq_retriever.invoke(query)\n",
    "display_docs(top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6217aab-558f-4855-8c7b-008b36b6c0b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What does NLP stand for and what are its main applications?  ', 'Can you explain the concept of natural language processing and its significance?  ', 'What are the key components and techniques involved in NLP?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 2, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1\n",
       "Introduction\n",
       "Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly\n",
       "ﬂexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word\n",
       "vectors [MCCD13, PSM14] and fed to task-speciﬁc architectures, then RNNs with multiple layers of representations\n",
       "and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to\n",
       "task-speciﬁc architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have\n",
       "been directly ﬁne-tuned, entirely removing the need for task-speciﬁc architectures [RNSS18, DCLT18, HR18].\n",
       "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension,\n",
       "question answering, textual entailment, and many others, and has continued to advance based on new architectures\n",
       "and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 70, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\n",
       "cut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint\n",
       "arXiv:1909.11942, 2019.\n",
       "[LCH+20] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao.\n",
       "Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994, 2020.\n",
       "[LDL19] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert. arXiv preprint\n",
       "arXiv:1905.07504, 2019.\n",
       "[LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth\n",
       "International Conference on the Principles of Knowledge Representation and Reasoning, 2012.\n",
       "[LGG+20] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and\n",
       "Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. arXiv preprint\n",
       "arXiv:2001.08210, 2020.\n",
       "[LGH+15] Xiaodong L"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 3, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Figure 1.2: Larger models make increasingly efﬁcient use of in-context information. We show in-context learning\n",
       "performance on a simple task requiring the model to remove random symbols from a word, both with and without a\n",
       "natural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate\n",
       "improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range\n",
       "of tasks.\n",
       "sufﬁcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing\n",
       "to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans\n",
       "to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy\n",
       "dialogue. To be broadly useful, we would someday like our NLP systems to have this same ﬂuidity and generality.\n",
       "One potential route towards addressing these issues is meta"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 69, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural ques-\n",
       "tions: a benchmark for question answering research. Transactions of the Association of Computational\n",
       "Linguistics, 2019.\n",
       "[KR16] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. Arxiv, 2016.\n",
       "[LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002.\n",
       "[LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint\n",
       "arXiv:1901.07291, 2019.\n",
       "70"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20240410211143Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/1706.03762v7.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240410211143Z', 'page': 11, 'producer': 'pdfTeX-1.40.25', 'source': './data/1706.03762v7.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
       "corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n",
       "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
       "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,\n",
       "pages 152–159. ACL, June 2006.\n",
       "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
       "model. In Empirical Methods in Natural Language Processing, 2016.\n",
       "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
       "summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
       "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
       "and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
       "Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n",
       "2006.\n",
       "[30] Ofir Press "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/2005.11401v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 13, 'producer': 'pdfTeX-1.40.21', 'source': './data/2005.11401v4.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\n",
       "Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop\n",
       "Proceedings. CEUR-WS.org, 2016.\n",
       "URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\n",
       "2016_paper9.pdf.\n",
       "[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\n",
       "arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\n",
       "[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\n",
       "and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\n",
       "of the 2019 Conference of the North American Chapter of the Association for Computational\n",
       "Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association\n",
       "for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\n",
       "org/anthology/N19-4009.\n",
       "[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun\n",
       "Cho. Finding gen"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/2005.11401v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 12, 'producer': 'pdfTeX-1.40.21', 'source': './data/2005.11401v4.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for\n",
       "Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\n",
       "anthology/P19-1612.\n",
       "[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\n",
       "Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\n",
       "pre-training for natural language generation, translation, and comprehension. arXiv preprint\n",
       "arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\n",
       "[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\n",
       "objective function for neural conversation models. In Proceedings of the 2016 Conference of the\n",
       "North American Chapter of the Association for Computational Linguistics: Human Language\n",
       "Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational\n",
       "Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\n",
       "N16-1014.\n",
       "[34] Margaret L"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/2005.11401v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 10, 'producer': 'pdfTeX-1.40.21', 'source': './data/2005.11401v4.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\n",
       "hension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.\n",
       "arXiv: 1710.10723.\n",
       "[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\n",
       "Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\n",
       "ference of the North American Chapter of the Association for Computational Linguistics: Human\n",
       "Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,\n",
       "Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\n",
       "URL https://www.aclweb.org/anthology/N19-1423.\n",
       "[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\n",
       "ard of wikipedia: Knowledge-powered conversational agents. In International Conference on\n",
       "Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\n",
       "[10] Matthew Dunn, Levent Sagun, Mi"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is nlp?\"\n",
    "top_docs = mq_retriever.invoke(query)\n",
    "display_docs(top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What does NLP stand for and what are its main applications?  ', 'Can you explain the concept of natural language processing and its significance?  ', 'What are the key components and techniques involved in NLP?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 2, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1\n",
       "Introduction\n",
       "Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly\n",
       "ﬂexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word\n",
       "vectors [MCCD13, PSM14] and fed to task-speciﬁc architectures, then RNNs with multiple layers of representations\n",
       "and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to\n",
       "task-speciﬁc architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have\n",
       "been directly ﬁne-tuned, entirely removing the need for task-speciﬁc architectures [RNSS18, DCLT18, HR18].\n",
       "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension,\n",
       "question answering, textual entailment, and many others, and has continued to advance based on new architectures\n",
       "and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 70, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\n",
       "cut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint\n",
       "arXiv:1909.11942, 2019.\n",
       "[LCH+20] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao.\n",
       "Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994, 2020.\n",
       "[LDL19] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert. arXiv preprint\n",
       "arXiv:1905.07504, 2019.\n",
       "[LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth\n",
       "International Conference on the Principles of Knowledge Representation and Reasoning, 2012.\n",
       "[LGG+20] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and\n",
       "Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. arXiv preprint\n",
       "arXiv:2001.08210, 2020.\n",
       "[LGH+15] Xiaodong L"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 3, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Figure 1.2: Larger models make increasingly efﬁcient use of in-context information. We show in-context learning\n",
       "performance on a simple task requiring the model to remove random symbols from a word, both with and without a\n",
       "natural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate\n",
       "improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range\n",
       "of tasks.\n",
       "sufﬁcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing\n",
       "to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans\n",
       "to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy\n",
       "dialogue. To be broadly useful, we would someday like our NLP systems to have this same ﬂuidity and generality.\n",
       "One potential route towards addressing these issues is meta"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20200724000408Z', 'creator': 'LaTeX with hyperref package', 'file_path': './data/2005.14165v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20200724000408Z', 'page': 69, 'producer': 'pdfTeX-1.40.17', 'source': './data/2005.14165v4.pdf', 'subject': '', 'title': '', 'total_pages': 75, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural ques-\n",
       "tions: a benchmark for question answering research. Transactions of the Association of Computational\n",
       "Linguistics, 2019.\n",
       "[KR16] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. Arxiv, 2016.\n",
       "[LB02] Edward Loper and Steven Bird. Nltk: The natural language toolkit, 2002.\n",
       "[LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint\n",
       "arXiv:1901.07291, 2019.\n",
       "70"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20240410211143Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/1706.03762v7.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240410211143Z', 'page': 11, 'producer': 'pdfTeX-1.40.25', 'source': './data/1706.03762v7.pdf', 'subject': '', 'title': '', 'total_pages': 15, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
       "corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n",
       "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
       "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,\n",
       "pages 152–159. ACL, June 2006.\n",
       "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
       "model. In Empirical Methods in Natural Language Processing, 2016.\n",
       "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
       "summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
       "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
       "and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
       "Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n",
       "2006.\n",
       "[30] Ofir Press "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/2005.11401v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 13, 'producer': 'pdfTeX-1.40.21', 'source': './data/2005.11401v4.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\n",
       "Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop\n",
       "Proceedings. CEUR-WS.org, 2016.\n",
       "URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\n",
       "2016_paper9.pdf.\n",
       "[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\n",
       "arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.\n",
       "[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\n",
       "and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\n",
       "of the 2019 Conference of the North American Chapter of the Association for Computational\n",
       "Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association\n",
       "for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\n",
       "org/anthology/N19-4009.\n",
       "[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun\n",
       "Cho. Finding gen"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/2005.11401v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 12, 'producer': 'pdfTeX-1.40.21', 'source': './data/2005.11401v4.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for\n",
       "Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\n",
       "anthology/P19-1612.\n",
       "[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\n",
       "Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\n",
       "pre-training for natural language generation, translation, and comprehension. arXiv preprint\n",
       "arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.\n",
       "[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\n",
       "objective function for neural conversation models. In Proceedings of the 2016 Conference of the\n",
       "North American Chapter of the Association for Computational Linguistics: Human Language\n",
       "Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational\n",
       "Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\n",
       "N16-1014.\n",
       "[34] Margaret L"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata: {'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': './data/2005.11401v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 10, 'producer': 'pdfTeX-1.40.21', 'source': './data/2005.11401v4.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "Content Brief:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\n",
       "hension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.\n",
       "arXiv: 1710.10723.\n",
       "[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\n",
       "Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\n",
       "ference of the North American Chapter of the Association for Computational Linguistics: Human\n",
       "Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,\n",
       "Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\n",
       "URL https://www.aclweb.org/anthology/N19-1423.\n",
       "[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\n",
       "ard of wikipedia: Knowledge-powered conversational agents. In International Conference on\n",
       "Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.\n",
       "[10] Matthew Dunn, Levent Sagun, Mi"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is nlp?\"\n",
    "top_docs = mq_retriever.invoke(query)\n",
    "display_docs(top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_prompt = \"\"\"You are an assistant who is an expert in question-answering tasks.\n",
    "                Answer the following question using only the following pieces of retrieved context.\n",
    "                If the answer is not in the context, do not make up answers, just say that you don't know.\n",
    "                Keep the answer detailed and well formatted based on the information from the context.\n",
    "\n",
    "                Question:\n",
    "                {question}\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Answer:\n",
    "            \"\"\"\n",
    "\n",
    "rag_prompt_template = ChatPromptTemplate.from_template(rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_rag_chain = (\n",
    "    {\n",
    "        \"context\": (mq_retriever\n",
    "                      |\n",
    "                    format_docs),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "      |\n",
    "    rag_prompt_template\n",
    "      |\n",
    "    chatgpt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What are the key concepts and definitions of machine learning?  ', 'Can you explain the fundamentals and applications of machine learning?  ', 'How does machine learning work and what are its main types?']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Machine learning is not explicitly defined in the provided context. However, the context discusses concepts related to machine learning, particularly in the realm of natural language processing (NLP) and meta-learning. \n",
       "\n",
       "From the context, we can infer that machine learning involves the development of models that can learn from data and adapt to new tasks. Specifically, it mentions \"meta-learning,\" which refers to a process where a model develops a broad set of skills and pattern recognition abilities during training and then uses these abilities at inference time to rapidly adapt to or recognize the desired task. This process can involve different learning paradigms such as \"zero-shot,\" \"one-shot,\" or \"few-shot\" learning, which describe how many demonstrations are provided to the model at inference time.\n",
       "\n",
       "The context also highlights the importance of pre-trained language models and their ability to perform various NLP tasks without needing extensive task-specific datasets, which is a significant aspect of modern machine learning approaches. \n",
       "\n",
       "In summary, while the term \"machine learning\" itself is not defined in the context, it is closely related to the concepts of model training, adaptation, and the use of learned skills to perform tasks, as discussed in the context of meta-learning and language models. \n",
       "\n",
       "If you need a more specific definition or details about machine learning, I don't know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "query = \"What is machine learning?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What are the key elements of a RAG model and what is their interaction process?', 'Can you explain the primary parts of a RAG model and how they work together?', 'What are the essential features of a RAG model and how do they connect with each other?']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The main components of a RAG (Retrieval-Augmented Generation) model are:\n",
       "\n",
       "1. **Retriever**: This component, denoted as \\( p_{\\eta}(z|x) \\), is responsible for retrieving relevant text documents based on a given input query \\( x \\). It returns a distribution over the top-K truncated text passages that are deemed relevant to the query. The retriever is initialized using the Dense Passage Retrieval (DPR) method, which employs retrieval supervision on datasets like Natural Questions and TriviaQA.\n",
       "\n",
       "2. **Generator**: This component, represented as \\( p_{\\theta}(y_i|x, z, y_{1:i-1}) \\), generates the target sequence \\( y \\) using the input query \\( x \\) and the retrieved documents \\( z \\) as additional context. The generator is designed to produce free-form, abstractive text responses, allowing it to generate answers that may not be verbatim from the retrieved documents.\n",
       "\n",
       "### Interaction Between Components:\n",
       "- The interaction between the retriever and the generator is crucial for the RAG model's performance. The retriever first processes the input query to fetch relevant documents, which are then used by the generator to create a response. This allows the model to leverage both the generative capabilities of the generator and the contextual information provided by the retrieved documents.\n",
       "- The model demonstrates that it can achieve state-of-the-art results in open-domain question answering without relying on traditional extractive methods, as it can generate answers based on clues from the documents even if the exact answer is not present verbatim.\n",
       "\n",
       "Overall, the RAG model effectively combines retrieval and generation, allowing for more flexible and accurate responses in various knowledge-intensive NLP tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "query = \"What are the main components of a RAG model and how do they interact?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "display(Markdown(result.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "RAG system for Question Answering",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
