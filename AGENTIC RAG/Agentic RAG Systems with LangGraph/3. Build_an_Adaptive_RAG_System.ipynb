{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bghaot7QKkHQ"
      },
      "source": [
        "# Building an Adaptive RAG System with LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlrCWa9jKulS"
      },
      "source": [
        "This project will cover a full hands-on workflow and demonstration of how to build an Agentic Adaptive RAG System with LangGraph\n",
        "\n",
        "The idea would be to implement the workflow taking inspiration from the [Adaptive-RAG](https://arxiv.org/pdf/2403.14403) research paper.\n",
        "\n",
        "The main challenge of RAG systems include:\n",
        "\n",
        "- Poor Retrieval can lead to issues in LLM response generation\n",
        "- Bad retrieval or lack of information in the vector database can also lead to out of context or hallucinated answers\n",
        "- Even complex RAG Systems like CRAG suffer from the fact that they are fixed flows which will always be executed regardless of query complexity\n",
        "\n",
        "The idea is to build a dynamic agentic RAG system which can adapt based on the input query and route it to the best possible RAG workflow to handle it. In our case we will take a user query and route it either to be handled by the:\n",
        "- Web Search based RAG Flow\n",
        "- Vector DB based RAG flow\n",
        "\n",
        "We will also add in elements of Corrective RAG and Self-Reflective RAG here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIfUG-flaSMB"
      },
      "source": [
        "We can build this as an agentic RAG system by having a specific functionality step as a node in the graph and use LangGraph to implement it. Key steps in the node will include prompts being sent to LLMs, tools, DBs to perform specific tasks as seen in the detailed workflow below:\n",
        "\n",
        "![](https://i.imgur.com/ESG2Jc7.png)\n",
        "\n",
        "\n",
        "\n",
        "### Adaptive RAG System with LangGraph\n",
        "\n",
        "This project implements an **Adaptive RAG System** using LangGraph, designed to dynamically route user queries through the most suitable Retrieval-Augmented Generation (RAG) workflow. It supports both **Vector DB-based** and **Web Search-based** retrieval paths and incorporates elements from **Corrective RAG** (document grading) and **Self-Reflective RAG** (hallucination detection and feedback correction).\n",
        "\n",
        "The adaptive workflow includes the following key components:\n",
        "\n",
        "1. **Dynamic Query Routing**:\n",
        "   - A **Query Router Prompt** classifies the user query and decides whether the information should be retrieved from:\n",
        "     - An internal **Vector Database**, or\n",
        "     - A **Web Search** engine.\n",
        "   - Based on the routing decision, the query is rephrased using:\n",
        "     - **VectorDB Rephrase Prompt** for vector-optimized semantic search, or\n",
        "     - **Web Search Rephrase Prompt** for better recall from web sources.\n",
        "\n",
        "2. **Retrieval and Context Preparation**:\n",
        "   - **If the source is Vector DB**:\n",
        "     - Rephrased query is used to retrieve documents from the **Vector Database**.\n",
        "     - Retrieved documents are passed through an **LLM Grader Prompt**, which filters relevant documents using semantic grading (`yes` or `no`).\n",
        "     - If **more than 50%** of documents are relevant, they are forwarded to the next stage.\n",
        "     - If **50% or fewer** documents are relevant, the system switches to the **Web Search** path.\n",
        "     - Any irrelevant documents are removed.\n",
        "\n",
        "   - **If the source is Web Search** (or >=50% docs irrelevant from Vector DB route):\n",
        "     - The rephrased query is used directly to search the web and retrieve external context documents.\n",
        "\n",
        "3. **Answer Generation (RAG Prompt)**:\n",
        "   - Final relevant documents — either from the Vector DB (filtered) and / or Web Search (raw) — are used in the **RAG Prompt**.\n",
        "   - The prompt instructs the LLM to answer only using the provided context.\n",
        "   - If feedback exists (from a prior iteration in case there were hallucinations), it is also included in the prompt to improve the response.\n",
        "\n",
        "4. **Self-Reflective Hallucination Check**:\n",
        "   - The generated answer undergoes evaluation using a **Hallucination Check Prompt**.\n",
        "   - The LLM checks if the answer is grounded in the context documents and returns:\n",
        "     - A **hallucination flag** (`yes` or `no`)\n",
        "     - Optional **feedback** explaining the judgment.\n",
        "\n",
        "5. **Feedback Loop for Regeneration**:\n",
        "   - If the hallucination flag is `no`, the answer is finalized.\n",
        "   - If it is `yes`, the feedback is used to regenerate the response using the same context via the **RAG Prompt**, making the system more reliable and self-correcting.\n",
        "\n",
        "This **Adaptive RAG System** combines adaptive routing, corrective RAG, and self-reflective RAG elements to produce accurate, high-quality answers.\n",
        "\n",
        "___Created By: [Dipanjan (DJ)](https://www.linkedin.com/in/dipanjans/)___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEZgavRjpUy2"
      },
      "source": [
        "## Install OpenAI, Tavily, LangGraph and LangChain dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5Z0XUbPaI6S",
        "outputId": "b4896678-ebf5-405e-98d2-b64f9aec7ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain==0.3.20\n",
            "  Using cached langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain==0.3.20) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain==0.3.20) (0.3.9)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain==0.3.20) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain==0.3.20) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain==0.3.20) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain==0.3.20) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain==0.3.20) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain==0.3.20) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain==0.3.20) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain==0.3.20) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain==0.3.20) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain==0.3.20) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.20) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.20) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.20) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.20) (0.23.0)\n",
            "Requirement already satisfied: anyio in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.20) (4.9.0)\n",
            "Requirement already satisfied: certifi in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.20) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.20) (1.0.9)\n",
            "Requirement already satisfied: idna in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.20) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.20) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.20) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.20) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.20) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests<3,>=2->langchain==0.3.20) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests<3,>=2->langchain==0.3.20) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.20) (3.2.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.20) (1.3.1)\n",
            "Using cached langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
            "Installing collected packages: langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "Successfully installed langchain-0.3.20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.3.20 requires langchain<1.0.0,>=0.3.21, but you have langchain 0.3.20 which is incompatible.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-openai==0.3.9 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (0.3.9)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-openai==0.3.9) (0.3.72)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.66.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-openai==0.3.9) (1.99.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-openai==0.3.9) (0.10.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (2.11.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (3.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (0.10.0)\n",
            "Requirement already satisfied: sniffio in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (3.10)\n",
            "Requirement already satisfied: certifi in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.9) (2025.7.34)\n",
            "Requirement already satisfied: requests>=2.26.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.9) (2.32.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (0.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.9) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.9) (2.5.0)\n",
            "Requirement already satisfied: colorama in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (0.4.6)\n",
            "Requirement already satisfied: langchain-community==0.3.20 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (0.3.20)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-community==0.3.20) (0.3.72)\n",
            "Collecting langchain<1.0.0,>=0.3.21 (from langchain-community==0.3.20)\n",
            "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-community==0.3.20) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-community==0.3.20) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-community==0.3.20) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-community==0.3.20) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-community==0.3.20) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-community==0.3.20) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-community==0.3.20) (2.10.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-community==0.3.20) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-community==0.3.20) (0.4.1)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-community==0.3.20) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.20) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.20) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.21->langchain-community==0.3.20) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.21->langchain-community==0.3.20) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community==0.3.20) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community==0.3.20) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community==0.3.20) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community==0.3.20) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (0.23.0)\n",
            "Requirement already satisfied: anyio in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (4.9.0)\n",
            "Requirement already satisfied: certifi in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (1.0.9)\n",
            "Requirement already satisfied: idna in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community==0.3.20) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community==0.3.20) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community==0.3.20) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.20) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-community==0.3.20) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-community==0.3.20) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.20) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.20) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (1.3.1)\n",
            "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "Installing collected packages: langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.20\n",
            "    Uninstalling langchain-0.3.20:\n",
            "      Successfully uninstalled langchain-0.3.20\n",
            "Successfully installed langchain-0.3.27\n",
            "Requirement already satisfied: langgraph==0.3.18 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (0.3.18)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langgraph==0.3.18) (0.3.72)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langgraph==0.3.18) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langgraph==0.3.18) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langgraph==0.3.18) (0.1.74)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (2.11.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph==0.3.18) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph==0.3.18) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (3.11.1)\n",
            "Requirement already satisfied: anyio in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (4.9.0)\n",
            "Requirement already satisfied: certifi in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (1.0.9)\n",
            "Requirement already satisfied: idna in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (0.16.0)\n",
            "Requirement already satisfied: requests<3,>=2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.1->langgraph==0.3.18) (2.32.4)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.1->langgraph==0.3.18) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.1->langgraph==0.3.18) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.1->langgraph==0.3.18) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.1->langgraph==0.3.18) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.1->langgraph==0.3.18) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<0.4,>=0.1->langgraph==0.3.18) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<0.4,>=0.1->langgraph==0.3.18) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (1.3.1)\n",
            "Requirement already satisfied: langchain-tavily==0.1.5 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (0.1.5)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-tavily==0.1.5) (3.12.15)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.20 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-tavily==0.1.5) (0.3.27)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-tavily==0.1.5) (0.3.72)\n",
            "Requirement already satisfied: mypy<2.0.0,>=1.15.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-tavily==0.1.5) (1.17.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (1.20.1)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily==0.1.5) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily==0.1.5) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily==0.1.5) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily==0.1.5) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-tavily==0.1.5) (3.0.0)\n",
            "Requirement already satisfied: mypy_extensions>=1.0.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from mypy<2.0.0,>=1.15.0->langchain-tavily==0.1.5) (1.1.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from mypy<2.0.0,>=1.15.0->langchain-tavily==0.1.5) (0.12.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (3.2.3)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.23.0)\n",
            "Requirement already satisfied: anyio in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.3.20\n",
        "!pip install langchain-openai==0.3.9\n",
        "!pip install langchain-community==0.3.20\n",
        "!pip install langgraph==0.3.18\n",
        "!pip install langchain-tavily==0.1.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s38ofVuySJCr"
      },
      "source": [
        "## Install PyMuPDF for loading PDF documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HIvpdTHSovm",
        "outputId": "888e80f4-9d20-42fd-bc63-327e2bf81dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymupdf==1.25.4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (1.25.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf==1.25.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCu8VZzeSN5A"
      },
      "source": [
        "## Install ChromaDB LangChain Wrapper for Vector DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-igNmKAdSphZ",
        "outputId": "714d7514-add6-4233-841f-48befe5486b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-chroma==0.2.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (0.2.2)\n",
            "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-chroma==0.2.2) (0.3.72)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-chroma==0.2.2) (1.26.4)\n",
            "Requirement already satisfied: chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-chroma==0.2.2) (0.6.3)\n",
            "Requirement already satisfied: build>=1.0.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.11.7)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.116.1)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.35.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (6.5.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (4.14.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.57b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.21.4)\n",
            "Requirement already satisfied: pypika>=0.48.9 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (9.1.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.11.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (14.1.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma==0.2.2) (0.3.45)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma==0.2.2) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma==0.2.2) (25.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma==0.2.2) (3.0.0)\n",
            "Requirement already satisfied: pyproject_hooks in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.2.0)\n",
            "Requirement already satisfied: colorama in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.4.6)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.47.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.3.1)\n",
            "Requirement already satisfied: certifi in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.16.0)\n",
            "Requirement already satisfied: six>=1.9.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.40.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.8.0)\n",
            "Requirement already satisfied: requests in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.10)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.6.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma==0.2.2) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma==0.2.2) (0.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.4.2)\n",
            "Requirement already satisfied: coloredlogs in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (25.2.10)\n",
            "Requirement already satisfied: protobuf in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (6.31.1)\n",
            "Requirement already satisfied: sympy in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.57b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.57b0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.57b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.57b0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.57b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.57b0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.57b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from opentelemetry-instrumentation==0.57b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.57b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.9.1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.34.4)\n",
            "Requirement already satisfied: filelock in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2025.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.1.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (15.0.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (10.0)\n",
            "Requirement already satisfied: pyreadline3 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.5.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\2. my github codebase\\langgraph_demystified\\.venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-chroma==0.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9c37cLnSrbg"
      },
      "source": [
        "## Enter Open AI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv3JzCEx_PAd",
        "outputId": "df082721-37f6-4454-e0e3-eecb554c2378"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucWRRI3QztL2"
      },
      "source": [
        "## Enter Tavily Search API Key\n",
        "\n",
        "Get a free API key from [here](https://tavily.com/#api)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK-1WLzOrJdb",
        "outputId": "043cd43c-6796-4313-bd1f-7ee99f84b3e8"
      },
      "outputs": [],
      "source": [
        "TAVILY_API_KEY = getpass('Enter Tavily Search API Key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T0s0um5Svfa"
      },
      "source": [
        "## Setup Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n",
        "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6OW_DlJSV70"
      },
      "source": [
        "## Build a Search Index for Research Paper Data\n",
        "\n",
        "We will build a vector database for retrieval and search by indexing a few research paper documents, similar to any standard RAG workflows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQL9rhyAp55U"
      },
      "source": [
        "### Open AI Embedding Models\n",
        "\n",
        "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hdBHpH3cdrYD"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA_-hzHbFeSP"
      },
      "source": [
        "### Get the research paper data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZFMYH-yFhWn",
        "outputId": "45b3dcf0-e927-4563-a505-0f3db8a761cc"
      },
      "outputs": [],
      "source": [
        "# if you can't download using the following code\n",
        "# go to https://drive.google.com/file/d/1ZOtPmuR-2KpzPvkiQiTVxAyJFo6NszG-/view?usp=sharing download it\n",
        "# manually upload it on colab\n",
        "\n",
        "# !gdown 1ZOtPmuR-2KpzPvkiQiTVxAyJFo6NszG-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKBiOapobqB9",
        "outputId": "05196c4f-5564-460e-b17b-4100c59fa5f0"
      },
      "outputs": [],
      "source": [
        "# !unzip research_papers.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_ReSz-3PVwW"
      },
      "source": [
        "### Load and Chunk Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8h-eIek5Hn8"
      },
      "source": [
        "We create a directory loader to use a PDF loader (using pymupdf) and load all PDF documents from a given folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UXxybBRKY5M7"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "# Define a function to create a DirectoryLoader for a specific file type\n",
        "def create_directory_loader(file_type, directory_path, loader_class, loader_args):\n",
        "    return DirectoryLoader(\n",
        "        path=directory_path,\n",
        "        glob=f\"**/*{file_type}\",\n",
        "        loader_cls=loader_class,\n",
        "        loader_kwargs=loader_args,\n",
        "        show_progress=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_eUpM5ecaaj",
        "outputId": "482bab64-0bad-4a22-9800-31148c2d3a08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  3.13it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "pdf_extn = '.pdf'\n",
        "pdf_loader_class = PyMuPDFLoader\n",
        "pdf_loader_args = {} # in case you want to change any settings in pymupdfloader\n",
        "directory= './research_papers'\n",
        "\n",
        "pdf_loader = create_directory_loader(file_type=pdf_extn,\n",
        "                                     directory_path=directory,\n",
        "                                     loader_class=pdf_loader_class,\n",
        "                                     loader_args=pdf_loader_args)\n",
        "\n",
        "# load docs\n",
        "docs = pdf_loader.load()\n",
        "len(docs) # PyMuPDF loads every document and breaks it per page by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP3i0I7sUYE5",
        "outputId": "27782921-2327-4f7d-dc8d-d13fb32f6a24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250321115731', 'source': 'research_papers\\\\attention.pdf', 'file_path': 'research_papers\\\\attention.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250321115731', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DggMVhwSlTO"
      },
      "source": [
        "We then use standard recursive character text chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WwLEBC4nF9ly"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Chunk docs\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=300)\n",
        "chunked_docs = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4E1zYFSG7J-",
        "outputId": "f6373b4b-4382-41bf-d159-ac6cd3ffeaa8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunked_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSbhERAyGw0v",
        "outputId": "8c970756-339e-4f77-cb0d-7c3625da81c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250321115731', 'source': 'research_papers\\\\attention.pdf', 'file_path': 'research_papers\\\\attention.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250321115731', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250321115731', 'source': 'research_papers\\\\attention.pdf', 'file_path': 'research_papers\\\\attention.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250321115731', 'page': 1}, page_content='1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250321115731', 'source': 'research_papers\\\\attention.pdf', 'file_path': 'research_papers\\\\attention.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250321115731', 'page': 1}, page_content='self-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2')]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunked_docs[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PnV9lAXZw9a"
      },
      "source": [
        "### Create a Vector DB and persist on disk\n",
        "\n",
        "Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kRYfcrsHUxyZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# create vector DB of docs and embeddings - takes < 30s on Colab\n",
        "chroma_db = Chroma.from_documents(documents=chunked_docs,\n",
        "                                  collection_name='rag_db',\n",
        "                                  embedding=openai_embed_model,\n",
        "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
        "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
        "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                  persist_directory=\"./rag_db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bprZC4S6TLfj"
      },
      "source": [
        "### Setup a Vector Database Retriever\n",
        "\n",
        "Here we use the following retrieval strategy:\n",
        "\n",
        "- Similarity with Threshold Retrieval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8foBD2xmCDYc"
      },
      "source": [
        "### Similarity with Threshold Retrieval\n",
        "\n",
        "We use cosine similarity here and retrieve the top 5 similar documents based on the user input query and also introduce a cutoff to not return any documents which are below a certain similarity threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6ROSNwqeCMRS"
      },
      "outputs": [],
      "source": [
        "similarity_threshold_retriever = chroma_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                                        search_kwargs={\"k\": 5,\n",
        "                                                                       \"score_threshold\": 0.35})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q68kFszS4O4"
      },
      "source": [
        "### Test out a few queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv93k_QpCZv7",
        "outputId": "afbb7ab5-8a9d-440f-e1a6-c648eed7f3ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(id='e63344e3-7bf4-4181-ad96-a3f9b68c9f3e', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers\\\\peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 0, 'producer': 'PDFium', 'source': 'research_papers\\\\peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='keeping the rest unaltered. Furthermore, the application of\\nPEFT extends beyond the realm of NLP and quickly attracts\\ninterest in the CV community for handling fine-tuning vision\\nmodels with large parameters, such as Vision Transformers\\n(ViT) and diffusion models, as well as disciplinary models\\nsuch as vision-language models.\\nIn this survey, we systematically review and categorize\\nrecent advancements in PEFT algorithms as well as the system\\nimplementation costs associated with various PEFT algorithms\\nacross diverse scenarios. Figure 1 presents the overview con-\\ntent for this survey. In section II, we present some fundamental\\nconcepts for LLM and PEFT, including computational flow\\nfor LLM, basic knowledge of PEFT, commonly used datasets\\nand tasks, and evaluation benchmarks. We categorize all\\ntypes of PEFT algorithms in Section III according to their\\ncomputational flow. In Section III-A, we detail additive algo-\\nrithms that either introduce new weight parameters or modify\\nactivations. Algorithms that only require fine-tuning of existing\\nparameters are categorized as selective approaches, which are\\nintroduced in Section III-B. In Section III-C, we explore\\nreparameterized PEFT, which constructs a (low- dimensional)\\nreparameterization of original model parameters for training\\nwhile transforming the weights back to maintain the inference\\nspeed. Additionally, there exist algorithms that combine the\\nabove techniques, and we have classified these as hybrid\\napproaches, elaborating on them in Section III-D. We also\\ninvestigate strategies for further reducing the computational\\ncomplexity of different PEFT algorithms, including KV-cache\\nmanagement, pruning, quantization, and memory optimization,\\nin Section IV.\\nIn Section V, we expand the scope of this survey beyond\\nthe computational perspective to involve various potential\\napplication scenarios. Specifically, we explore innovations that\\narXiv:2403.14608v7  [cs.LG]  16 Sep 2024'),\n",
              " Document(id='b70c2645-9f55-4c35-b2f1-e293954e0434', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers\\\\peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 18, 'producer': 'PDFium', 'source': 'research_papers\\\\peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='mark for PEFT is still lacking. This gap hinders the ability\\nto fairly compare the performance and efficiency of different\\nPEFT approaches. A well-accepted, up-to-date benchmark\\nakin to MMDetection [255] for object detection would enable\\nresearchers to validate their methods against a standard set\\nof tasks and metrics, fostering innovation and collaboration\\nwithin the community.\\nC. Enhance training efficiency\\nThe presumed parameter efficiency of PEFT is not always\\nconsistent with computational and memory savings during\\ntraining. Given that trainable parameters are intertwined within\\nthe pre-trained model’s architecture, computing and storing\\nactivations and gradients for the full model often become\\nnecessary during fine-tuning. This oversight calls for a rethink-\\ning of what constitutes efficiency. As outlined in Section IV,\\npotential solutions lie in the integration of model compres-\\nsion techniques such as pruning and quantization, alongside\\ninnovations specifically designed to optimize memory during\\nPEFT tuning [256]. Further research into enhancing the com-\\nputational efficiency of PEFT methodologies is imperative.\\nD. Explore scaling laws\\nThe design and effectiveness of PEFT methods originally\\ndeveloped for smaller Transformer models do not necessarily\\nscale with larger models. As the size of foundation models\\nincreases, identifying and adapting PEFT strategies that remain\\neffective is crucial. This investigation will aid in customizing\\nPEFT methodologies to suit the evolving landscape of large\\nmodel architectures.\\nE. Serve more models and tasks\\nThe rise of large foundation models across various domains\\npresents new opportunities for PEFT. Designing PEFT meth-\\nods tailored to the unique characteristics of models, such as\\nSora [257], Mamba [258], and LVM [259], can unlock new\\napplication scenarios and opportunities.\\nF. Enhancing data privacy\\nTrusting centralized systems to serve or fine-tune personal-\\nized PEFT modules is yet another issue for system developers.\\nMultiple types of inversion attacks [260], [261] have been pro-\\nposed to reconstruct user’s data by hijacking the intermediate\\nresults. One perspective of future trust-worthy LLM system\\ndesign involves developing an encryption protocol for both\\npersonal data and intermediate training and inference results.'),\n",
              " Document(id='d7c5e1b1-bd3e-441c-bd69-ec4c05d501c6', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers\\\\peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 15, 'producer': 'PDFium', 'source': 'research_papers\\\\peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='content. To overcome this, IP-Adapter introduces a novel\\ndecoupled cross-attention mechanism to distinguish between\\ntext and image features. IP-Adapter adds an additional cross-\\nattention layer exclusively for image features in each cross-\\nattention layer, and only the parameters of the new cross-\\nattention layers are trained.\\nVI. SYSTEM DESIGN CHALLENGE FOR PEFT\\nA. System design for PEFT\\nIn this section, we begin by providing a concise overview\\nof cloud-based PEFT systems and analyzing the design chal-\\nlenges. These include the efficient handling of numerous task-\\nspecific queries via centralized PEFT query servicing, the\\nresolution of privacy and data transmission issues through\\ndistributed PEFT training, and the complexities associated\\nwith concurrent multi-PEFT training processes. Centralized\\nsystems are required to process a substantial volume of queries\\nwith minimal latency and maximal throughput. Distributed\\ntraining frameworks must address privacy concerns and the\\ncomputational inefficiencies that arise from data exchanges\\nbetween users and cloud services. Furthermore, multi-PEFT\\ntraining necessitates the optimization of memory utilization,\\nthe management of simultaneous model training, and the\\nformulation of system architectures capable of supporting\\nmulti-tenant workloads effectively. These challenges under-\\nscore the imperative for innovative approaches to improve\\nscalability, safeguard privacy, and optimize resource allocation\\nin PEFT system architectures. Following this, we present the\\ncorresponding metrics employed for evaluating the system\\nperformance. Furthermore, we delve into three prospective\\nutilization scenarios to illustrate the challenges in system\\ndesign.\\n1) Centralized PEFT Query Serving: Cloud providers have\\nrecently introduced a range of LLM services aimed at pro-\\nviding user applications through application programming\\ninterfaces (APIs) [246], [247]. These APIs facilitate the seam-\\nless integration of many machine-learning functionalities into\\napplications. When receiving one query for one specific down-\\nstream task through API, the cloud-based server processes the\\nquery with one featured LLM model. Under this scenario, the\\nimportance of PEFT becomes apparent. Cloud providers store\\nonly a single copy of the LLM and multiple PEFT modules\\nfeaturing different downstream tasks. This setup allows the\\nLLM to maintain various branches of PEFT modules, each\\nlinked to specific API queries, i.e., PEFT queries.\\nCentralized PEFT query serving solutions address scenarios\\nwhere multiple PEFT queries arrive in quick succession. A\\ncase study of one state-of-the-art system for this purpose\\nis discussed in Section VI-B. Figure 10 (b) illustrates the\\ncomputation pattern for multi-query PEFT inference, wherein\\npacked PEFT queries are scheduled and executed according\\nto their deadlines and current system conditions.\\n2) Distributed PEFT Training: In most cases, personal-\\nized tasks are not fully supported with pre-trained models,'),\n",
              " Document(id='0cd0fc9a-e42f-4ce6-a480-98940e29d160', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers\\\\peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 16, 'producer': 'PDFium', 'source': 'research_papers\\\\peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='17\\nLLMs\\nEdge \\nDevice \\nPersonal data\\nCloud\\nTrainable \\nModules\\n🔥\\nFrozen Large Models\\nScheduler\\nRequest Pool\\nQuery\\nResponse\\nExecution\\nEngine\\nServing System\\nI like\\nI enjoy\\nLLM\\nprogramming\\n(a)\\n(b)\\nFig. 10: (a) Distributed-based system computation pattern; (b)\\ncentralized PEFT Query inference.\\nconsequently, extra fine-tuning is required to be executed\\nwith the methodologies mentioned in the previous sections.\\nHowever, significant concerns arise when considering the\\ntransfer of datasets to cloud providers, given the issues related\\nto data privacy, copyright, proprietary information, and the\\ncomplexities and inefficiencies involved in data transmission.\\nSection VI-C gives two approaches that address this concern.\\n3) Multi-PEFT Training: Different from multiple-PEFT\\nserving, tuning with multiple customized PEFTs always in-\\nvolves different backbone LLMs. Therefore, simultaneously\\ntuning multiple PEFTs can pose considerable challenges.\\nChallenges like how to manage memory gradient and model\\nweights storage, and how to design an efficient kernel for\\nbatching PEFT training remain unsolved. PEFTs will be cat-\\negorized based on their PEFT algorithms and backbone LLM\\nmodels. The design challenge involves how to consolidate\\nmultiple PEFTs with the same LLM backbone and multiple\\ndifferent LLM backbones simultaneously. We present case\\nstudies related to this topic in Section VI-D.\\n4) Evaluation Metrics: For the proposed evaluation met-\\nrics, without loss of generality, we adopt large language\\nmodels as the basis for our metric definitions.\\nTo evaluate the system performance of PEFT serving sys-\\ntems, we propose a set of evaluation metrics:\\n‚ System throughput: Considering PEFT queries as inter\\nand intra tasks, we use tokens per second to measure the\\nsystem throughput.\\n‚ Memory footprint: Run-time memory consumption dur-\\ning query serving, the memory utilization comes from\\nboth model parameters and KV-cache as mentioned in\\nSection IV-A.\\n‚ Accuracy performance: Real-world queries normally\\nhave different context lengths, and performance with\\nvariation length serves as a performance benchmark.\\n‚ Quality of services: Queries are associated with latency\\nrequirements and deadline missing rates are considered\\nas another benchmark.\\nTo assess the efficacy of PEFT training systems, we also\\nestablish a set of evaluative metrics:\\n‚ Accuracy performance: Performance of the fine-tuned\\nmodel over the downstream tasks.\\n‚ Compute cost: The compute cost during forward and\\nbackward propagation operations on cloud servers and\\nedge devices.\\n‚ Communication cost: Refers to the volume of data\\ninvolved during the transfer of intermediate data between\\nthe edge device and the cloud.\\nB. Centralized PEFT Serving Frameworks\\nThe PEFT algorithm is notable for its ability to distin-\\nguish between modifiable and immutable weights within a\\nmodel. This characteristic inspires developers to amalgamate\\ndiverse LLMs with distinct PEFT techniques into collective\\nunits. PetS, as introduced in [248], advocates for a com-\\nprehensive approach to managing multiple PEFT tasks by\\nsuggesting a unified serving framework. The framework’s\\ncore advancement lies in the translation of varying PEFT\\ntasks into integrated computation kernels to enhance efficiency.\\nMoreover, PetS pioneers an orchestrated batching approach\\nand a scheduling methodology, aiming to augment system\\nthroughput and leverage task parallelism respectively.\\nAs depicted in Figure 11, the PetS framework begins\\nwith users registering PEFT tasks through a standardized\\nApplication Programming Interface (API). Upon registration,\\ndevelopers are expected to provide the Pre-Trained Model Tag\\n(e.g., LLaMA), PEFT parameters in a compressed format,\\nand the specific PEFT algorithms (e.g., LoRA, Adapter, Bitfit,\\netc.). These tasks are then endowed with unique identifiers,\\nand the inference engine takes charge of query processing.\\nPetS bifurcates the primary computational workload (e.g.,'),\n",
              " Document(id='b97e4014-b66b-4a82-87f9-7eae3a41b311', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers\\\\peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 18, 'producer': 'PDFium', 'source': 'research_papers\\\\peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='19\\nThe second challenge is beyond the computational cost,\\ndesigning an efficient system architecture that can effectively\\nserve multi-tenant PEFT model workloads on the smallest set\\nof GPUs possible while occupying the least amount of GPU\\nresources is another significant challenge. Punica addresses\\nthis by scheduling user requests to active GPUs that already\\nserve or train PEFT models, thereby improving GPU utiliza-\\ntion. For older requests, Punica periodically migrates them to\\nconsolidate workloads, thus freeing up GPU resources for new\\nrequests.\\nb) Multi-Tenant PEFT design: Designing an efficient\\nsystem for the multi-tenant PEFT model serving in the Punica\\nframework focuses on addressing several key challenges to\\nmaximize hardware utilization and minimize resource con-\\nsumption. The system aims to consolidate multi-tenant LoRA\\nserving workloads onto the smallest set of GPUs possible. This\\nconsolidation is achieved through strategic scheduling of user\\nrequests to active GPUs that are already serving or training\\nLoRA models, thereby improving GPU utilization. For older\\nrequests, Punica periodically migrates them to consolidate\\nworkloads further, thus freeing up GPU resources for new\\nrequests. It incorporates on-demand loading of LoRA model\\nweights, which introduces only millisecond-level latency. This\\nfeature provides Punica with the flexibility to dynamically\\nconsolidate user requests to a small set of GPUs, without being\\nconstrained by the specific LoRA models already running on\\nthose GPUs. Besides that, Punica identifies that the decode\\nstage is a predominant factor in the cost of model serving,\\nPunica’s design primarily focuses on optimizing decode stage\\nperformance. Other aspects of model serving leverage straight-\\nforward techniques, such as on-demand loading of LoRA\\nmodel weights, to efficiently manage resource utilization.\\nVII. CONCLUSION AND FUTURE DIRECTIONS\\nIn the current era dominated by large models and large\\ndatasets, PEFT stands out as a highly attractive method for\\nefficiently adapting models to downstream tasks. This tech-\\nnique gains its appeal by addressing the significant challenges\\nposed by traditional full-model fine-tuning, which often places\\nsubstantial computational and data demands. This survey of-\\nfers a comprehensive examination of the most recent advance-\\nments in PEFT, including algorithmic design, computational\\nefficiency, application scenarios, and system implementation\\nfor PEFT. It offers a comprehensive taxonomy and explanation\\nthat serves as an excellent guidance and knowledge base,\\nwhich enables readers of various levels and disciplines to\\nswiftly grasp the core concepts of PEFT.\\nFor further research on PEFT, we propose a series of pos-\\nsible directions from both algorithm and system perspectives,\\nhoping to inspire more researchers to engage in further studies\\nin these areas.\\nA. Simplify hyperparameter tuning\\nThe effectiveness of PEFT is often sensitive to its hyperpa-\\nrameters, such as the bottleneck dimension of the adapter, the\\nrank of LoRA, and the arrangement of various additive PEFT\\nlayers. Manually tuning these hyperparameters will cost lots\\nof effort. Therefore, future efforts could focus on developing\\nmethods that are less dependent on manual tuning of these\\nparameters, or automatically find the optimal configuration\\nsettings. Several studies [82], [83], [84], [98], [99], [100] have\\nstarted to address this issue, but there’s a need for more simple\\nand efficient solutions optimizing these hyperparameters.\\nB. Establish a unified benchmark\\nDespite the existence of libraries like HuggingFace’s\\nPEFT [253] and AdapterHub [254], a comprehensive bench-\\nmark for PEFT is still lacking. This gap hinders the ability\\nto fairly compare the performance and efficiency of different\\nPEFT approaches. A well-accepted, up-to-date benchmark\\nakin to MMDetection [255] for object detection would enable\\nresearchers to validate their methods against a standard set')]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"what is PEFT?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "topk_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7miJY5r9S8ms"
      },
      "source": [
        "Looks like it is getting relevant documents from the vector DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Nfg_PAaiC2y",
        "outputId": "13c62585-2ca1-4d44-dedc-822156b50798"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No relevant docs were retrieved using the relevance score threshold 0.35\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"what are AI Agents?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "topk_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jw2knBlS_tM"
      },
      "source": [
        "Whoops seems like there exists no relevant docs for this. The Adaptive RAG System should be able to handle this also"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyyigxMUTStB"
      },
      "source": [
        "## Create AI Workflows and Tools for Key Components in our Agentic RAG System\n",
        "\n",
        "There are a few AI workflows (sequential pipelines) and tools we would need to create which we will be using in different steps in our Agentic Adaptive RAG workflow. These include:\n",
        "\n",
        "- **Query Source Classifier:** This is an essential workflow which can take in a user query, analyze it with an LLM and return the optimal data source which can be used to answer this query - in our case it will either be `vectordb` or `web_search`\n",
        "\n",
        "- **VectorDB Query Rewriter Workflow:** This is a workflow which uses an LLM to rewrite the given user query (if needed) and make it more optimized for retrieval from a vector database\n",
        "\n",
        "- **Web Search Query Rewriter Workflow:** This is a workflow which uses an LLM to rewrite the given user query (if needed) and make it more optimized for web search\n",
        "\n",
        "- **Query Retrieval Grader Workflow:** This is an essential workflow which can take in a user query, a list of retrieved documents from the vector DB and grade each context document as 'yes' or 'no' based on if the document is relevant to the user query or not\n",
        "\n",
        "- **QA RAG Workflow:** This is a workflow which can take in a user query, list of retrived context documents and use a standard RAG workflow where an LLM uses these context documents to generate a contextual response for the user query\n",
        "\n",
        "- **Hallucination Checker Workflow:** This is an essential workflow which can take in the RAG generated response from the LLM, the list of context documents and uses an LLM as a judge to check if the generated response is grounded in the facts mentioned in the context documents. If there are any contradictions or hallucinations it grades it as 'yes' or else 'no' and it also creates 'feedback' which contains the reasoning from the LLM judge as to why the grade was 'yes' or 'no'.\n",
        "\n",
        "- **Web Search Tool:** Build a custom tool which uses the Tavily Search API to search for a user query, get the top web page results and also extract the text content from those web pages\n",
        "\n",
        "Most of these workflows will be built as LangChain chains (pipelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWVO2Qssq0kC"
      },
      "source": [
        "### Create a Query Source Classifier workflow\n",
        "\n",
        "This is an essential workflow which can take in a user query, analyze it with an LLM and return the optimal data source which can be used to answer this query - in our case it will either be `vectordb` or `web_search`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Y4j7jiXGindI"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Data model\n",
        "class QuerySource(BaseModel):\n",
        "    \"\"\"Classify a user query in terms of what datasource should be useful for answering the query.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"vectordb\", \"web_search\"] = Field(\n",
        "        description=\"Given a user question classifies it to either web_search or vectordb depending on which source is more useful to answer the query.\",\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM for classification\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "qs_llm_router = llm.with_structured_output(QuerySource)\n",
        "\n",
        "# Prompt\n",
        "QS_SYS = \"\"\"You are an expert at deciding if a user question should be answered using a vector database (vectordb) or by searching the web (web_search).\n",
        "The vectorstore has documents related to the following topics:\n",
        "  - Chain of Thought Prompting\n",
        "  - Transformer Language Model and Attention Mechanisms (self-attention, multi-headed attention etc.)\n",
        "  - Self supervised vision transformers like the DINO model\n",
        "  - Diffusion generative models\n",
        "  - Parameter efficient fine-tuning techniques for language models (PEFT, LoRa, Prompt tuning etc.)\n",
        "\n",
        "Given the following user query, analyze it carefully and try to see if it is related to any of the above mentioned topics.\n",
        "Remember to focus on the overall meaning and concepts related to the above topics and not just keywords.\n",
        "If there is a relation then return vectordb as the source else return web_search.\n",
        "\"\"\"\n",
        "qs_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", QS_SYS),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Query source classifier\n",
        "qs_classifier = qs_prompt | qs_llm_router"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Rp7K3bEXw3a",
        "outputId": "7a9d986b-f301-43a2-fea4-1a9239cf9e6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuerySource(datasource='vectordb')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qs_classifier.invoke({\"question\": \"what is chain of thought prompting?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO83H10MYh-g",
        "outputId": "4c47ac81-f881-4c80-9b95-c31503aa550a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuerySource(datasource='web_search')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qs_classifier.invoke({\"question\": \"what is an AI Agent?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFLMiPfTYmj6",
        "outputId": "c020c5c6-57b4-406d-e096-74f1dcf66c89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuerySource(datasource='web_search')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qs_classifier.invoke({\"question\": \"Explain popular design patterns of Agentic AI Systems in detail\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YTuhEMJb6eV"
      },
      "source": [
        "### Create a VectorDB Query Rewriter Workflow\n",
        "\n",
        "This is a workflow which uses an LLM to rewrite the given user query (if needed) and make it more optimized for retrieval from a vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MUpixs1vcCwr"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Prompt\n",
        "VDB_REWRITER_SYS = \"\"\"You are an expert in analyzing input queries and rewriting or rephrasing them if necessary,\n",
        "to make them more optimized for better retrieval from a vector database which uses semantic search.\n",
        "Given the following input query or task, analyze it carefully and try to reason about the underlying semantic intent / meaning carefully before rephrasing.\n",
        "Focus more on rephrasing it to an optimized query which can retrieve data optimally from a vector database using similairty search.\n",
        "Remember to rephrase only if necessary and make the query concise, to the point.\n",
        "Just return the query and nothing else.\"\"\"\n",
        "vdb_query_rewrite_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", VDB_REWRITER_SYS),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"\"\"Here is the initial user query:\n",
        "               {query}\n",
        "\n",
        "               Rephrase the query for an improved version. Do NOT answer the query.\n",
        "            \"\"\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# VectorDB Query Rewriter\n",
        "vdb_query_rewriter = vdb_query_rewrite_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QcRlHo05dyIB",
        "outputId": "5be0e7f6-744d-4099-a518-4ca5b78097f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What is Chain of Thought (CoT) prompting?'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vdb_query_rewriter.invoke('what is CoT prompting?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Wgs05JFFd-Ud",
        "outputId": "360a13a6-837d-4bd0-cd76-43475bd304a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Detail popular design patterns in Agentic AI Systems.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vdb_query_rewriter.invoke('Explain popular design patterns of Agentic AI Systems in detail')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W_bNF4Rb10V"
      },
      "source": [
        "### Create a Web Search Query Rewriter Workflow\n",
        "\n",
        "This is a workflow which uses an LLM to rewrite the given user query (if needed) and make it more optimized for web search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "srnKGZNhfUb8"
      },
      "outputs": [],
      "source": [
        "# LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Prompt\n",
        "WS_REWRITER_SYS = \"\"\"You are an expert in analyzing input queries and rewriting or rephrasing them if necessary,\n",
        "to make them more optimized for searching on the web.\n",
        "Given the following input query or task, analyze it carefully and try to reason about the underlying semantic intent / meaning carefully before rephrasing.\n",
        "Focus more on rephrasing it to an optimized query which can get the right information from web search.\n",
        "Remember to rephrase only if necessary and make the query concise, to the point.\n",
        "Just return the query and nothing else.\"\"\"\n",
        "ws_query_rewrite_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", WS_REWRITER_SYS),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"\"\"Here is the initial user query:\n",
        "               {query}\n",
        "\n",
        "               Rephrase the query for an improved version. Do NOT answer the query.\n",
        "            \"\"\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Web Search Query Rewriter\n",
        "ws_query_rewriter = ws_query_rewrite_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-0tvV5cyfkCa",
        "outputId": "c5725fc3-a60d-4226-ff9f-de542ff8842a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What is Chain of Thought (CoT) prompting?'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ws_query_rewriter.invoke('what is CoT prompting?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8ipEVGcVflxM",
        "outputId": "0ada22b3-7ef5-4024-ffb0-1e3e354a7cb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Detailed explanation of popular design patterns in Agentic AI Systems'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ws_query_rewriter.invoke('Explain popular design patterns of Agentic AI Systems in detail')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3jIcXuErQia"
      },
      "source": [
        "### Create a Query Retrieval Grader Workflow\n",
        "\n",
        "This is an essential workflow which can take in a user query, a list of retrieved documents from the vector DB and grade each query as 'yes' or 'no' based on if the document is relevant to the user query or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-lzWTNGfj2Tj"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Data model for LLM output format\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM for grading\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt template for grading\n",
        "SYS_PROMPT = \"\"\"You are an expert grader assessing relevance of a retrieved document to a user question.\n",
        "                Follow these instructions for grading:\n",
        "                  - If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
        "                  - The overall grade should focus more on the semantic meaning rather than just individual words.\n",
        "                  - Your grade should be either 'yes' or 'no' to indicate whether the document is relevant to the question or not.\n",
        "             \"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        (\"human\", \"\"\"Retrieved document:\n",
        "                     {document}\n",
        "\n",
        "                     User question:\n",
        "                     {question}\n",
        "                  \"\"\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Build grader chain\n",
        "doc_grader = (grade_prompt\n",
        "                  |\n",
        "              structured_llm_grader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIQgbTj1gYC5",
        "outputId": "953f595c-eeb8-444c-ad4e-095778df6c45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the patterns underlying inputs and outputs via a large training dataset).\n",
            "2\n",
            "Chain-of-Thought Prompting\n",
            "Consider one’s own thought process when solving a complicated reasoning task such as a multi-step\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "Chain-of-Thought Prompting Elicits Reasoning\n",
            "in Large Language Models\n",
            "Jason Wei\n",
            "Xuezhi Wang\n",
            "Dale Schuurmans\n",
            "Maarten Bosma\n",
            "Brian Ichter\n",
            "Fei Xia\n",
            "Ed H. Chi\n",
            "Quoc V. Le\n",
            "Denny Zhou\n",
            "Google Research, Brain Te\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\n",
            "reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\n",
            "chai\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\n",
            "instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\n",
            "out\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "1\n",
            "Introduction\n",
            "Math Word Problems (GSM8K)\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "33\n",
            "55\n",
            "18\n",
            "57\n",
            "Solve rate (%)\n",
            "Finetuned GPT-3 175B\n",
            "Prior best\n",
            "PaLM 540B: standard prompting\n",
            "PaLM 540B: chain-of-thought prompting\n",
            "Figure 2:\n",
            "PaL\n",
            "GRADE: binary_score='yes'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = 'What is Chain of Thought (CoT) prompting?'\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "for doc in topk_docs:\n",
        "    print(doc.page_content[:200])\n",
        "    print('GRADE:', doc_grader.invoke({\"question\": query, \"document\": doc.page_content}))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHLZv6B-gipo",
        "outputId": "237d5f85-eb3a-4330-a52a-8f81354dfb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
            "3\n",
            "Model Architecture\n",
            "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
            "\n",
            "GRADE: binary_score='no'\n",
            "\n",
            "20\n",
            "G. PEFT with model compression\n",
            "Model compression is one of the most effective ways to\n",
            "make LLM executable on resource-limited devices. Yet, the\n",
            "impact of model compression techniques on the perform\n",
            "GRADE: binary_score='no'\n",
            "\n",
            "7\n",
            "V\n",
            "K\n",
            "Q\n",
            "⊙\n",
            "lk\n",
            "lv\n",
            "lff\n",
            "⊙\n",
            "softmax\n",
            "Wdown\n",
            "𝛔\n",
            "⊙\n",
            "Wup\n",
            "(a) (IA)3\n",
            "⊙\n",
            "⊕\n",
            "Operation 1\n",
            "Operation 2\n",
            "(b) SSF\n",
            "scale\n",
            "shift\n",
            "Fig. 6: Illustration of (IA)3 and SSF. Blue represents frozen,\n",
            "while yellow represents trainable.\n",
            "\n",
            "GRADE: binary_score='no'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = 'Explain popular design patterns of Agentic AI Systems in detail?'\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "for doc in topk_docs:\n",
        "    print(doc.page_content[:200])\n",
        "    print('GRADE:', doc_grader.invoke({\"question\": query, \"document\": doc.page_content}))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfn4Z4Em55bb"
      },
      "source": [
        "### Create a QA RAG Workflow\n",
        "\n",
        "This is a workflow which can take in a user query, list of retrived context documents and use a standard RAG workflow where an LLM uses these context documents to generate a contextual response for the user query\n",
        "\n",
        "It can also optionally take in feedback from a LLM Hallucination Grader telling what were the issues with the previous response generated by this RAG chain and try to fix those issues when generating the answer again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8Pu5U9HNkl-q"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "\n",
        "# Create RAG prompt for response generation\n",
        "prompt = \"\"\"You are an assistant for question-answering tasks on popular research topics.\n",
        "            Use the following pieces of retrieved context to answer the question.\n",
        "            If no context is present or if you don't know the answer, just say that you don't know the answer.\n",
        "            Do not make up the answer unless it is there in the provided context.\n",
        "            Give a detailed answer and to the point answer with regard to the question.\n",
        "\n",
        "            In case the feedback section below is not empty, reflect on the feedback given based on the last generated response\n",
        "            and try to incorporate the feedback and generate an improved response.\n",
        "\n",
        "            Question:\n",
        "            {question}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Feedback from last response generated for question:\n",
        "            {feedback}\n",
        "\n",
        "            Answer:\n",
        "         \"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt, optional_variables=['feedback'])\n",
        "\n",
        "# Initialize connection with GPT-4o\n",
        "chatgpt = ChatOpenAI(model_name='gpt-4o', temperature=0)\n",
        "# Used for separating context docs with new lines\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# create QA RAG chain\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": (itemgetter('context')\n",
        "                        |\n",
        "                    RunnableLambda(format_docs)),\n",
        "        \"question\": itemgetter('question'),\n",
        "        \"feedback\": lambda x: itemgetter('feedback')(x) if 'feedback' in x else ''\n",
        "    }\n",
        "      |\n",
        "    prompt_template\n",
        "      |\n",
        "    chatgpt\n",
        "      |\n",
        "    StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Wj-MZr2eEq8",
        "outputId": "88bb9bf6-67eb-4866-ebba-7d03fc764cf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chain of Thought (CoT) prompting is a technique used to enhance the reasoning capabilities of large language models by guiding them through a series of intermediate reasoning steps, similar to how humans solve complex problems. This method involves providing the model with a few examples that include not just the input and the final output, but also the intermediate steps or \"chain of thought\" that lead to the solution.\n",
            "\n",
            "The primary goal of CoT prompting is to enable language models to tackle complex reasoning tasks, such as multi-step arithmetic problems, commonsense reasoning, and symbolic reasoning, by breaking them down into manageable parts. This approach has been shown to significantly improve the performance of large language models on these tasks. For instance, experiments have demonstrated that using CoT prompting with a large model like PaLM 540B can achieve state-of-the-art accuracy on benchmarks like GSM8K, which involves math word problems.\n",
            "\n",
            "CoT prompting is particularly effective because it leverages the model's ability to perform in-context few-shot learning. Instead of requiring extensive fine-tuning or large training datasets, CoT prompting uses a few carefully crafted examples to demonstrate the reasoning process. This method not only improves the model's performance on reasoning tasks but also highlights the potential of large language models to generalize from a small number of examples.\n",
            "\n",
            "However, there are limitations to this approach. While CoT prompting emulates human-like reasoning, it does not necessarily mean that the model is truly \"reasoning\" in the human sense. Additionally, the effectiveness of CoT prompting is more pronounced in larger models, which can be costly to deploy in real-world applications. Despite these challenges, CoT prompting represents a significant advancement in the field of natural language processing, expanding the range of tasks that language models can perform successfully.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is Chain of Thought (CoT) prompting?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "result = qa_rag_chain.invoke(\n",
        "    {\"context\": topk_docs, \"question\": query}\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_W2AoqYtbUG",
        "outputId": "0ddbd3e6-2e9d-43e6-9f41-9643da1b0384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chain of Thought (CoT) prompting is a technique used to enhance the reasoning capabilities of large language models by guiding them through a series of intermediate reasoning steps, similar to how humans solve complex problems. This method involves providing the model with a structured prompt that includes not just the input and the expected output, but also a detailed sequence of logical steps that lead to the final answer. This approach is particularly useful for tasks that require multi-step reasoning, such as arithmetic, commonsense, and symbolic reasoning problems.\n",
            "\n",
            "The concept is inspired by the natural human thought process where a problem is broken down into smaller, manageable parts, each solved sequentially to arrive at the final solution. For example, when solving a math word problem, one might first calculate intermediate values before arriving at the final answer. CoT prompting aims to replicate this process in language models.\n",
            "\n",
            "Research has shown that CoT prompting significantly improves the performance of large language models on complex reasoning tasks. For instance, experiments with the PaLM 540B model demonstrated that using CoT prompting with just a few examples can achieve state-of-the-art accuracy on benchmarks like GSM8K, which involves math word problems. This method outperforms standard prompting techniques, which often struggle with tasks requiring deep reasoning.\n",
            "\n",
            "One of the key advantages of CoT prompting is that it does not require fine-tuning the model with a large dataset. Instead, it leverages few-shot learning, where the model is prompted with a few examples that include the chain of thought. This makes it a cost-effective and efficient approach to enhance reasoning without the need for extensive retraining.\n",
            "\n",
            "However, there are limitations to this approach. While CoT prompting emulates human reasoning, it does not necessarily mean that the model is truly \"reasoning\" in a human-like manner. Additionally, the quality of the reasoning path is not guaranteed, which can lead to both correct and incorrect answers. The effectiveness of CoT prompting also tends to emerge only at larger model scales, which can be costly to implement in real-world applications.\n",
            "\n",
            "Overall, Chain of Thought prompting represents a promising direction for improving the reasoning capabilities of language models, expanding the range of tasks they can perform successfully, and inspiring further research into language-based reasoning methods.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is Chain of Thought (CoT) prompting?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "result = qa_rag_chain.invoke(\n",
        "    {\"context\": topk_docs, \"question\": query, \"feedback\": \"The answer was not having enough depth last time, please fix it\"}\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE1VFHXvhSau",
        "outputId": "c844ff54-4dfc-46f4-aaae-5466f4af1d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Self-attention is a mechanism used in neural networks, particularly in models like Transformers, to compute a representation of a sequence by relating different positions within the same sequence. It allows the model to weigh the importance of each part of the sequence when computing the representation of a particular part, enabling the model to capture dependencies between distant parts of the sequence efficiently.\n",
            "\n",
            "### Key Components of Self-Attention:\n",
            "\n",
            "1. **Queries, Keys, and Values:**\n",
            "   - Each input element in the sequence is transformed into three vectors: a query vector, a key vector, and a value vector. These vectors are used to compute the attention scores.\n",
            "\n",
            "2. **Scaled Dot-Product Attention:**\n",
            "   - The attention mechanism computes a score for each pair of query and key vectors by taking their dot product. This score indicates the relevance of the key to the query.\n",
            "   - The scores are scaled by the square root of the dimension of the key vectors (\\(\\sqrt{d_k}\\)) to prevent the dot products from becoming too large, which can push the softmax function into regions with very small gradients.\n",
            "   - A softmax function is applied to these scores to obtain the attention weights, which are then used to compute a weighted sum of the value vectors. This weighted sum is the output of the attention mechanism.\n",
            "\n",
            "3. **Multi-Head Attention:**\n",
            "   - Instead of performing a single attention function, multi-head attention projects the queries, keys, and values into multiple subspaces and performs the attention function in parallel. This allows the model to jointly attend to information from different representation subspaces at different positions.\n",
            "   - The outputs from each attention head are concatenated and linearly transformed to produce the final output.\n",
            "\n",
            "### Advantages of Self-Attention:\n",
            "\n",
            "- **Parallelization:**\n",
            "  - Unlike recurrent neural networks (RNNs), which process sequences sequentially, self-attention allows for parallelization across the sequence, significantly speeding up computation.\n",
            "\n",
            "- **Long-Range Dependencies:**\n",
            "  - Self-attention can capture dependencies between distant parts of a sequence more effectively than RNNs, as it does not rely on the sequential processing of data.\n",
            "\n",
            "- **Constant Path Length:**\n",
            "  - The path length between any two positions in the input sequence is constant, which facilitates learning long-range dependencies.\n",
            "\n",
            "### Positional Encoding:\n",
            "\n",
            "Since self-attention does not inherently capture the order of the sequence, positional encodings are added to the input embeddings to provide information about the position of each token in the sequence. These encodings can be fixed (e.g., using sine and cosine functions) or learned.\n",
            "\n",
            "### Applications:\n",
            "\n",
            "Self-attention is a core component of the Transformer architecture, which has been highly successful in tasks such as machine translation, language modeling, and more recently, in vision tasks with Vision Transformers (ViTs). It has become a fundamental building block in modern deep learning models due to its efficiency and effectiveness in capturing complex dependencies in data.\n"
          ]
        }
      ],
      "source": [
        "query = \"Explain self-attention in detail\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "result = qa_rag_chain.invoke(\n",
        "    {\"context\": topk_docs, \"question\": query}\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_BK5xvbeYmz",
        "outputId": "19c35fb3-989d-472c-c0f4-4e63e3a18f83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know the answer. The provided context does not contain information about the most popular design patterns for Agentic AI.\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the most popular design patterns for Agentic AI?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "result = qa_rag_chain.invoke(\n",
        "    {\"context\": topk_docs, \"question\": query}\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efgc6K-cu-Ea"
      },
      "source": [
        "### Create a Hallucination Checker Workflow\n",
        "\n",
        "This is an essential workflow which can take in the RAG generated response from the LLM, the list of context documents and uses an LLM as a judge to check if the generated response is grounded in the facts mentioned in the context documents.\n",
        "\n",
        "If there are any contradictions or hallucinations it grades it as 'yes' or else 'no' and it also creates 'feedback' which contains the reasoning from the LLM judge as to why the grade was 'yes' or 'no'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Wzd9bKU3mHkH"
      },
      "outputs": [],
      "source": [
        "# Data model\n",
        "class CheckHallucinations(BaseModel):\n",
        "    \"\"\"Binary score to say if hallucination is present in generated answer.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Generated Answer may have hallucinations and is not grounded in the facts provided by the context documents, 'yes' or 'no'\"\n",
        "    )\n",
        "    feedback: str = Field(\n",
        "        description=\"Feedback based on why the score was yes or no\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "hallucination_llm_grader = llm.with_structured_output(CheckHallucinations)\n",
        "\n",
        "# Prompt\n",
        "HALLUCINATION_SYS = \"\"\"You are an expert assessing whether an LLM generated answer is grounded in / supported\n",
        "by a set of retrieved facts provided by the context documents mentioned below or not.\n",
        "Given the LLM generated response, return a binary score 'yes' or 'no'.\n",
        " - 'no' means that the answer is grounded in / supported by the set of facts in the context documents\n",
        " - 'yes' means the answer could contain contain wrong information or deviate or contradict the set of facts in the context documents\n",
        "\n",
        " Also return the feedback based on why the score was yes or no in the feedback field. Keep the feedback concise and to the point.\n",
        " \"\"\"\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", HALLUCINATION_SYS),\n",
        "        (\"human\", \"\"\"LLM generated response:\n",
        "                     {generation}\n",
        "\n",
        "                     Context documents:\n",
        "                     {documents}\n",
        "\n",
        "                  \"\"\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Hallucination grader\n",
        "hallucination_grader = hallucination_prompt | hallucination_llm_grader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Ksr-oW-DlXcF"
      },
      "outputs": [],
      "source": [
        "query = \"What is Chain of Thought (CoT) prompting?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "result = qa_rag_chain.invoke(\n",
        "    {\"context\": topk_docs, \"question\": query}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "5RkGq0rHAxwN",
        "outputId": "c47b9a78-ba64-4043-8353-2e837a3e0ba6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Chain of Thought (CoT) prompting is a technique used to enhance the reasoning capabilities of large language models. It involves guiding the model to generate a series of intermediate reasoning steps, akin to a human\\'s thought process, when solving complex problems. This method is particularly useful for tasks that require multi-step reasoning, such as arithmetic, commonsense, and symbolic reasoning tasks.\\n\\nThe core idea behind CoT prompting is to provide the model with exemplars that include not just the input and the final output, but also the intermediate steps that lead to the solution. For example, when solving a math word problem, instead of directly providing the answer, the model is prompted to break down the problem into smaller, manageable steps and solve each one sequentially. This approach mimics how humans typically solve complex problems by decomposing them into simpler parts.\\n\\nResearch has shown that CoT prompting significantly improves the performance of large language models on various reasoning tasks. For instance, experiments have demonstrated that using CoT prompting with a large model like PaLM 540B can achieve state-of-the-art accuracy on benchmarks such as the GSM8K math word problems, surpassing even fine-tuned models like GPT-3 with a verifier.\\n\\nOne of the key advantages of CoT prompting is that it does not require fine-tuning the model with a large dataset. Instead, it leverages few-shot learning by providing a few examples that include the chain of thought, making it a cost-effective method for enhancing reasoning without extensive data annotation.\\n\\nHowever, there are limitations to this approach. While CoT prompting emulates human reasoning, it does not necessarily mean that the model is truly \"reasoning\" in a human-like manner. Additionally, the emergence of CoT reasoning is more pronounced in larger models, which can be costly to deploy in real-world applications. There is also the challenge of ensuring the correctness of the reasoning paths, as incorrect intermediate steps can lead to wrong answers.\\n\\nOverall, Chain of Thought prompting is a promising method for expanding the range of tasks that large language models can perform successfully, by enabling them to tackle complex reasoning tasks more effectively.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Kr2eOJn5rdD",
        "outputId": "c64e839e-055c-47e8-c774-930a88c394b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CheckHallucinations(binary_score='no', feedback='The LLM-generated response accurately reflects the information provided in the context documents. It correctly describes Chain of Thought (CoT) prompting as a technique to enhance reasoning in large language models by using intermediate reasoning steps. The response also mentions the use of exemplars, the improvement in performance on tasks like GSM8K, and the cost-effectiveness of CoT prompting, all of which are supported by the context documents.')"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hallucination_grader.invoke({\"documents\": topk_docs, \"generation\": result})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piu3Tgh7lyv4",
        "outputId": "e470ecc2-8663-428f-efba-aa80232ded27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CheckHallucinations(binary_score='yes', feedback=\"The LLM generated response provides a simplistic and incorrect definition of 'chain of thought' as a general human thinking process. The context documents specifically describe 'chain of thought' in the context of language models as a method to improve reasoning by breaking down problems into intermediate steps. The generated response does not reflect this specific application and understanding from the documents.\")"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# grading a hallucinated answer\n",
        "result = \"Chain of thought is when humans have a chain of thoughts from where they think of one thing and go into another\"\n",
        "hallucination_grader.invoke({\"documents\": topk_docs, \"generation\": result})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "howf-v0ARWbv"
      },
      "source": [
        "### Create a Web Search Tool\n",
        "\n",
        "Build a custom tool which uses the Tavily Search API to search for a user query, get the top web page results and also extract the text content from those web pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Ue8xgu9WpuPi"
      },
      "outputs": [],
      "source": [
        "from langchain_tavily._utilities import TavilySearchAPIWrapper\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "tavily_search = TavilySearchAPIWrapper()\n",
        "\n",
        "@tool\n",
        "def search_web(query: str) -> list:\n",
        "    \"\"\"Search the web for a query. Userful for general information or general news\"\"\"\n",
        "    results = tavily_search.raw_results(query=query,\n",
        "                                        max_results=6,\n",
        "                                        search_depth='advanced',\n",
        "                                        include_answer=False,\n",
        "                                        include_raw_content=True)\n",
        "    results = [r['raw_content'] for r in results['results']]\n",
        "    results = [doc for doc in results if doc is not None] # remove blank page content\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5tUXzqPsbQi"
      },
      "source": [
        "## Define the Agent State Schema\n",
        "\n",
        "Here we define the key state schema that maintains the agent's state across different steps of execution in the LangGraph workflow.\n",
        "\n",
        "We define a `GraphState` typed dictionary to track relevant information during the execution of the Agentic Adaptive RAG System:\n",
        "\n",
        "- **orig_question**: The original user query submitted to the system.\n",
        "- **rephrased_question**: A rewritten version of the query, optimized for either vector retrieval or web search depending on the route taken.\n",
        "- **generation**: The final response generated by the LLM using the selected set of documents and the RAG prompt.\n",
        "- **hallucination_grade**: The result of the hallucination check, indicating whether the generated response has hallucinations or not (`yes` or `no`).\n",
        "- **hallucination_feedback**: Feedback from the hallucination checker LLM explaining whether and why the response was considered hallucinated or not.\n",
        "- **web_search_needed**: A flag (`yes` or `no`) indicating whether a web search is necessary (lack of enough relevant context in vectordb route).\n",
        "- **documents**: A list of the final context documents used for generating the answer. These may come from a vector DB, a web search, or both.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_B2EFrwTpuXB"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of the Adaptive RAG agent during execution.\n",
        "\n",
        "    Attributes:\n",
        "        orig_question: The original user question before rephrasing.\n",
        "        rephrased_question: Query rephrased for vector DB or web search optimization.\n",
        "        generation: The final LLM-generated response to the query.\n",
        "        hallucination_grade: 'yes' or 'no' depending on whether the response contains hallucinations or not.\n",
        "        hallucination_feedback: Explanation from the hallucination check step about grounding validity.\n",
        "        web_search_needed: Flag indicating if web search is necessary (lack of enough relevant context in vectordb route).\n",
        "        documents: Final list of context documents used to generate the answer (from vector DB, web, or both).\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    orig_question: str\n",
        "    rephrased_question: str\n",
        "    generation: str\n",
        "    hallucination_grade: str\n",
        "    hallucination_feedback: str\n",
        "    web_search_needed: str\n",
        "    documents: List[str]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnp10mXId7Wd"
      },
      "source": [
        "## Plan the Agent Workflow Structure\n",
        "\n",
        "This is the Agent workflow we will be using\n",
        "\n",
        "![](https://i.imgur.com/KoY9Dx0.png)\n",
        "\n",
        "Next up we will define python functions for each of the nodes in this Agent graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRlO4Hu7mdwl"
      },
      "source": [
        "## Create Node Functions\n",
        "\n",
        "Each function below represents a stage in processing a user query in the **Agentic Adaptive RAG System**:\n",
        "\n",
        "1. **rewrite_query_vectordb**: Rephrases the user’s original query to optimize it for retrieving relevant documents from a vector database.\n",
        "\n",
        "2. **retrieve_vectordb**: Retrieves a set of potentially relevant documents from the vector database using the rephrased query.\n",
        "\n",
        "3. **grade_documents**: Uses an LLM-based grading prompt to evaluate the relevance of each document retrieved from the vector database.\n",
        "   - Calculates the percentage of relevant documents.\n",
        "   - If **more than 50%** of the documents are relevant, the system proceeds directly to generate the answer.\n",
        "   - If **50% or fewer** documents are relevant (or none are retrieved), the `web_search_needed` flag is set to `\"yes\"` to enable the agent to go to the web search nodes later on  to get more context from the web.\n",
        "\n",
        "4. **rewrite_query_web_search**: Rephrases the original query to be optimized for web search. This improves the chances of retrieving high-quality context from web sources.\n",
        "\n",
        "5. **web_search**: Performs a real-time web search using the rephrased query and retrieves context documents for RAG-based answer generation.\n",
        "\n",
        "6. **generate_answer**: Takes the original or rephrased query along with the most relevant set of context documents — whether from vector DB, web, or both — and generates a grounded final response using a RAG prompt. Can also optionally add in feedback in the RAG prompt to instruct the LLM to generate a better response in case there were hallucinations in a previous iteration.\n",
        "\n",
        "7. **grade_hallucinations**: Applies a hallucination detection prompt that evaluates whether the generated response is fully grounded in the provided context documents. Returns the **hallucination_grade = yes or no** grade and the feedback of the grade in terms of the LLM's reasoning in the  **hallucination_feedback** field\n",
        "\n",
        "Finally:\n",
        "   - If the response is **not hallucinated**, the response is returned as the final response.\n",
        "   - If **hallucination_grade = yes**, the feedback is incorporated and the system goes back to **generate_answer** and regenerates answer to prevent hallucinated content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWGx0oR1nuRa"
      },
      "source": [
        "### Create the rewrite_query_vectordb function\n",
        "\n",
        "Rephrases the user’s original query to optimize it for retrieving relevant documents from a vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ZQBRn4VunuRa"
      },
      "outputs": [],
      "source": [
        "def rewrite_query_vectordb(state):\n",
        "    \"\"\"\n",
        "    Rewrite user query to improve retrieval from vector db\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, rephrased_question - that contains rephrased and optimized user query\n",
        "    \"\"\"\n",
        "    print(\"---REWRITING USER QUERY FOR OPTIMAL RETRIEVAL FROM VECTOR DB---\")\n",
        "    original_question = state[\"orig_question\"]\n",
        "    rephrased_question = vdb_query_rewriter.invoke({\"query\": original_question})\n",
        "\n",
        "    return {\"orig_question\": original_question, \"rephrased_question\": rephrased_question}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2e9vbSKodN4"
      },
      "source": [
        "### Create the rewrite_query_web_search function\n",
        "\n",
        "Rephrases the original query to be optimized for web search. This improves the chances of retrieving high-quality context from web sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "NwDH1ckoodN4"
      },
      "outputs": [],
      "source": [
        "def rewrite_query_web_search(state):\n",
        "    \"\"\"\n",
        "    Rewrite user query to improve retrieval from web search\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, rephrased_question - that contains rephrased and optimized user query\n",
        "    \"\"\"\n",
        "    print(\"---REWRITING USER QUERY FOR OPTIMAL RETRIEVAL FOR WEB SEARCH---\")\n",
        "    original_question = state[\"orig_question\"]\n",
        "    rephrased_question = ws_query_rewriter.invoke({\"query\": original_question})\n",
        "\n",
        "    return {\"orig_question\": original_question, \"rephrased_question\": rephrased_question}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXfVLhOWtHJJ"
      },
      "source": [
        "### Create the retrieve_vectordb function\n",
        "\n",
        "Retrieves a set of potentially relevant documents from the vector database using the rephrased query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "W0rVVBGDpuYw"
      },
      "outputs": [],
      "source": [
        "def retrieve_vectordb(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents from vector database\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents - that contains retrieved context documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVAL FROM VECTOR DB---\")\n",
        "    # get optimized question\n",
        "    question = state[\"rephrased_question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = similarity_threshold_retriever.invoke(question)\n",
        "    return {\"documents\": documents}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpOsUnzn6Yo1"
      },
      "source": [
        "### Create the grade_documents function\n",
        "\n",
        "Uses an LLM-based grading prompt to evaluate the relevance of each document retrieved from the vector database.\n",
        "   - Calculates the percentage of relevant documents.\n",
        "   - If **more than 50%** of the documents are relevant, the system proceeds directly to generate the answer.\n",
        "   - If **50% or fewer** documents are relevant (or none are retrieved), the `web_search_needed` flag is set to `\"yes\"` to enable the agent to go to the web search nodes later on  to get more context from the web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "NI20nh1DtTwJ"
      },
      "outputs": [],
      "source": [
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question\n",
        "    by using an LLM Grader.\n",
        "\n",
        "    If any document are not relevant to question or documents are empty - Web Search needs to be done\n",
        "    If all documents are relevant to question - Web Search is not needed\n",
        "    Helps filtering out irrelevant documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"rephrased_question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    RELEVANCE_THRESHOLD = 0.5 # what %age of retrieved documents should at least be relevant\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    web_search_needed = \"No\"\n",
        "    total_irrelevant = 0\n",
        "    if documents:\n",
        "        for d in documents:\n",
        "            score = doc_grader.invoke(\n",
        "                {\"question\": question, \"document\": d.page_content}\n",
        "            )\n",
        "            grade = score.binary_score\n",
        "            if grade == \"yes\":\n",
        "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "                filtered_docs.append(d)\n",
        "            else:\n",
        "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "                total_irrelevant += 1\n",
        "\n",
        "        relevance_frac = 1 - (total_irrelevant / len(documents))\n",
        "        if relevance_frac <= RELEVANCE_THRESHOLD:\n",
        "            print(\"---SEVERAL DOCUMENTS (\"+str((1-relevance_frac)*100)+\"%) ARE NOT RELEVANT TO QUESTION - WEB SEARCH NEEDED---\")\n",
        "            web_search_needed = \"Yes\"\n",
        "        else:\n",
        "            print(\"---MOST DOCUMENTS (\"+str(relevance_frac*100)+\"%) ARE RELEVANT TO QUESTION - WEB SEARCH NOT NEEDED---\")\n",
        "\n",
        "    else:\n",
        "        print(\"---NO DOCUMENTS RETRIEVED - WEB SEARCH NEEDED---\")\n",
        "        web_search_needed = \"Yes\"\n",
        "\n",
        "    return {\"documents\": filtered_docs, \"web_search_needed\": web_search_needed}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMogEnhT7Icn"
      },
      "source": [
        "### Create the web_search function\n",
        "\n",
        "Performs a real-time web search using the rephrased query and retrieves context documents for RAG-based answer generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "YM7f6AyCvUP_"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based on the re-written question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with appended web results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    question = state[\"rephrased_question\"]\n",
        "    documents = state.get('documents', [])\n",
        "\n",
        "    # Web search\n",
        "    docs = search_web.invoke(question)\n",
        "    web_results = [Document(page_content=d) for d in docs]\n",
        "    documents.extend(web_results)\n",
        "\n",
        "    return {\"documents\": documents}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruTBxSkm7R2R"
      },
      "source": [
        "### Create the generate_answer function\n",
        "\n",
        "Takes the original or rephrased query along with the most relevant set of context documents — whether from vector DB, web, or both — and generates a grounded final response using a RAG prompt.\n",
        "\n",
        "Can also optionally add in feedback in the RAG prompt to instruct the LLM to generate a better response in case there were hallucinations in a previous iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "MemqMTolwLhA"
      },
      "outputs": [],
      "source": [
        "def generate_answer(state):\n",
        "    \"\"\"\n",
        "    Generate answer from context document using LLM\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE ANSWER---\")\n",
        "    question = state[\"orig_question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    feedback = state.get(\"hallucination_feedback\", \"\")\n",
        "    hallucination_grade = state.get(\"hallucination_grade\", \"\")\n",
        "\n",
        "    if hallucination_grade and hallucination_grade == 'yes':\n",
        "        print('Hallucination Check Feedback:', feedback)\n",
        "        # grading has been done for a previous response and hallucinated response has occured\n",
        "        print('---REFLECTING ON HALLUCINATED FEEDBACK - GENERATING ANSWER---')\n",
        "        generation = qa_rag_chain.invoke({\"context\": documents, \"question\": question, \"feedback\": feedback})\n",
        "    else:\n",
        "        # standard RAG generation\n",
        "        print('---STANDARD RAG FLOW - GENERATING ANSWER---')\n",
        "        generation = qa_rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "        # Uncomment the following code line for the last demo below\n",
        "        # This is to simulate a hallucinated answer to check how the agent handles it\n",
        "        # generation = 'The observer and factory pattern are the main design patterns'\n",
        "    return {\"generation\": generation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU7jaehmwhr8"
      },
      "source": [
        "### Create the grade_hallucinations function\n",
        "\n",
        "Applies a hallucination detection prompt that evaluates whether the generated response is fully grounded in the provided context documents.\n",
        "\n",
        "Returns the **hallucination_grade = yes or no** grade and the feedback of the grade in terms of the LLM's reasoning in the  **hallucination_feedback** field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "bsZscGzpwf9S"
      },
      "outputs": [],
      "source": [
        "def grade_hallucinations(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
        "\n",
        "    grader_response = hallucination_grader.invoke({\"documents\": formatted_docs, \"generation\": generation})\n",
        "\n",
        "    # Check hallucination\n",
        "    if grader_response.binary_score == \"no\":\n",
        "        print(\"---HALLUCINATION CHECK: SUCCESSFUL - GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "    else:\n",
        "        print(\"---HALLUCINATION CHECK: UNSUCCESSFUL - GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRYING RESPONSE GENERATION---\")\n",
        "    return {'hallucination_grade': grader_response.binary_score, 'hallucination_feedback': grader_response.feedback}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpjPx4v89BVV"
      },
      "source": [
        "## Build and Compile the Agent Graph Workflow\n",
        "\n",
        "We construct a LangGraph agent workflow for the Agentic Adaptive RAG System with the following dynamic routing sequence:\n",
        "\n",
        "1. The agent begins by running a **query source classifier** to determine the best information source for the user's query.\n",
        "   - If the best source is **Vector DB**, the flow continues with:\n",
        "     - **rewrite_query_vectordb** → Rephrases the query for optimal semantic retrieval from the internal vector database.\n",
        "     - **retrieve_vectordb** → Retrieves documents from the vector database using the rephrased query.\n",
        "     - **grade_documents** → Grades the relevance of retrieved documents. If most are relevant, continue to generate an answer. If not, switch to web search.\n",
        "\n",
        "2. If the best source is **Web Search**, or if Vector DB results are insufficient:\n",
        "   - **rewrite_query_web_search** → Rewrites the query to optimize it for web search.\n",
        "   - **web_search** → Performs a web search to retrieve external context documents.\n",
        "\n",
        "3. **generate_answer** → Generates a grounded response using the best available documents (from vector DB or web).\n",
        "\n",
        "4. **grade_hallucinations** → Runs a hallucination check to evaluate if the generated response is grounded in the retrieved context:\n",
        "   - If the response is **not hallucinated**, the workflow ends.\n",
        "   - If the response **contains hallucinations**, the system loops back to **generate_answer** using hallucination feedback for regeneration.\n",
        "\n",
        "This adaptive workflow allows the agent to dynamically switch between vector-based and web-based RAG strategies, incorporate corrective web search, and apply self-reflection to ensure high-quality, grounded responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "F24b6qm_yhnE"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import START, END, StateGraph\n",
        "\n",
        "# Create a typed LangGraph state graph using the custom GraphState\n",
        "adaptive_rag = StateGraph(GraphState)\n",
        "\n",
        "# Register each functional node in the graph that represents a step in the agent workflow\n",
        "adaptive_rag.add_node(\"rewrite_query_vectordb\", rewrite_query_vectordb)  # rewrite query for vector db\n",
        "adaptive_rag.add_node(\"rewrite_query_web_search\", rewrite_query_web_search)  # rewrite query for web search\n",
        "adaptive_rag.add_node(\"retrieve_vectordb\", retrieve_vectordb)  # retrieve from vector db\n",
        "adaptive_rag.add_node(\"grade_documents\", grade_documents) # grade documents\n",
        "adaptive_rag.add_node(\"web_search\", web_search)  # web search\n",
        "adaptive_rag.add_node(\"generate_answer\", generate_answer)  # generate answer\n",
        "adaptive_rag.add_node(\"grade_hallucinations\", grade_hallucinations)  # check for hallucinations\n",
        "\n",
        "\n",
        "# Define the router function that directs the flow based on best possible route to answer the user query\n",
        "def route_user_query(state):\n",
        "    \"\"\"\n",
        "    Route question to web search or RAG.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ROUTE USER QUESTION---\")\n",
        "    question = state[\"orig_question\"]\n",
        "    source = qs_classifier.invoke({\"question\": question}) # uses our query source classifier workflow\n",
        "    if source.datasource == \"web_search\":\n",
        "        print(\"---ROUTING QUESTION TO WEB SEARCH WORKFLOW---\")\n",
        "        return \"web_search_route\"\n",
        "    elif source.datasource == \"vectordb\":\n",
        "        print(\"---ROUTE QUESTION TO STANDARD RAG + VECTOR DB WORKFLOW---\")\n",
        "        return \"vectordb_route\"\n",
        "\n",
        "# Add conditional edges based on the query routing function\n",
        "adaptive_rag.add_conditional_edges(\n",
        "    START,\n",
        "    route_user_query,\n",
        "    {\n",
        "        # if routing function returns web_search_route then call the rewrite_query_web_search node function\n",
        "        \"web_search_route\": \"rewrite_query_web_search\",\n",
        "        # if routing function returns vectordb_route then call the rewrite_query_vectordb node function\n",
        "        \"vectordb_route\": \"rewrite_query_vectordb\"\n",
        "    }\n",
        ")\n",
        "\n",
        "## Define edges for web search route\n",
        "adaptive_rag.add_edge(\"rewrite_query_web_search\", \"web_search\")\n",
        "adaptive_rag.add_edge(\"web_search\", \"generate_answer\")\n",
        "\n",
        "## Define edges for vectordb route\n",
        "adaptive_rag.add_edge(\"rewrite_query_vectordb\", \"retrieve_vectordb\")\n",
        "adaptive_rag.add_edge(\"retrieve_vectordb\", \"grade_documents\")\n",
        "\n",
        "# Define the router function that decides if web search or rag response generation should be next step in vectordb route\n",
        "def router_search_or_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-phrase a question for web search.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    web_search_needed = state[\"web_search_needed\"]\n",
        "\n",
        "    if web_search_needed == \"Yes\":\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\"---DECISION: SOME or ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, RUN rewrite_query_web_search NODE---\")\n",
        "        return \"goto_web_search\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: RELEVANT DOCUMENTS EXIST, RUN generate_answer NODE---\")\n",
        "        return \"goto_generate_answer\"\n",
        "\n",
        "# Add conditional edges based on the search or generate routing function\n",
        "adaptive_rag.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    router_search_or_generate,\n",
        "    {\n",
        "        # if routing function returns goto_web_search then call the rewrite_query_web_search node function\n",
        "        \"goto_web_search\": \"rewrite_query_web_search\",\n",
        "        # if routing function returns goto_generate_answer then call the generate_answer node function\n",
        "        \"goto_generate_answer\": \"generate_answer\"\n",
        "    }\n",
        ")\n",
        "\n",
        "adaptive_rag.add_edge(\"generate_answer\", \"grade_hallucinations\")\n",
        "\n",
        "# Define the router function that decides if answer needs to be regenerated or stop the agent\n",
        "def router_regenerate_or_stop(state):\n",
        "    \"\"\"\n",
        "    Determines whether to regenerate an answer when hallucinations occured, or stop the agent as we have the response.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS HALLUCINATION GRADE---\")\n",
        "    hallucination_grade = state[\"hallucination_grade\"]\n",
        "\n",
        "    if hallucination_grade == \"yes\":\n",
        "        # Last generated response has hallucinations so try to regenerate answer with feedback\n",
        "        print(\"---DECISION: HALLUCINATION EXISTS RERUNNING generate_answer NODE with hallucination_feedback---\")\n",
        "        return \"goto_regenerate_answer\"\n",
        "    else:\n",
        "        # We have the generated response which is hallucination free so stop the end\n",
        "        print(\"---DECISION: STOP AGENT---\")\n",
        "        return \"stop_agent\"\n",
        "\n",
        "# Add conditional edges based on the regenerate answer or stop agent routing function\n",
        "adaptive_rag.add_conditional_edges(\n",
        "    \"grade_hallucinations\",\n",
        "    router_regenerate_or_stop,\n",
        "    {\n",
        "        # if routing function returns goto_regenerate_answer then call the generate_answer node function\n",
        "        \"goto_regenerate_answer\": \"generate_answer\",\n",
        "        # if routing function returns stop_agent then go to the END node and stop executing the agent\n",
        "        \"stop_agent\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# Compile the agent graph\n",
        "adaptive_rag = adaptive_rag.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "id": "X3D6GCcN0ElZ",
        "outputId": "d9b39cb4-41a5-4486-c41a-cba2e4b047f5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "display(Image(adaptive_rag.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq4-5VuZwnI3"
      },
      "source": [
        "## Test the Agentic Adaptive RAG System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyqZtAgS3aA_"
      },
      "source": [
        "### Test Vector DB Route"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbddOYWp97II",
        "outputId": "0b42f3bf-f070-4f7d-fb5f-163aa1f338f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---ROUTE USER QUESTION---\n",
            "---ROUTE QUESTION TO STANDARD RAG + VECTOR DB WORKFLOW---\n",
            "---REWRITING USER QUERY FOR OPTIMAL RETRIEVAL FROM VECTOR DB---\n",
            "---RETRIEVAL FROM VECTOR DB---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---MOST DOCUMENTS (100.0%) ARE RELEVANT TO QUESTION - WEB SEARCH NOT NEEDED---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: RELEVANT DOCUMENTS EXIST, RUN generate_answer NODE---\n",
            "---GENERATE ANSWER---\n",
            "---STANDARD RAG FLOW - GENERATING ANSWER---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---HALLUCINATION CHECK: SUCCESSFUL - GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---ASSESS HALLUCINATION GRADE---\n",
            "---DECISION: STOP AGENT---\n"
          ]
        }
      ],
      "source": [
        "query = \"what is cot prompting?\"\n",
        "response = adaptive_rag.invoke({\"orig_question\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "89HhMPa935ux",
        "outputId": "8edce260-ecdf-4fe2-c7ef-17689533a32a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Chain-of-Thought (CoT) prompting is a technique used to enhance the reasoning capabilities of large language models. It involves providing the model with a series of intermediate reasoning steps, or a \"chain of thought,\" that lead to the final answer for a given problem. This approach is particularly useful for complex reasoning tasks, such as multi-step math word problems, where it is beneficial to break down the problem into smaller, manageable steps.\n",
              "\n",
              "The concept is inspired by the way humans solve complicated reasoning tasks by decomposing them into intermediate steps. For example, when solving a math problem, one might first calculate intermediate values before arriving at the final solution. CoT prompting aims to replicate this process in language models by including these intermediate steps in the prompt.\n",
              "\n",
              "Empirical evaluations have shown that CoT prompting significantly improves the performance of language models on various reasoning tasks, including arithmetic, commonsense, and symbolic reasoning. For instance, in experiments with the PaLM 540B model, CoT prompting achieved state-of-the-art accuracy on the GSM8K benchmark of math word problems, outperforming standard prompting methods.\n",
              "\n",
              "The key advantage of CoT prompting is that it allows language models to perform reasoning tasks without the need for extensive fine-tuning or large training datasets. Instead, a few examples of input-output pairs with intermediate reasoning steps are provided as exemplars, enabling the model to generalize and solve similar tasks effectively. This method highlights the potential of large language models to learn and perform complex reasoning tasks through natural language data and few-shot learning."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response['generation']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPKsAkX93_9h",
        "outputId": "b6486cca-9231-4f0c-dcc3-1836b925892c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---ROUTE USER QUESTION---\n",
            "---ROUTE QUESTION TO STANDARD RAG + VECTOR DB WORKFLOW---\n",
            "---REWRITING USER QUERY FOR OPTIMAL RETRIEVAL FROM VECTOR DB---\n",
            "---RETRIEVAL FROM VECTOR DB---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---MOST DOCUMENTS (100.0%) ARE RELEVANT TO QUESTION - WEB SEARCH NOT NEEDED---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: RELEVANT DOCUMENTS EXIST, RUN generate_answer NODE---\n",
            "---GENERATE ANSWER---\n",
            "---STANDARD RAG FLOW - GENERATING ANSWER---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---HALLUCINATION CHECK: SUCCESSFUL - GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---ASSESS HALLUCINATION GRADE---\n",
            "---DECISION: STOP AGENT---\n"
          ]
        }
      ],
      "source": [
        "query = \"Compare Diffusions models vs GANs\"\n",
        "response = adaptive_rag.invoke({\"orig_question\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "aZIafbrF4K0i",
        "outputId": "13810935-a1ed-4540-d8ec-3888211366dc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Diffusion models and Generative Adversarial Networks (GANs) are two prominent approaches in the field of generative modeling, each with its own strengths and weaknesses.\n",
              "\n",
              "**Diffusion Models:**\n",
              "\n",
              "1. **Mechanism**: Diffusion models generate samples by reversing a gradual noising process. They start with a noisy sample and iteratively refine it to produce a clean image. This process is akin to denoising, where each step reduces the noise level in the sample.\n",
              "\n",
              "2. **Sample Quality**: Recent advancements have shown that diffusion models can achieve superior image sample quality compared to GANs. For instance, they have achieved impressive Fréchet Inception Distance (FID) scores on datasets like ImageNet, indicating high-quality image generation.\n",
              "\n",
              "3. **Training and Scalability**: Diffusion models have a stationary training objective, which makes them easier to train and scale compared to GANs. They are less prone to issues like mode collapse, which is a common problem in GANs.\n",
              "\n",
              "4. **Diversity vs. Fidelity**: Diffusion models can trade off diversity for fidelity using techniques like classifier guidance. This allows them to maintain high fidelity while covering a broader distribution of the data.\n",
              "\n",
              "5. **Sampling Speed**: A notable drawback of diffusion models is their slower sampling speed due to the multiple denoising steps required. However, techniques are being developed to reduce this gap.\n",
              "\n",
              "**GANs:**\n",
              "\n",
              "1. **Mechanism**: GANs consist of two neural networks, a generator and a discriminator, that are trained simultaneously. The generator creates samples, while the discriminator evaluates them against real data, pushing the generator to improve.\n",
              "\n",
              "2. **Sample Quality**: GANs have historically been the state-of-the-art for image generation tasks, producing high-fidelity images. They are particularly effective at generating sharp and realistic images.\n",
              "\n",
              "3. **Training Challenges**: GANs are notoriously difficult to train. They require careful tuning of hyperparameters and are susceptible to issues like mode collapse, where the generator produces limited diversity in its outputs.\n",
              "\n",
              "4. **Diversity vs. Fidelity**: GANs can trade off diversity for fidelity using techniques like the truncation trick, where the latent space is sampled from a truncated distribution to improve image quality at the cost of diversity.\n",
              "\n",
              "5. **Sampling Speed**: GANs are generally faster at sampling compared to diffusion models, as they do not require iterative refinement steps.\n",
              "\n",
              "In summary, while GANs have been the traditional choice for high-quality image generation, diffusion models are emerging as a strong alternative, offering better distribution coverage and easier training. However, diffusion models still face challenges in sampling speed, which is an area of active research."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response['generation']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYVIBmZDwzrP",
        "outputId": "f6e21485-0af5-4d97-a342-d17fbadb60e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---ROUTE USER QUESTION---\n",
            "---ROUTE QUESTION TO STANDARD RAG + VECTOR DB WORKFLOW---\n",
            "---REWRITING USER QUERY FOR OPTIMAL RETRIEVAL FROM VECTOR DB---\n",
            "---RETRIEVAL FROM VECTOR DB---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---MOST DOCUMENTS (80.0%) ARE RELEVANT TO QUESTION - WEB SEARCH NOT NEEDED---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: RELEVANT DOCUMENTS EXIST, RUN generate_answer NODE---\n",
            "---GENERATE ANSWER---\n",
            "---STANDARD RAG FLOW - GENERATING ANSWER---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---HALLUCINATION CHECK: SUCCESSFUL - GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---ASSESS HALLUCINATION GRADE---\n",
            "---DECISION: STOP AGENT---\n"
          ]
        }
      ],
      "source": [
        "query = \"Compare attention methods in transformer in detail\"\n",
        "response = adaptive_rag.invoke({\"orig_question\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "1b0_0B1W0BL7",
        "outputId": "33ec38c2-f67e-4797-97e7-cba0ab44af9b"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The Transformer model, introduced by Vaswani et al. in \"Attention Is All You Need,\" is a groundbreaking architecture that relies entirely on attention mechanisms, specifically self-attention, to process input and output sequences. This approach eliminates the need for recurrent or convolutional neural networks, which were traditionally used in sequence transduction models.\n",
              "\n",
              "### Attention Mechanisms in Transformers\n",
              "\n",
              "1. **Self-Attention**:\n",
              "   - **Definition**: Self-attention, or intra-attention, is a mechanism that relates different positions of a single sequence to compute a representation of the sequence.\n",
              "   - **Functionality**: It allows the model to weigh the importance of different words in a sequence when encoding a particular word. This is crucial for capturing dependencies regardless of their distance in the sequence.\n",
              "   - **Advantages**: Self-attention enables parallelization, as it does not rely on sequential processing like RNNs. This significantly speeds up training and inference.\n",
              "\n",
              "2. **Multi-Head Attention**:\n",
              "   - **Concept**: Instead of having a single attention mechanism, the Transformer uses multiple attention heads. Each head learns different aspects of the input data.\n",
              "   - **Implementation**: The input is linearly projected into multiple smaller dimensions, and attention is applied in parallel across these projections. The outputs are then concatenated and linearly transformed.\n",
              "   - **Benefits**: Multi-head attention allows the model to focus on different parts of the input sequence simultaneously, enhancing its ability to capture complex patterns and relationships.\n",
              "\n",
              "3. **Scaled Dot-Product Attention**:\n",
              "   - **Mechanism**: This is the core operation in the attention mechanism, where the dot products of the query with all keys are computed, scaled by the square root of the dimension of the keys, and passed through a softmax function to obtain the weights on the values.\n",
              "   - **Purpose**: The scaling factor helps in stabilizing the gradients during training, especially when the dimensionality of the keys is large.\n",
              "\n",
              "### Encoder and Decoder Stacks\n",
              "\n",
              "- **Encoder**: Consists of a stack of identical layers, each with two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. Residual connections and layer normalization are applied around each sub-layer.\n",
              "  \n",
              "- **Decoder**: Similar to the encoder but includes an additional third sub-layer for multi-head attention over the encoder's output. It also incorporates masking in the self-attention sub-layer to prevent positions from attending to subsequent positions, ensuring that predictions for a position depend only on known outputs.\n",
              "\n",
              "### Advantages of Transformer Attention Mechanisms\n",
              "\n",
              "- **Parallelization**: Unlike RNNs, Transformers allow for parallel processing of data, which significantly reduces training time.\n",
              "- **Long-Range Dependencies**: The attention mechanism can capture dependencies between distant positions in the sequence, which is challenging for RNNs.\n",
              "- **Scalability**: The architecture can be scaled by increasing the number of attention heads or layers, allowing it to handle more complex tasks.\n",
              "\n",
              "In summary, the attention mechanisms in Transformers, particularly self-attention and multi-head attention, provide a powerful and efficient way to model sequence data, leading to superior performance in tasks like machine translation and beyond."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response['generation']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibr_w81w0XaW",
        "outputId": "2a913017-40f9-4bff-c1b0-b6db3bb0d7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---ROUTE USER QUESTION---\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---ROUTE QUESTION TO STANDARD RAG + VECTOR DB WORKFLOW---\n",
            "---REWRITING USER QUERY FOR OPTIMAL RETRIEVAL FROM VECTOR DB---\n",
            "---RETRIEVAL FROM VECTOR DB---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---SEVERAL DOCUMENTS (60.0%) ARE NOT RELEVANT TO QUESTION - WEB SEARCH NEEDED---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: SOME or ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, RUN rewrite_query_web_search NODE---\n",
            "---REWRITING USER QUERY FOR OPTIMAL RETRIEVAL FOR WEB SEARCH---\n",
            "---WEB SEARCH---\n",
            "---GENERATE ANSWER---\n",
            "---STANDARD RAG FLOW - GENERATING ANSWER---\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-S87Jqg5ocbUr83IVDwKuwC46 on tokens per min (TPM): Limit 30000, Requested 63974. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mCompare LoRA vs QLoRa vs Prompt Tuning in PEFT\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[43madaptive_rag\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morig_question\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2669\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[39m\n\u001b[32m   2667\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2668\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2669\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2670\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2673\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2675\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2679\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2323\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   2317\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2318\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2319\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2320\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2321\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2322\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2323\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2324\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2325\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2330\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:146\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    144\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:600\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    596\u001b[39m config = patch_config(\n\u001b[32m    597\u001b[39m     config, callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    598\u001b[39m )\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    602\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:365\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m         ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse:\n\u001b[32m    367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mgenerate_answer\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# standard RAG generation\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m---STANDARD RAG FLOW - GENERATING ANSWER---\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     generation = \u001b[43mqa_rag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Uncomment the following code line for the last demo below\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# This is to simulate a hallucinated answer to check how the agent handles it\u001b[39;00m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# generation = 'The observer and factory pattern are the main design patterns'\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mgeneration\u001b[39m\u001b[33m\"\u001b[39m: generation}\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3046\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3044\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3045\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3046\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3047\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3048\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:980\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    973\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    977\u001b[39m     **kwargs: Any,\n\u001b[32m    978\u001b[39m ) -> LLMResult:\n\u001b[32m    979\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:799\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    797\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    798\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m         )\n\u001b[32m    806\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    807\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1045\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1043\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1045\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1049\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:925\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    923\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1135\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1092\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1132\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1133\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1134\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\2. My Github Codebase\\LangGraph_Demystified\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-S87Jqg5ocbUr83IVDwKuwC46 on tokens per min (TPM): Limit 30000, Requested 63974. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
            "During task with name 'generate_answer' and id 'ae12eac8-5a80-f3fc-d181-a71e60b8ee0a'"
          ]
        }
      ],
      "source": [
        "query = \"Compare LoRA vs QLoRa vs Prompt Tuning in PEFT\"\n",
        "response = adaptive_rag.invoke({\"orig_question\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "bfynqDEc1WO5",
        "outputId": "21aeca69-46dd-4e2e-9c0d-06099b4f9291"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LoRA, QLoRA, and Prompt Tuning are all techniques under the umbrella of Parameter Efficient Fine-Tuning (PEFT) methods, which aim to fine-tune large language models efficiently by reducing the number of trainable parameters. Here's a comparison of these techniques:\n",
              "\n",
              "1. **LoRA (Low-Rank Adaptation):**\n",
              "   - **Approach:** LoRA introduces low-rank matrices to approximate the weight updates needed during fine-tuning. Instead of updating the full weight matrix, LoRA updates two smaller matrices, which are then used to compute the weight updates.\n",
              "   - **Efficiency:** This method significantly reduces the number of trainable parameters, making it more memory and compute-efficient compared to full fine-tuning.\n",
              "   - **Use Case:** LoRA is particularly useful when the task requires significant changes to the model's behavior, as it allows for more substantial updates than prompt-based methods.\n",
              "\n",
              "2. **QLoRA (Quantized Low-Rank Adaptation):**\n",
              "   - **Approach:** QLoRA builds on LoRA by incorporating quantization techniques. It uses 4-bit quantization for the model weights, further reducing memory usage while maintaining performance.\n",
              "   - **Efficiency:** QLoRA allows for fine-tuning large models on hardware with limited memory, such as a single GPU, by reducing the precision of the weights.\n",
              "   - **Use Case:** Ideal for scenarios where memory resources are constrained, and the model needs to be fine-tuned on large datasets or tasks.\n",
              "\n",
              "3. **Prompt Tuning:**\n",
              "   - **Approach:** Prompt Tuning involves optimizing a set of continuous prompt tokens that are prepended to the input. These tokens are learned during fine-tuning and help guide the model's responses.\n",
              "   - **Efficiency:** This method is highly parameter-efficient as it only involves learning a small number of additional parameters (the prompt tokens).\n",
              "   - **Use Case:** Best suited for tasks where the model's existing capabilities are sufficient, and the main challenge is to guide the model's output more effectively through optimized prompts.\n",
              "\n",
              "**Comparison:**\n",
              "- **Parameter Efficiency:** Prompt Tuning is the most parameter-efficient, as it only involves learning a few prompt tokens. LoRA and QLoRA involve more parameters but still significantly fewer than full fine-tuning.\n",
              "- **Memory Usage:** QLoRA is the most memory-efficient due to its use of quantization, making it suitable for very large models.\n",
              "- **Task Adaptability:** LoRA and QLoRA are better suited for tasks that require substantial model adaptation, while Prompt Tuning is ideal for tasks where the model's existing knowledge is largely sufficient.\n",
              "\n",
              "In summary, the choice between these methods depends on the specific requirements of the task, the available computational resources, and the degree of model adaptation needed."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response['generation']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gglo2FwJc2is",
        "outputId": "9acb952d-21b1-4593-d931-f62f30782423"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'orig_question': 'Compare LoRA vs QLoRa vs Prompt Tuning in PEFT',\n",
              " 'rephrased_question': 'LoRA vs QLoRa vs Prompt Tuning in PEFT comparison',\n",
              " 'generation': \"LoRA, QLoRA, and Prompt Tuning are all techniques under the umbrella of Parameter Efficient Fine-Tuning (PEFT) methods, which aim to fine-tune large language models efficiently by reducing the number of trainable parameters. Here's a comparison of these techniques:\\n\\n1. **LoRA (Low-Rank Adaptation):**\\n   - **Approach:** LoRA introduces low-rank matrices to approximate the weight updates needed during fine-tuning. Instead of updating the full weight matrix, LoRA updates two smaller matrices, which are then used to compute the weight updates.\\n   - **Efficiency:** This method significantly reduces the number of trainable parameters, making it more memory and compute-efficient compared to full fine-tuning.\\n   - **Use Case:** LoRA is particularly useful when the task requires significant changes to the model's behavior, as it allows for more substantial updates than prompt-based methods.\\n\\n2. **QLoRA (Quantized Low-Rank Adaptation):**\\n   - **Approach:** QLoRA builds on LoRA by incorporating quantization techniques. It uses 4-bit quantization for the model weights, further reducing memory usage while maintaining performance.\\n   - **Efficiency:** QLoRA allows for fine-tuning large models on hardware with limited memory, such as a single GPU, by reducing the precision of the weights.\\n   - **Use Case:** Ideal for scenarios where memory resources are constrained, and the model needs to be fine-tuned on large datasets or tasks.\\n\\n3. **Prompt Tuning:**\\n   - **Approach:** Prompt Tuning involves optimizing a set of continuous prompt tokens that are prepended to the input. These tokens are learned during fine-tuning and help guide the model's responses.\\n   - **Efficiency:** This method is highly parameter-efficient as it only involves learning a small number of additional parameters (the prompt tokens).\\n   - **Use Case:** Best suited for tasks where the model's existing capabilities are sufficient, and the main challenge is to guide the model's output more effectively through optimized prompts.\\n\\n**Comparison:**\\n- **Parameter Efficiency:** Prompt Tuning is the most parameter-efficient, as it only involves learning a few prompt tokens. LoRA and QLoRA involve more parameters but still significantly fewer than full fine-tuning.\\n- **Memory Usage:** QLoRA is the most memory-efficient due to its use of quantization, making it suitable for very large models.\\n- **Task Adaptability:** LoRA and QLoRA are better suited for tasks that require substantial model adaptation, while Prompt Tuning is ideal for tasks where the model's existing knowledge is largely sufficient.\\n\\nIn summary, the choice between these methods depends on the specific requirements of the task, the available computational resources, and the degree of model adaptation needed.\",\n",
              " 'hallucination_grade': 'no',\n",
              " 'hallucination_feedback': \"The LLM-generated response is well-grounded in the context documents. It accurately describes LoRA, QLoRA, and Prompt Tuning as Parameter Efficient Fine-Tuning (PEFT) methods, and provides a correct comparison of their approaches, efficiency, and use cases. The details about LoRA and QLoRA, including the use of low-rank matrices and quantization, are consistent with the information in the context. Prompt Tuning is also correctly described as optimizing prompt tokens, aligning with the context's explanation.\",\n",
              " 'web_search_needed': 'Yes',\n",
              " 'documents': [Document(id='fdc86801-ec86-4413-b35b-bafabd8e48ee', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers/peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 10, 'producer': 'PDFium', 'source': 'research_papers/peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='to achieve, LoRAPruning enables the weights to be merged\\neasily. Additionally, this work also introduces a novel criterion\\nthat utilizes LoRA’s gradients as an approximation of the\\ngradients for the pre-trained weights, enabling the estimation\\nof weight importance. ProPETL [121] constructs a single\\nshared prototype (e.g., adapter, prefix, or LoRA) across layers\\nand tasks. In addition, ProPETL learns binary masks to prune\\ndifferent sub-networks in different layers and tasks. As a result,\\nthe parameters can be reused across layers and tasks, largely\\nincreasing the parameter efficiency.\\nC. Quantization Strategies for PEFT\\nQuantization serves as another popular technique for im-\\nproving computational efficiency and reducing memory us-\\nage. For example, by investigating the loss landscape of\\nadapters, BI-Adapter [122] finds that adapters are resistant\\nto noise in parameter space. Building on this insight, the\\nauthors introduce a clustering-based quantization approach.\\nRemarkably, they demonstrate that a 1-bit quantization of\\nadapters not only minimizes storage requirements but also\\nachieves superior performance among all precision settings.\\nPEQA (Parameter-Efficient and Quantization-aware Adapta-\\ntion) [123] uses a two-stage pipeline to achieve parameter-\\nefficient and quantization-aware fine-tuning. In the first stage,\\nthe pre-trained FFN weight matrix W P Rnˆm is quantized\\nto W “ s ¨ W, where s P Rnˆ1 represents per-channel\\nscales and W denotes the quantized weight. In the second\\nstage, W remains fixed, and fine-tuning is only conducted\\non s. This approach not only ensures memory efficiency but\\nalso facilitates parameter efficiency. QLoRA [124] proposes\\nseveral novel techniques, including a 4-bit NormalFloat, a\\nDouble Quantization, and a Paged Optimizers, to backprop-\\nagate a 4-bit quantized pretrained language model into LoRA.'),\n",
              "  Document(id='b1026d34-375a-4502-8701-fc948eb0bd42', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers/peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 11, 'producer': 'PDFium', 'source': 'research_papers/peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='12\\nThese techniques enable the fine-tuning for a 65B language\\nmodel on a single 48GB GPU while maintaining similar\\nperformance to the full 16-bit fine-tuning. Similar to the\\noriginal implementation [76], QLoRA attaches the fixed zero-\\ninitialized LoRA weights to the quantized pre-trained model\\nas the training start point. However, when applying the ex-\\ntreme low-bit (e.g., 2-bit) quantization, the huge quantization\\nerror can adversely impact the initialization of LoRA fine-\\ntuning, i.e., quantizationpW0q ` WdownWup ‰ W0 where\\nWdown “ 0, which will harm the fine-tuning performance as\\nshown in the work by [134]. To solve this, several quanti-\\nzation strategies are proposed to eliminate the quantization\\nerror. For example, LoftQ (LoRA-Fine-Tuning-aware Quanti-\\nzation) [125] presents an innovative framework that provides\\na superior initialization point of quantized backbone weights\\nand LoRA weights for subsequent LoRA fine-tuning. This\\napproach addresses the discrepancies caused by quantization\\nthrough the optimization of a Frobenius norm objective during\\nnetwork initialization, which takes both the LoRA weights\\nand the quantized pre-trained backbone into consideration.\\nLoftQ exhibits superior performance in 2-bit quantization over\\nQLoRA, as well as greater generalization for downstream\\ntasks. LQ-LoRA [126] uses an iterative algorithm inspired\\nby robust principal components analysis [141], [142] which\\ndecomposes the weight W0 such that W0 « Q ` L1L2\\nto resolve the inaccuracy caused by the quantization error,\\nwhere Q is the quantized component which remains fixed\\nand L1L2 is the trainable low-rank component. Moreover, this\\napproach leverages integer linear programming to determine\\na mixed quantization strategy, enabling dynamic quantization\\nconfigurations for each weight matrix while adhering to a\\npredetermined total bit rate limit. QA-LoRA [127] address\\nanother limitation of QLoRA, which struggles to preserve its\\nquantized property post-fine-tuning. In QLoRA, the quantized\\npre-trained weight (NF4) has to be recovered to FP16 to match\\nthe LoRA weight precision (FP16) during weight merging.\\nInstead, QA-LoRA uses INT4 quantization and introduces\\ngroup-wise operators to enable quantization during the infer-\\nence stage, therefore improving the efficiency and accuracy\\ncompared with QLoRA. BitDelta [130] introduces a novel 1-\\nbit post-training quantization method that acts on the weight\\ndelta between a fine-tuned model and its underlying pre-\\ntrained model. Specifically, given the weight matrices Wfine\\nand Wbase from the fine-tuned and base models respectively,\\nthe weight delta ∆“ Wfine ´ Wbase is binarized as ˆ∆“ α d\\nSignp∆q. Here, α, a high-precision scalar, is initialized based\\non the mean absolute delta value α “\\n1\\nnm\\nř\\nij |Wij|, with\\nSignp¨q indicating the sign of ∆. BitDelta further calibrates\\nthe scaling factors via distillation on a compact calibration\\ndataset, while the binary matrices remain unchanged. This\\napproach notably streamlines the deployment of multiple fine-\\ntuned models on shared servers by utilizing a singular full-\\nprecision base model alongside efficiently batched 1-bit deltas.\\nD. Memory-efficient PEFT Methods\\nFine-tuning the full LLMs necessitates substantial training\\nmemory owing to their considerable size. While most PEFT\\nmethods primarily target parameter efficiency, they still in-\\ncur a significant memory overhead during training because\\ngradient computation and backpropagation are still necessary\\nfor these methods. For example, prevalent PEFT techniques\\nsuch as adapters and LoRA can only reduce memory usage\\nto approximately 70% compared to full model fine-tuning ac-\\ncording to some literature [132], [137]. From a computational\\nperspective, memory efficiency also remains a critical factor\\nthat cannot be overlooked.\\nTo improve memory efficiency, various techniques have\\nbeen developed to minimize the need for caching gradi-\\nents for the entire LLM during fine-tuning, thereby reducing'),\n",
              "  Document(metadata={}, page_content=\"Guide to fine-tuning LLMs using PEFT and LoRa techniques\\n\\nServicesBlogContact\\n\\n\\nGuide to fine-tuning LLMs using PEFT and LoRa techniques\\nPranav Patel\\nLarge Language Models\\nWhat is Fine-tuning?\\nHow does Fine-tuning work?\\nWhy use Fine-Tuning?\\nWhat is PEFT?\\nWhy PEFT?\\nTransfer Learning\\nHow Transfer Learning Works\\nAdaptersLoRA - Low-Rank Adaptation\\nEfficiency of LoRA\\nLoRA in Stable Diffusion\\nIA3 - Infused Adapter by Inhibiting and Amplifying Inner Activations\\nEfficiency of IA3\\nP-Tuning\\nEfficiency of P-Tuning\\nPrefix Tuning\\nEfficiency of Prefix Tuning\\nPrompt Tuning\\nEfficiency of Prompt Tuning\\nLoRA vs Prompt TuningLoRA and PEFT in comparison to full FinetuningWhy you should Fine-tune models for your business use caseWant to Train Custom LLMs with PEFT?\\nLarge Language Models (LLMs) like GPT are getting only larger in size. Even open-source models like MPT and Falcon have reached 30 and 40 billion parameters respectively. With size, the capabilities and complexities of these models have also increased. But this increased complexity and model size can also create challenges. Training larger models requires more extensive data sets, and as the model grows, more parameters must be tuned. This can be very compute-heavy and as a result costly too. This is where fine-tuning comes in. Fine-tuning is a technique that allows for the re-purposing of pre-trained models and can help reduce the complexity of building larger models.\\n\\u200d\\nIn this blog, we will discuss advanced fine-tuning techniques like PEFT (Parameter Efficient Fine-Tuning) and see how they can save you a ton of time and money on training massive LLMs.\\n\\u200d\\nWhat is Fine-tuning?\\nFine-tuning is the process of taking a model that is already trained on some task and then tweaking it to perform a similar task. It is often used when a new dataset or task requires the model to have some modifications, or when the model is not performing well on a specific task.\\n\\u200d\\nFor example, a model trained to generate stories can be fine-tuned to generate poems. This is possible because the model has already learned how to generate casual language and write stories, this skill can also be used to generate poems if the model is tweaked properly.\\nHow does Fine-tuning work?\\nAs mentioned, fine-tuning is tweaking an already-trained model for some other task. The way this works is by taking the weights of the original model and adjusting them to fit a new task.\\n\\u200d\\nModels when trained learn to do some specific task, for example, GPT-3 has been trained on a massive dataset and as a result, it has learned to generate stories, poems, songs, letters, and a lot of other things. One can take this ability of GPT-3 and fine-tune it on a specific task like generating answers to customer queries in a specific manner.\\n\\u200d\\nThere are different ways and techniques to fine-tune a model, the most popular being transfer learning. Transfer learning comes out of the computer vision world, it is the process of freezing the weights of the initial layers of a network and only updating the weights of the later layers. This is because the lower layers, the layers closer to the input, are responsible for learning the general features of the training dataset. And the upper layers, closer to the output, learn more specific information which is directly tied to generating the correct output.\\n\\u200d\\nHere is a quick visualization of how fine-tuning works:\\n\\nAlammar, J (2018).\\xa0 The Illustrated Transformer [Blog post].\\nWhy use Fine-Tuning?\\nAs the model size increases, it becomes more costly and time-consuming to train it. And with more size it requires more training data, otherwise, models usually overfit and generate poor results in a production environment. Fine-tuning allows us to not run into these issues by efficiently using a pre-trained model for our purposes. Here are some reasons why you should consider fine tuning instead of training a model from scratch:\\nLarger models generalize to downstream tasks well\\nWe all know how large models like GPT-3 and GPT-4 can perform really well on complicated tasks. This is because they have very sophisticated architectures and are trained on massive datasets, this helps them generalize on a lot of tasks really well. These models understand the underlying properties of language and that helps them learn any new tasks with minimal effort like prompt engineering.\\n\\u200d\\nBut if we want to use these models for some very specific tasks, like building a legal contract generator, you should probably fine-tune the model instead of using prompt engineering. This is because a model performing well in a very general task like language generation will perform well in a downstream task like generating legal contracts.\\n\\u200d\\nCheaper than training a whole model\\nAs mentioned before, these large models can be very expensive to train from scratch. Also very time-consuming. It is always cheaper to train an already-trained model. This also allows you to leverage what is already out there instead of doing everything yourself. Most of the time good datasets can be very hard and time-consuming to build. Open-source models like MPT and LLaMA have already been trained and made sure that they work well by some of the best researchers out there. It is very easy to load and train them in a cloud infrastructure.\\nGood for online training\\nOne of the biggest challenges in AI is to keep the model up to date with the latest data. Models when deployed in production can start degrading in performance if not updated regularly. For example, if you deploy an AI model to predict customer behavior in a store, it might stop performing well once the store is restocked with products with different prices or if they introduce new products in the store. This is a classic example of how changes in data can drastically change the performance of a model.\\n\\u200d\\nFine-tuning can help you to keep updating the model with the latest data without having to re-train the whole model. This makes it possible to deploy models in production without much effort and cost. This is called online learning or online training and is absolutely necessary for any model in production.\\n\\u200d\\n\\nWhat is PEFT?\\nPEFT, Parameter Efficient Fine-Tuning, is a set of techniques or methods to fine-tune a large model in the most compute and time-efficient way possible, without losing any performance which you might see from full fine-tuning. This is done because with models growing bigger and bigger like BLOOM which has a whopping 176 billion parameters, it is almost impossible to finetune them without spending tens of thousands of dollars. But it is sometimes almost necessary to use such big models for better performance. This is where PEFT comes in. It helps you solve the problems faced during such big models.\\n\\u200d\\nHere are some PEFT techniques:\\n\\u200d\\n\\nWhy PEFT?\\nAs mentioned above, it has become a necessity to fine-tune and use bigger models when it comes to production-grade applications. PEFT techniques allow you to fine-tune the models efficiently and save money and time as a result. This is done by fine-tuning only the most important and relevant parameters in the neural network. The techniques introduce new parameters in the network or freeze the whole model except for some parts to make it easier to train the model.\\n\\u200d\\nTransfer Learning\\nTransfer learning is when we take some of the learned parameters of a model and use them for some other task. This sounds similar to fine-tuning but is different. In finetuning, we re-adjust all the parameters of the model or freeze some of the weights and adjust the rest of the parameters. But in fine-tuning, we use some of the learned parameters from a model and use them in other networks. This gives us more flexibility in terms of what we can do. For example, we cannot change the architecture of the model when fine-tuning, this limits us in many ways. But when using transfer learning, we use only a part of the trained model, which we can then attach to any other model with any architecture.\\nHow Transfer Learning Works\\nTransfer learning has been a common practice in the computer vision world for a very long time now. This is because of the nature of the visual models and how they learn. In CNN models, the early layers extract more general features like edges and curves, whereas the later layers extract more complicated features like whole eyes and faces. This is because the receptive field of CNNs grows as they are stacked on top of each other.\\n\\u200d\\n\\n\\u200d\\nLet’s say for example you are trying to train a neural network to classify if a vehicle in front of you is a car or a motorbike. This is a very basic task. But let’s say you have very limited data and you don’t want to train your model too much. Here is what a basic CNN network looks like.\\n\\u200d\\n\\n\\u200d\\nThere are 2 major parts of the network here, the CNN head and the later fully connected layers. As mentioned, CNN layers extract representations of the data which then are used by the fully connected network to classify the image. Here we can use any other CNN network trained on a similar classification problem and use that as the CNN head for this new problem.\\n\\u200d\\n\\n\\u200d\\nHere as you can see, we are using transfer learning by using the weights of a network pretrained to classify the car type. We are only freezing the first two layers of the CNN network, and leaving the latter two free to be updated during the training process. This makes sure that the CNN head of the model learns new features from the images which might be necessary for the new task we are training the model for.\\n\\u200d\\nTransfer learning is also often seen in NLP tasks with LLMs where people use the encoder part of the transformer network from a pretrained model like T5 and train the later layers.\\nAdapters\\nAdapters were one of the first parameter-efficient fine-tuning techniques released. In the paper, they showed that you can add more layers to the pre-existing transformer architecture and only finetune them instead of the whole model. They showed that this technique resulted in similar performance when compared to complete fine-tuning.\\n\\u200d\\n\\n\\u200d\\nOn the left, there is the modified transformer architecture with added adapter layers. You can see adapter layers are added after the attention stack and the feed-forward stack. And on the right, you can see the architecture of the adapter layer itself. The adapter layer comprises a bottleneck architecture, it takes the input and narrows it down to a smaller dimension representation and then passes it through a non-linear activation function, and then scales it back up to the dimension of the input. This makes sure that the next layer in the transformer stack will be able to receive the generated output from the adapter layer.\\n\\u200d\\nIn the paper, the authors show that this method of fine-tuning is comparable to complete fine-tuning while consuming much less compute resources and training time. They were able to attain 0.4% of full fine-tuning on the GLUE benchmark while adding 3.6% of the parameters.\\n\\u200d\\n\\n\\u200d\\nLoRA - Low-Rank Adaptation\\nLoRA is a similar strategy to Adapter layers but it aims to further reduce the number of trainable parameters. It takes a more mathematically rigorous approach. LoRA works by modifying how the updatable parameters are trained and updated in the neural network.\\n\\u200d\\nLet’s explain mathematically, you can skip to the next paragraph if you are not interested. We know that the weights matrices of a pretrained neural network are full rank, meaning each weight is unique and can't be made by combining other weights. But in this paper authors showed that when pretrained language models are adjusted to a new task the weights have a lower “intrinsic dimension”. Meaning, that the weights can be represented in a smaller matrix, or that it has a lower rank. This in turn means that during backpropagation, the weight update matrix has a lower rank, as most of the necessary information has already been captured by the pre-training process and only task-specific adjustments are made during fine-tuning.\\n\\u200d\\nA much simpler explanation is that during finetuning only a very few weights are updated a lot as most of the learning is done during the pretraining phase of the neural network. LoRA uses this information to reduce the number of trainable parameters.\\n\\u200d\\n\\nThe image above gives a visual representation of what LoRA is doing. The Δ_WAxB_ is the weight updation matrix, these are the changes needed to be applied to the neural network in order for it to learn a new task. This matrix can be broken down into two matrices and then we can only train them and then use them to get back our weight updation matrix. As you can see in the image, the matrix is broken down into matrices with columns and rows r, it can be understood as the rank of the weight updation matrix if it was actually trained. The bigger the rank, the more parameters will be updated during training.\\nEfficiency of LoRA\\nAuthors in the paper show that LoRA can outperform full finetuning with only 2% of total trainable parameters.\\n\\u200d\\n\\n\\u200d\\nAs for the number of parameters it trains, we can largely control that using the rank r parameter. For example, let’s say the weight updation matrix has 100,000 parameters, A being 200 and B being 500. The weight updation matrix can be decomposed into smaller matrixes of lower dimensions, A being 200 x 3 and B being 3 x 500. This gives us 200 x 3 + 3 x 500 = 2100 trainable parameters only, which is only 2.1% of the total number of parameters. This can be further reduced as we can decide to only apply LoRA to specific layers only.\\n\\u200d\\nAs the number of parameters trained and applied are MUCH smaller than the actual model, the files can be as small as 8MB. This makes loading, applying, and transferring the learned models much easier and faster.\\n\\u200d\\nYou can read the LoRA paper if you want to learn more and do a deeper dive into the topic.\\nLoRA in Stable Diffusion\\nOne of the most interesting use cases of LoRA can be shown in image generation applications. Images have an inherent style that can be visually seen. Instead of training massive models to get specific styles of images out of models, users can now only train LoRA weights and use them with techniques like Dreambooth to achieve really good quality images with a lot of customizability.\\n\\u200d\\nLoRA weights can also be combined with other LoRA weights and be used in a weighted combination to generate images that carry multiple styles. You can find a ton of LoRA adapters online and load them into your models on CivitAI.\\n\\u200d\\n\\n\\u200d\\nIA3 - Infused Adapter by Inhibiting and Amplifying Inner Activations\\nIA3 is an adapter-based technique that is somewhat similar to LoRA. The goal of the authors was to replicate the advantages of ICL (in context learning or Few-Shot prompting) without the issues that come with it. ICL can get messy in terms of cost and inference as it requires prompting the model with examples. Longer length prompts require more time and computation to process. But ICL is perhaps the easiest way to get started working with models.\\n\\u200d\\nIA3 works by introducing rescaling vectors that target the activations of the model. A total of 3 vectors are introduced, lv, ik, and lff. These vectors target the value, keys in the attention layer, and the non-linear layer in the dense layers. These vectors are multiplied elementwise to the default values in the model. Once injected, these parameters are then learned during the training process, while the rest of the model remains frozen. These learned vectors essentially rescale or optimize the targeted pretrained model weights for the task at hand.\\n\\u200d\\n\\n\\u200d\\nSo far this seems like a basic adapter type PEFT method. But that’s not all. The authors also use 3 loss terms to enhance the learning process. The 3 losses are LLM, LUL, and LLN. LLM is the standard cross-entropy loss, which increases the likelihood of generating the correct response. Then there is LUL which is Unlikelihood Loss. This loss term reduces the probability of incorrect outputs using Rank Classification. Finally, we have LLN, which is a length-normalized loss that applies a softmax cross-entropy loss to length-normalized log probabilities of all output choices. Multiple losses are used here to ensure faster and better learning of the model. Because we are trying learn using few-shot examples, these losses are necessary.\\nNow let’s talk about two very important concepts in IA3. Rank Classification and Length Normalization.\\n\\u200d\\nIn Rank Classification a model is asked to rank a set of responses by their correctness. This is done by calculating the probability scores for the potential responses. The LUL is then used to reduce the probability of the wrong responses and as a result, increase the probability of the correct response. But with Rank classification, we face a critical problem, which is that the responses with fewer tokens will rank higher, because of how probability works. A smaller amount of generated tokens ensures a higher probability as the probability of every generated token is < 1. To fix this, the authors propose dividing the score of the response by the number of tokens in the response. Doing this will normalize the scores. One very important thing to note here is that normalization is done over log probabilities, not raw probabilities. Log probabilities are negative and between zero to one.\\n\\u200d\\nEfficiency of IA3\\nIA3 just like LoRA reduces the number of trainable parameters. But instead of using low-rank matrices, IA3 uses rescaling vectors. This reduces the trainable parameters to about 0.01%, compared to LoRA's > 0.1%, for the T0 model trained in the paper. The frozen state of the LLM also provides us with the option of having multiple adapters for multiple use cases. Also, because the authors used element-wise multiplication, it is super easy to merge the adapter to the LLM weights because of the commutative property of multiplication.\\n\\u200d\\n\\n\\u200d\\nThe above figure shows that IA3 performs better than LoRA and barely affects the FLOPs. This makes IA3 a highly efficient and desirable technique. Also because IA3 is an additive adapter technique, just like LoRA we can target specific parts of the model and decide where to introduce the rescaling vectors. This helps us reduce the training time and even more.\\n\\u200d\\n\\u200d\\nP-Tuning\\nThe P-tuning method aims to optimize the representation of the prompt which is passed to the model. In the P-Tuning paper, the authors emphasize how prompt engineering is a very strong technique when working with large language models. The p-Tuning method builds up on top of prompt engineering and tries to further improve the effectiveness of a good prompt.\\n\\u200d\\nP-Tuning works by creating a small encoder network for your prompt that creates a soft prompt for your passed prompt. To tune your LLM using P-tuning, you are supposed to create a prompt template that represents your prompt. And a context x which is used in the template to get label y. This is the approach mentioned in the paper. The tokens used for the prompt template are trainable and learnable parameters, these are called pseudo tokens. We also add a prompt encoder which then helps us update pseudo tokens to the specific task at hand. The prompt encoder is usually a bi-LSTM network that learns the optimal representation of the prompt for the model and then passes the representation to it. The LSTM network is attached to the original model. Only the encoder network and the pseudo tokens are trained here, the weights of the original network remain unaffected. Once the training is done, the LSTM head is discarded as we have the hi which can be used directly.\\n\\u200d\\nIn short, the prompt encoder only changes the embeddings of the passed prompt to better represent the task, everything else remains unchanged.\\n\\u200d\\n\\nEfficiency of P-Tuning\\nIn terms of efficiency, P-tuning is just as good as any other method. In the paper, the authors show that P-Tuning was able to perform better than full fine-tuning on most of the benchmarks. It can be said that P-Tuning is comparable to the full fine-tuning of large language models.\\n\\u200d\\n\\n\\u200d\\nBut there is a core issue when it comes to P-Tuning. P-Tuning is a prompt optimization technique, it optimizes the prompt that is passed to the bigger model. This means that we are still largely based on the large model in terms of capability. If a model has not been trained on sentiment classification optimizing sentiment classification prompts using P-Tuning will not do a lot of good to the model. P-Tuning is an assistive technique. It is always very important to pick a model that can do the required task out of the box “well” with some prompt engineering, and then further optimize it.\\n\\u200d\\nPrefix Tuning\\nPrefix tuning can be considered the next version of P-Tuning. The authors of P-Tuning published a paper on P-Tuning V-2 addressing the issues of P-Tuning. In this paper, they implemented the Prefix tuning introduced in this paper. Prefix tuning and P-Tuning do not have a lot of differences but can still lead to different results. Let’s dive into a deeper explanation.\\n\\u200d\\n\\n\\u200d\\nIn P-Tuning, we added learnable parameters only to the input embeddings but in Prefix Tuning we add them to all the layers of the network. This ensures that the model itself learns more about the task it is being finetuned on. We append learnable parameters to the prompt and to every layer activation in the transformer layers. The difference from P-Tuning is that instead of completely modifying the prompt embeddings, we only add very few learnable parameters at the start of the prompt at every layer. Here’s a visual explanation:\\n\\u200d\\n\\n\\u200d\\nAt every layer in the transformer, we concatenate a soft prompt with the input which has learnable parameters. These learnable parameters are tuned using a very small MLP, only 2 fully connected layers. This is done because in the paper authors note that directly updating these prompt tokens is very sensitive to learning rate and initialization. The soft prompts increase the number of trainable parameters but substantially increase the learning ability of the model too. The MLP or fully connected layers can be dropped later as we only care about the soft prompts, which will be appended to the input sequences during inference and will guide the model.\\n\\u200d\\n\\n\\u200d\\nEfficiency of Prefix Tuning\\nPrefix tuning shows massive gains over P-Tuning. And as the model size increases, these gains increase too. This is perhaps because there are more trainable parameters for larger models. In the chart, you can see the authors compare the performance of P-Tuning, full finetuning, and Prefix tuning. Prefix tuning performs better than or as well as P-tuning in almost all tasks. In many cases, it performs even better than Full fine-tuning!\\n\\u200d\\n\\n\\u200d\\nOne big reason why prefix tuning works really well is that the number of trainable parameters is not limited only to the input sequence. Learnable parameters are added at every layer, making the model much more flexible. Prefix tuning, unlike P-tuning, not only affects the prompt tokens but also the model itself. This allows the model to learn more. But this approach is still largely based on the prompt. It is still suggested to take a model that can perform the task and only then optimize it, as that will lead to much better results. As for the size of parameters, the number of trained parameters increase substantially, from 0.01% to 0.1 to 3% parameters. But the size of parameters still remains small enough to be transferred and loaded easily and quickly.\\n\\u200d\\nPrompt Tuning\\nPrompt tuning was one of the first papers to build upon the idea of finetuning only with soft prompts. The ideas of P-Tuning and Prefix Tuning come from this paper. Prompt tuning is a very simple and easy-to-implement idea. It involves prepending a specific prompt to the input and using virtual tokens or new trainable tokens for that specific prompt. These new virtual tokens can be finetuned during the process to learn a better representation of the prompt. This means that the model is tuned to understand the prompt better. Here is a comparison of prompt tuning with full fine-tuning from the paper:\\n\\u200d\\n\\n\\u200d\\nHere you can see that full model tuning requires multiple copies of the model to exist if we want to use the model for multiple tasks. But with Prompt Tuning, you only need to store the learned virtual tokens of the prompt tokens. So for example, if you use a prompt like “Classify this tweet: {tweet}” the goal will be to learn new better embeddings for the prompt. And during inference, only these new embeddings will be used to generate the outputs. This allows the model to tune the prompt to help itself generate better outputs during inference.\\n\\u200d\\nEfficiency of Prompt Tuning\\nThe biggest advantage of using prompt tuning is the small size of learned parameters. The files can be in KBs. As we can determine the dimension size and number of parameters to use for the new tokens, we can greatly control the number of parameters we are going to learn. In the paper, the authors show how even with a very small number of trainable tokens method performs really well. And the performance only goes up as bigger models are used. You can read the paper here.\\n\\u200d\\n\\n\\u200d\\nAnother big advantage is that we can use the same model without any changes for multiple tasks, as the only thing being updated are the embeddings of the prompt tokens. Meaning you can use the same model for a tweet classification task and for a language generation task without any changes to the model itself, given the model is big and sophisticated enough to perform those tasks. But a big limitation is that the model itself doesn’t learn anything new. This is purely a prompt optimization task. This means if the model has never trained on a sentiment classification dataset, prompt tuning might not be of any help. It is very important to note that this method optimizes the prompts, not the model. So, if you cannot handcraft a hard prompt that can do the task relatively well, there is no use of trying to optimize for a soft prompt using prompt optimization techniques.\\n\\u200d\\nLoRA vs Prompt Tuning\\nNow we have explored various PEFT techniques. Now the question becomes whether to use an additive technique like Adapter and LoRA or you use a Prompt based technique like P-Tuning and Prefix Tuning.\\n\\u200d\\nOn comparing LoRA vs P-Tuning and Prefix Tuning, one can say for sure LoRA is the best strategy in terms of getting the most out of the model. But it might not be the most efficient based on your needs. If you want to train the model on a much different task than what it has been trained on, LoRA is without a doubt the best strategy for tuning the model efficiently. But if your task is more or less already understood by the model, but the challenge is to properly prompt the model, then you should use Prompt Tuning techniques. Prompt Tuning doesn’t modify many parameters in the model and mainly focuses on the passed prompt instead.\\n\\u200d\\nOne important point to note is that LoRA decomposes the weight updation matrix into smaller rank matrices and uses them to update the weights of the model. Even though trainable parameters are low, LoRA updates all the parameters in the targeted parts of the neural network. Whereas in Prompt Tuning techniques, a few trainable parameters are added to the model, this usually helps the model adjust to and understand the task better but does not help the model learn new properties well.\\n\\u200d\\nLoRA and PEFT in comparison to full Finetuning\\nPEFT, Parameter Efficient Fine Tuning, is proposed as an alternative to full Finetuning. For most of the tasks, it has already been shown in papers that PEFT techniques like LoRA are comparable to full finetuning, if not better. But, if the new task you want the model to adapt to is completely different from the tasks the model has been trained on, PEFT might not be enough for you. The limited number of trainable parameters can result in major issues in such scenarios.\\n\\u200d\\nIf you are trying to build a code generation model using a text-based model like LLaMA or Alpaca, you should probably consider fine-tuning the whole model instead of tuning the model using LoRA. This is because the task is too different from what the model already knows and has been trained on. Another good example of such a task is training a model, which only understands English, to generate text in the Nepali language.\\n\\u200d\\nWhy you should Fine-tune models for your business use case\\nFinetuning model is an important step for any business that wants to get the most out of its machine-learning applications. It allows you to customize the model to your specific use case, which can lead to improved accuracy and performance. It saves time, money, and resources by eliminating the need to build a new model from the ground up. Fine-tuning lets you optimize the use of your proprietary data, adjusting the model to better fit your available data, and even incorporating new data if needed. This ensures a more accurate model that better serves your business needs. Here are some more benefits:\\n\\u200d\\n\\nCustomization: Fine-tuning allows you to tailor the model to your specific needs, enhancing accuracy and performance.\\xa0\\nResource Efficiency: It saves time, money, and resources by eliminating the need to build a new model from scratch.\\xa0\\nPerformance Boost: Fine-tuning enhances the performance of the pretrained model using your unique datasets.\\xa0\\nData Optimization: It lets you make the most of your data, adjusting the model to better fit your available data, and even incorporating new data if needed.\\n\\n\\u200d\\nBut as the size of models grows to billions of parameters fine-tuning itself can be a challenge. The\\xa0 PEFT\\xa0 techniques we discussed in this blog help to reduce the time and resources needed to fine-tune a model. It helps speed up the training process by making use of the pretrained weights and parameters and allows you to fine-tune the model more efficiently. Also, using PEFT, you can easily transfer models over the internet and even use the same model for multiple purposes. PEFT opens up a whole new world of possibilities for businesses that want to make the most of their machine-learning applications.\\nWant to Train Custom LLMs with PEFT?\\nIf you want to build or train custom LLMs or Chatbots, we can help you fine-tune them to your specific needs. We have done a ton of work on building custom chatbots and training large language models. Contact us today and let us build a custom LLM that revolutionizes your business.\\n\\u200d\\nSubscribe to stay informed\\nSubscribe to our newsletter to stay updated on all things AI!\\nSubscribe\\nAwesome, you subscribed!\\nError! Please try again.\\n\\nLinkedInBlogResearchContact UsServices\\nTemplate pages\\nSubscribe to our newsletter!\\nSubscribe\\nAwesome, you subscribed!\\nError! Please try again.\\n© 2023, Mercity\\n\"),\n",
              "  Document(metadata={}, page_content='Sign up\\n\\nSign in\\n\\nSign up\\n\\nSign in\\n\\nHome\\n\\nFollowing\\n\\nLibrary\\n\\nStories\\n\\nStats\\n\\nOptimizing Language Model Fine-Tuning with PEFT, QLORA Integration, and Training Time Reduction using Unsloth: A Comprehensive Guide\\n\\nTejpal Kumawat\\n\\nFollow\\n\\n--\\n\\n2\\n\\nListen\\n\\nShare\\n\\nIntroduction:\\n\\nLarge language models (LLMs) have revolutionised the field of natural language processing with their comprehensive solutions and enhanced capabilities. These models perform exceptionally well on tasks like question-answering, summarization, translation, and text production since they were trained on large text datasets. Even with their strength, LLMs might not always fit into a particular task or domain.  We will examine how fine-tuning LLMs can lead to more accurate and context-specific outcomes, lower training costs, and much better model performance in this lesson.\\n\\nAbout Fine Tuning :\\n\\nAdjusting Using a smaller, domain-specific dataset, LLM entails retraining an existing model that has already learned patterns and features from a large dataset. When we talk about “LLM Fine-Tuning,” we’re talking about a “Large Language Model,” like OpenAI’s GPT series. This method is important since it saves a lot of time and processing power when training a big language model from scratch. By leveraging the pre-trained model’s incorporated current knowledge, good performance on certain tasks can be achieved with significantly less data and processing resources.\\n\\nAdjusting Natural language processing tasks including sentiment analysis, named entity recognition, summarization, translation, and other applications where producing coherent language and comprehending context are critical are frequent uses for LLMs. It facilitates the use of previously trained models’ expertise for more specialised and domain-specific activities.\\n\\nDifferent Ways of Fine Tuning any LLM\\n\\nFine-tuning a Large Language Model (LLM) involves a supervised learning process where a dataset with labeled examples is used to adjust the model’s weights, improving its performance in specific tasks. Let’s explore some notable techniques employed during the fine-tuning process.\\n\\n2. Parameter Efficient Fine-Tuning (PEFT):\\n\\nWhat is LORA and How it works?\\n\\nBefore it , What is quantization?\\n\\nconversion of higher memory format to lower memory format is quantization.\\n\\nfloating point 32 (7.2345_______)→ 32 bits is full precision/single precision\\n\\nWe use quantization to convert this 32 bits to 8 bits, this is very necessery to deploy our models on a edge device.\\n\\nLoRA is an improved finetuning method where instead of finetuning all the weights that constitute the weight matrix of the pre-trained large language model, two smaller matrices that approximate this larger matrix are fine-tuned. These matrices constitute the LoRA adapter. This fine-tuned adapter is then loaded into the pre-trained model and used for inference.\\n\\nAfter LoRA fine-tuning for a specific task or use case, the outcome is an unchanged original LLM and the emergence of a considerably smaller “LoRA adapter,” often representing a single-digit percentage of the original LLM size (in MBs rather than GBs).\\n\\nDuring inference, the LoRA adapter must be combined with its original LLM. The advantage lies in the ability of many LoRA adapters to reuse the original LLM, thereby reducing overall memory requirements when handling multiple tasks and use cases.\\n\\nIn this image you can see we have pretrained weights and LORA tracked weights, these LORA tracked weights will be decomposed in two smaller matrix A and B with the given hyperparameter RANK.\\n\\nHere is the explanation of Rank 1 matrix ,\\n\\nA rank-1 matrix is a matrix where the maximum rank is 1. In a 3x3 matrix with rank 1, it means that only one row (or column) is linearly independent, and the other two rows (or columns) are dependent on it.\\n\\nLet’s denote a 3x3 rank-1 matrix as A:\\n\\nFor a rank-1 matrix, there is only one linearly independent row (or column). Without loss of generality, let’s assume the first row (a, b, c) is linearly independent, and the other two rows are dependent on it.\\n\\nWhat is QLORA ?\\n\\nQLoRA represents a more memory-efficient iteration of LoRA. QLoRA takes LoRA a step further by also quantizing the weights of the LoRA adapters (smaller matrices) to lower precision (e.g., 4-bit instead of 8-bit). This further reduces the memory footprint and storage requirements. In QLoRA, the pre-trained model is loaded into GPU memory with quantized 4-bit weights, in contrast to the 8-bit used in LoRA. Despite this reduction in bit precision, QLoRA maintains a comparable level of effectiveness to LoRA.\\n\\nIn this blog we will first see the Fine tuning of LLM using PEFT with QLORA and later we will see how can we reduce the training time using unsloth.\\n\\nPART ONE: Parameter-efficient fine-tuning with QLoRA.\\n\\nNow let’s explore how we can fine-tune LLM on a custom dataset using QLoRA on a single GPU.\\n\\nSetting up the NoteBook\\n\\nYou can use Google Colab for this task , use its teslaV4 GPU capabilities\\n\\nInstall Necessery Libraries\\n\\nLet’s understand the importance of some of these libraries.\\n\\nLoading the required libraries\\n\\nLoading dataset\\n\\nNumerous datasets are available for fine-tuning the model. In this instance, we will utilize the DialogSum DataSet from HuggingFace for the fine-tuning process. DialogSum is an extensive dialogue summarization dataset, featuring 13,460 dialogues along with manually labeled summaries and topics.\\n\\nThere is no specific reason for selecting this dataset. Feel free to try this experiment with any custom dataset.\\n\\nLet’s execute the below code to load the above dataset from HuggingFace.\\n\\nOnce the dataset is loaded, we can take a look at it to understand what it contains:\\n\\nIt contains the below fields.\\n\\nCreate Bitsandbytes configuration\\n\\nTo load the model, we need a configuration class that specifies how we want the quantization to be performed. We’ll be using BitsAndBytesConfig to load our model in 4-bit format. This will reduce memory consumption considerably, at a cost of some accuracy.\\n\\nLoading the Pre-Trained model\\n\\nMicrosoft recently open-sourced the Phi-2, a Small Language Model(SLM) with 2.7 billion parameters. Here, we will use Phi-2 for the fine-tuning process. This language model exhibits remarkable reasoning and language understanding capabilities, achieving state-of-the-art performance among base language models.\\n\\nLet’s now load Phi-2 using 4-bit quantization from HuggingFace.\\n\\nThe model is loaded in 4-bit using the `BitsAndBytesConfig` from the bitsandbytes library. This is a part of the QLoRA process, which involves quantizing the pre-trained weights of the model to 4-bit and keeping them fixed during fine-tuning.\\n\\nTokenization\\n\\nNow, let’s configure the tokenizer, incorporating left-padding to optimize memory usage during training.\\n\\nTest the Model with Zero Shot Inferencing\\n\\nWe will evaluate the base model that we loaded above using a few sample inputs.\\n\\nFrom the observation above, it’s evident that the model faces challenges in summarizing the dialogue compared to the baseline summary. However, it manages to extract essential information from the text, suggesting the potential for fine-tuning the model for the specific task at hand.\\n\\nPreprartion of Data for the Fine Tuning\\n\\nThere are mainly two to prepare the dataset for the fine tuning , we will use first option\\n\\n2. Separated Format: Keeping instructions, inputs, and expected outputs in separate fields. This approach can offer more flexibility in how you present data to the model, potentially allowing for more complex instruction or feedback mechanisms during training.\\n\\nWe’ll create some helper functions to format our input dataset, ensuring its suitability for the fine-tuning process. Here, we need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM.\\n\\nThe above function can be used to convert our input into prompt format.\\n\\nNow, we will use our model tokenizer to process these prompts into tokenized ones.\\n\\nOur aim here is to generate input sequences with consistent lengths, which is beneficial for fine-tuning the language model by optimizing efficiency and minimizing computational overhead. It is essential to ensure that these sequences do not surpass the model’s maximum token limit.\\n\\nBy utilizing these functions, our dataset will be prepared for the fine-tuning process!\\n\\nPreparing the model for QLoRA\\n\\nHere, the model is prepared for QLoRA training using the `prepare_model_for_kbit_training()` function. This function initializes the model for QLoRA by setting up the necessary configurations.\\n\\nSetup PEFT for Fine-Tuning\\n\\nLet us now define the LoRA config for Fine-tuning the base model.\\n\\nNote the rank (r) hyper-parameter, which defines the rank/dimension of the adapter to be trained. r is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\\n\\nalpha here is the scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to the LoRA activations.\\n\\nOnce everything is set up and the PEFT is prepared, we can use the print_trainable_parameters() helper function to see how many trainable parameters are in the model.\\n\\ntrainable parameters\\n\\nTrain PEFT Adapter\\n\\nDefine training arguments and create Trainer instance.\\n\\nHere, we have used 1000 training steps. It seems to be good enough for our custom dataset. We need to try out different numbers before finalizing with training steps. Also, the hyperparameters used above might vary depending on the dataset/model we are trying to fine-tune. This is just to show the capability of fine-tuning.\\n\\nLet’s start the training now. Training the model will take some time depending upon the hyperparameters used in TrainingArguments.\\n\\nOnce the model is trained successfully, we can use it for inference. Let’s now prepare the inference model by adding an adapter to the original Phi-2 model. Here, we are setting is_trainable=False because the plan is only to perform inference with this PEFT model.\\n\\nFine-tuning is often an iterative process. Based on the validation and test sets results, we may need to make further adjustments to the model’s architecture, hyperparameters, or training data to improve its performance. Let’s now see how to evaluate the results of Fine-tuned LLM.\\n\\nEvaluation of our Model Outputs\\n\\nThere are two ways in which we can evaluate our fine tuned model\\n\\na) Evaluate the Model Qualitatively (Human Evaluation)\\n\\nNow, let’s perform inference using the same input but with the PEFT model, as we did previously in step 7 with the original model.\\n\\nb) Evaluate the Model Quantitatively (with ROUGE Metric)\\n\\nROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\\n\\nROUGE metric to quantify the validity of summarizations produced by models. It compares summarizations to a “baseline” summary which is usually created by a human. While it’s not a perfect metric, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.\\n\\nPART TWO: Reducing the Fine Tuning time of LLM with Unsloth\\n\\nIntroduction to Unsloth\\n\\nUnsloth stands out as an innovative framework that streamlines the fine-tuning process of language models, offering a straightforward approach to elevate model performance. This framework seamlessly integrates with popular open-source LLMs such as Mistral, Llama 2.0, and Gemma, providing researchers and practitioners in natural language processing with a versatile toolset for their endeavors.\\n\\nWhat sets Unsloth apart is its flexibility in fine-tuning methodologies, offering options like LoRa and DPO. This versatility empowers users to tailor the fine-tuning process to their specific needs, enhancing the adaptability of language models for diverse applications.\\n\\nA standout feature of Unsloth is its capability to operate efficiently with minimal memory usage during fine-tuning. This unique aspect opens up new possibilities for fine-tuning models on local GPUs and even on Colab GPU, making advanced natural language processing tasks accessible to a broader audience.\\n\\nFor this task I will be fine tuning gemma-2b , with databricks/databricks-dolly-15k dataset\\n\\nSetting up the Environment\\n\\nWe will be using Colab Notebook with free tier GPU (T5) for our end goal.Before delving into the fine-tuning process, the notebook begins by installing the Unsloth library and importing the necessary modules.The code showcases how to instantiate a FastLanguageModel, a component of Unsloth, with specific configurations such as maximum sequence length, data type, and 4-bit loading.In this article, we will be choosing the latest state-of-the-art LLM model: Gemma, which was released by Google.\\n\\nFastLanguageModel object provides a get_peft_model attribute where we can configure various parameters for finetuning, such as the number of attention heads, target modules, dropout rate, LoRa alpha and more.The use of gradient checkpointing and other advanced techniques showcases Unsloth’s capability to optimize model performance.\\n\\nData Preparation and Formatting\\n\\nIn order to finetune, we need to give LLM the access to an unseen dataset with a prompt template that it will get trained on.In our case, we are using a text generation dataset : databricks/databricks-dolly-15k\\n\\nTo make the data accessible to the model, we undertake the task of mapping all datapoints into a standardized format. This involves ensuring a consistent prompt structure with labeled inputs and outputs, specifically categorized as Instruction, Context, and Response (in this scenario)This standardized format ensures effective utilization of the dataset during the fine-tuning phase.\\n\\nFinetuning of LLM\\n\\nThe penultimate step is initialising a Supervised Fine-tuning Trainer that aids in the fine-tuning process.It initialises the model, along with the dataset that it has to finetune on, along with a tokenizer and all the required Training Arguments (learning rate, maximum steps, weight decay, optimisation, etc).\\n\\nThe fine-tuning looks something like this:\\n\\nGenerating Responses\\n\\nWe conclude the process by showcasing the model’s response generation capabilities. It utilizes a prompt format, including instructions, context, and an initial response.\\n\\nTo sum up, this article offers an in-depth guide on the fine-tuning of language models utilizing Unsloth and with normal approach. Covering everything from the initial setup of the environment to the meticulous mapping of datasets, the article also delves into the crucial steps of instantiating the fine-tuning process and conducting inference from the model.\\n\\nReference:\\n\\nFine-Tuning Large Language Models with Unsloth\\n\\nIn this article, we will explore the fine-tuning process of large language models using Unsloth, a powerful framework…\\n\\nmedium.com\\n\\nGitHub — unslothai/unsloth: 5X faster 60% less memory QLoRA finetuning\\n\\nhttps://huggingface.co/docs/peft/conceptual_guides/lora?source=post_page-----fb60abdeba07--------------------------------\\n\\n--\\n\\n--\\n\\n2\\n\\nWritten by Tejpal Kumawat\\n\\nArtificial Intelligence enthusiast that is constantly looking for new challenges and researching cutting-edge technology to improve the world !!\\n\\nResponses (2)\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nRules\\n\\nTerms\\n\\nText to speech\\n\\n'),\n",
              "  Document(metadata={}, page_content='Optimizing LLMs: A Step-by-Step Guide to Fine-Tuning with PEFT and QLoRA\\nSkip to content\\n \\n\\nHome\\nGithub\\nDocumentation\\nDiscord\\nYouTube\\n\\nSign in Subscribe\\nLight Dark System\\n\\nOptimizing LLMs: A Step-by-Step Guide to Fine-Tuning with PEFT and QLoRA\\nBlog 14 min read Sep 27, 2023\\nA Practical Guide to Fine-Tuning LLM using QLora\\nConducting inference with large language models (LLMs) demands significant GPU power and memory resources, which can be prohibitively expensive. To enhance inference performance and speed, it is imperative to explore lightweight LLM models. Researchers have developed a few techniques. In this blog, we’ll delve into these essential concepts that enable cost-effective and resource-efficient deployment of LLMs.\\nWhat is Instruction Fine-Tuning?\\nInstruction fine-tuning is a critical technique that empowers large language models (LLMs) to follow specific instructions effectively. When we begin with a base model, pre-trained on an immense corpus of worldly knowledge, it boasts extensive knowledge but might not always comprehend and respond to specific prompts or queries. In essence, it requires fine-tuning to tailor its behavior.\\nWhen Does Instruction Fine-Tuning Work?\\nInstruction fine-tuning shines in specific scenarios:\\n\\nPrecision Tasks: When precision in responses is paramount, such as classifying, summarizing, or translating content, instruction fine-tuning significantly enhances accuracy.\\nComplex Tasks: For intricate tasks involving multiple steps or nuanced understanding, instruction fine-tuning equips the model to generate meaningful outputs.\\nDomain-Specific Tasks: In specialized domains, instruction fine-tuning enables the model to adapt to unique language and context.\\nTasks Requiring Improved Accuracy: When base model responses require refinement for higher accuracy, instruction fine-tuning becomes invaluable.\\n\\nWhen Might Instruction Fine-Tuning Fall Short?\\nDespite its advantages, instruction fine-tuning may face challenges in specific situations:\\n\\nSmaller Models: Instruction fine-tuning can be tough for smaller LLMs with fewer parameters, impacting performance.\\nLimited Space in Prompts: Long examples or instructions in prompts may reduce space for essential context.\\nHigh Memory and Compute Demands: Full fine-tuning, and updating all model weights, needs significant memory and computation, which may not work in resource-limited setups.\\n\\nMitigating Catastrophic Forgetting in Fine-Tuning\\nWhile instruction fine-tuning is a potent tool for boosting LLM performance, it’s crucial to address a potential challenge called catastrophic forgetting. This issue can affect the model’s ability to generalize to other tasks, necessitating strategies to minimize its impact.\\nWhen Does Catastrophic Forgetting Occur?\\nCatastrophic forgetting usually arises during fine-tuning when an LLM is optimized for a single task, potentially erasing or degrading its prior capabilities. For example, fine-tuning a sentiment analysis task may lead the model to struggle with tasks it previously excelled at, like named entity recognition.\\nOptions to Address Catastrophic Forgetting: To mitigate or prevent catastrophic forgetting, consider these approaches:\\n1. Task-Specific Multitask Fine-Tuning: When aiming to preserve a model’s multitask capabilities and prevent catastrophic forgetting, you can opt for task-specific multitask fine-tuning. This approach involves fine-tuning multiple tasks simultaneously, ensuring the model maintains its versatility. However, it comes with a requirement for a substantial dataset containing examples across various tasks.\\ni)\\xa0Single Task Fine-Tuning Example: Imagine you have a base model and a specific task, such as sentiment analysis. Single-task fine-tuning involves optimizing the model exclusively for this task, resulting in improved performance in sentiment analysis. However, this process may lead to forgetting other tasks it previously excelled in.\\nii)Multitask Fine-Tuning Example\\xa0— FLAN-T5: FLAN-T5 serves as an excellent example of multitask fine-tuning. It’s a multitask fine-tuned version of the T5 foundation model. FLAN-T5 has been trained on a diverse range of datasets and tasks, encompassing 473 datasets across 146 task categories. This extensive multitask fine-tuning equips FLAN-T5 to excel in various tasks simultaneously, making it a versatile and capable model.\\n2. Parameter Efficient Fine-Tuning (PEFT): PEFT offers a more memory-efficient alternative to full fine-tuning. It preserves most of the original LLM’s weights while training only a small number of task-specific adapter layers and parameters. We’ll explore PEFT further in this blog series.\\nParameter Efficient Fine-Tuning (PEFT): Making LLMs More Efficient\\nTraining large language models (LLMs) is a computational behemoth. Full fine-tuning, where all model weights are updated during supervised learning, demands immense memory capacity, including storage for model weights, optimizer states, gradients, forward activations, and temporary memory throughout the training process. This memory load can swiftly surpass what’s feasible on consumer hardware.\\nIn contrast, parameter-efficient fine-tuning (PEFT) methods offer an elegant solution. PEFT strategically targets only a fraction of the model’s parameters for modification, significantly reducing memory requirements. Here’s why PEFT matters:\\n\\nFocused Parameter Updates: PEFT targets specific model parameters, reducing memory load.\\nMemory Efficiency: PEFT keeps most LLM weights frozen, using only a fraction of the original model’s parameters, making it suitable for limited hardware.\\nCatastrophic Forgetting Mitigation: PEFT minimizes the risk of catastrophic forgetting.\\nAdaptation to Multiple Tasks: PEFT efficiently adapts to various tasks without significant storage demands.\\n\\nPEFT Methods:\\n\\nSelective Methods: Fine-tune a subset of LLM parameters, offering a balance between parameter efficiency and computational cost.\\nReparameterization Methods: Reduce trainable parameters by creating new low-rank transformations of existing LLM weights, we can use LoRA, QLora methods\\nAdditive Methods: Keep original LLM weights frozen and introduce new trainable components, such as adapter layers or\\xa0soft prompt methods.\\n\\nStay tuned as we explore specific PEFT techniques like prompt tuning and LoRA to understand how they reduce memory requirements during LLM fine-tuning.\\nNow we’ll delve into specific PEFT techniques QLora, a deeper understanding of how these methods reduce memory requirements during LLM fine-tuning\\nLoRA (Low-rank Adaptation): Reducing Memory for LLM Fine-Tuning\\nLow-rank Adaptation, or LoRA, is a parameter-efficient fine-tuning technique categorized under re-parameterization methods. LoRA aims to drastically cut down the number of trainable parameters while fine-tuning large language models (LLMs). Here’s a closer look at how LoRA works:\\n\\nUnderstanding the Transformer Architecture: To appreciate LoRA, let’s revisit the fundamental architecture of a transformer model. The transformer architecture consists of an encoder and/or decoder part, each containing self-attention and feedforward networks. These components have weights that are initially learned during pre-training.\\nReducing Parameters with LoRA: LoRA employs a smart strategy — that freezes all the original model parameters and introduces a pair of rank decomposition matrices alongside the existing weights. The key steps in LoRA are as follows:\\n\\n\\nLow-Rank Matrices: LoRA introduces two low-rank matrices, Matrix A and Matrix B, alongside the original LLM weights.\\nMatrix Dimensions: The dimensions of these smaller matrices are carefully set so that their product results in a matrix of the same dimensions as the weights they’re modifying.\\nTraining Low-Rank Matrices: During fine-tuning, you keep the original LLM weights frozen while training Matrix A and Matrix B using supervised learning, a process you’re already familiar with.\\nInference: For inference, you multiply the two low-rank matrices together to create a matrix with the same dimensions as the frozen weights. You then add this new matrix to the original weights and replace them in the model.\\nSignificant Parameter Reduction: One of LoRA’s remarkable features is its ability to substantially reduce the number of trainable parameters. To put this into perspective, let’s consider an example based on the transformer architecture’s dimensions. A typical weights matrix in a transformer has 32,768 trainable parameters. If you apply LoRA with a rank of eight, you will train two small rank decomposition matrices. Matrix A with dimensions 8 by 64 results in 512 trainable parameters, while Matrix B with dimensions 512 by 8 amounts to 4,096 trainable parameters. In total, you’re training just 4,608 parameters, which is an 86% reduction compared to the original.\\n\\nDue to its parameter efficiency, LoRA can often be executed on a single GPU, eliminating the need for an extensive distributed GPU cluster. The memory required to store LoRA matrices is minimal, enabling fine-tuning for numerous tasks without the burden of storing multiple full-size LLM versions.\\nLoRA Performance: LoRA’s efficiency doesn’t come at the cost of performance. While the reduction in parameters might lead to slightly lower performance gains compared to full fine-tuning, LoRA still delivers impressive results, especially when compared to the base LLM model.\\nIn practice, LoRA is an invaluable tool for efficiently fine-tuning LLMs and adapting them to specific tasks without overwhelming computational and memory resources. It strikes a balance between parameter efficiency and performance, making it a go-to technique for many natural language processing applications.\\nBut wait, there’s a game-changer on the horizon — QLoRA.\\nWhat Is QLoRA?\\nQLoRA, which stands for Quantized Low-rank Adaptation, takes fine-tuning to the next level. It empowers you to fine-tune LLMs on a single GPU, pushing the boundaries of what’s possible. How does QLoRA differ from LoRA?\\nThe paper introduces QLoRA, an efficient fine-tuning method that enables the training of a 65-billion-parameter language model on a single 48GB GPU while maintaining good performance. QLoRA leverages 4-bit quantization and Low-Rank Adapters (LoRA) to achieve this. The authors’ best model family, named Guanaco, outperforms previously released models on the Vicuna benchmark, reaching 99.3% of ChatGPT’s performance with just 24 hours of fine-tuning on a single GPU.\\nKey innovations in QLoRA include the use of a 4-bit NormalFloat (NF4) data type for normally distributed weights, double quantization to reduce memory usage, and paged optimizers to manage memory spikes. They fine-tune over 1,000 models using QLoRA and analyze instruction following and chatbot performance across various datasets, model types (LLaMA, T5), and scales (33B and 65B parameters).\\nResults show that QLoRA fine-tuning with a small high-quality dataset achieves state-of-the-art results, even with smaller models compared to previous state-of-the-art models. The paper also discusses chatbot performance, highlighting that GPT-4 evaluations can be a cost-effective alternative to human evaluation. Additionally, the authors question the reliability of current chatbot benchmarks for evaluating chatbot performance and present a comparative analysis between Guanaco and ChatGPT.\\nThe authors have made their\\xa0models and code, including CUDA kernels for 4-bit training, available to the public.\\nThe QLoRA Advantage\\nQLoRA introduces innovative techniques that set it apart: Certainly, here’s a concise summary of QLORA’s key innovations:\\nQLora’s Memory-Efficient Innovations:\\n\\n4-bit NormalFloat: QLORA introduces a quantization data type optimized for normally distributed data, achieving efficient compression with minimal information loss.\\nDouble Quantization: This technique quantizes the quantization constants, saving an average of about 0.37 bits per parameter, leading to significant memory savings in large models.\\nPaged Optimizers: QLORA uses NVIDIA unified memory to tackle memory spikes during training, especially when processing long sequences, making it feasible to train large models without running into memory limitations.\\n\\nThese innovations collectively enable more efficient and memory-friendly training of large-scale language models, making QLORA a groundbreaking approach for AI research and development\\nGetting Hands-On with QLORA: Implementation and Fine-Tuning\\nEnough theory; it’s time to roll up our sleeves and dive into the exciting world of QLORA. In this hands-on session, we’ll walk through the steps to fine-tune a model using QLORA and save it in a quantized form. To achieve this, we’ll rely on two powerful libraries: Transformers and Bits & Bytes. These libraries are the cornerstone of implementing QLoRA, a remarkable evolution of the Low-Rank Adapter (LoRA) technique, supercharged with quantization.\\nBut before we embark on this coding adventure, let’s make sure we have all the essential libraries in place. So, fasten your seatbelts as we take the first steps towards unleashing the full potential of QLORA in your AI projects.”\\nLibrary Setup:\\nTo work with QLORA, we’ll use two essential libraries:\\n\\nTransformers: This library is crucial for handling pre-trained language models, including QLORA, and facilitates fine-tuning and deployment.\\nBits & Bytes: This is the core of QLORA’s functionality. It seamlessly integrates QLORA with Transformers, simplifying the process.\\n\\nAdditionally, we’ll harness the power of Hugging Face Accelerate, a library that optimizes training for large language models, ensuring faster and more efficient results.\\npython\\n!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\\n!pip install -q datasets bitsandbytes einops wandb\\n!pip install -Uqqq pip --progress-bar off\\n!pip install -qqq bitsandbytes==0.39.0 --progress-bar off\\n!pip install -qqq torch==2.0.1 --progress-bar off\\n!pip install -qqq -U git+https://github.com/huggingface/transformers.git@e03a9cc --progress-bar off\\n!pip install -qqq -U git+https://github.com/huggingface/peft.git@42a184f --progress-bar off\\n!pip install -qqq -U git+https://github.com/huggingface/accelerate.git@c9fbb71 --progress-bar off\\n!pip install -qqq datasets==2.12.0 --progress-bar off\\n!pip install -qqq loralib==0.1.1 --progress-bar off\\n!pip install -qqq einops==0.6.1 --progress-bar off\\npython\\nimport json\\nimport os\\nfrom pprint import pprint\\nimport bitsandbytes as bnb\\nimport pandas as pd\\nimport torch\\nimport torch.nn as nn\\nimport transformers\\nfrom datasets import load_dataset\\nfrom huggingface_hub import notebook_login\\nfrom peft import (\\n    LoraConfig,\\n    PeftConfig,\\n    PeftModel,\\n    get_peft_model,\\n    prepare_model_for_kbit_training,\\n)\\nfrom transformers import (\\n    AutoConfig,\\n    AutoModelForCausalLM,\\n    AutoTokenizer,\\n    BitsAndBytesConfig,\\n)\\nWe must include our Hugging Face token, ensure that it’s in write mode, and ultimately, use it to upload our model weights to the Hugging Face platform.\\npython\\nnotebook_login()\\nDownload the sample dataset of e-commerce faq & we will finetune using this dataset\\nconsole\\n!gdown 1tiAscG941evQS8RzjznoPu8meu4unw5A\\nCheck the few samples of data. we are using\\xa0pprint\\xa0library\\npython\\npprint(data[\"questions\"][0], sort_dicts=False)\\nbelow is a data format of our FAQ-based dataset.\\nThis structured dataset provides answers to common queries, ensuring a seamless shopping experience for our valued customers. & we are fine-tuning the model based on this data.\\n\\npython\\nwith open(\"dataset.json\", \"w\") as f:\\n    json.dump(data[\"questions\"], f)\\nModel selection\\nIn our current approach, we have implemented a sharded model\\xa0TinyPixel/Llama-2–7B-bf16-sharded\\xa0which involves dividing a large neural network model into multiple smaller pieces, typically more than 14 pieces in our case. This sharding strategy has proven to be highly beneficial when combined with the ‘accelerate’ framework\\nWhen a model is sharded, each shard represents a portion of the overall model’s parameters. Accelerate can then efficiently manage these shards by distributing them across various parts of the memory, including GPU memory and CPU memory. This dynamic allocation of shards allows us to work with very large models without requiring an excessive amount of memory\\npython\\nMODEL_NAME = \"TinyPixel/Llama-2-7B-bf16-sharded\"\\nNow, let’s explore the advanced usage of 4-bit quantization, a technique that can further optimize our model. Before diving in, let’s understand the key parameters and how to use them.\\nIn our code, we make use of the\\xa0BitsAndBytesConfig\\xa0from the \\'transformers\\' library and pass it as an argument to the\\xa0quantization_config\\xa0when calling\\xa0from_pretrained. Don\\'t forget to set\\xa0load_in_4bit = True\\xa0when using\\xa0BitsAndBytesConfig.\\nCompute Data Type: By default, the compute data type used during computation is\\xa0float32. However, you can change it to\\xa0bf16\\xa0(bfloat16) for faster processing. For example, you might want to use\\xa0bf16\\xa0it for computations while keeping hidden states in\\xa0float32.\\npython\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_compute_dtype=torch.bfloat16,  # Change this dtype for speedups.\\n)\\nThe 4-bit integration supports two quantization types:\\xa0FP4\\xa0and\\xa0NF4.\\xa0FP4 stands for Fixed Point 4, while NF4 stands for Normal Float 4. The latter is introduced in the QLoRA paper. You can switch between these two types using the\\xa0bnb_4bit_quant_type\\xa0parameter. By default, FP4 quantization is used.\\nFinally, we load our model and tokenizer with these configurations:\\n```python\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    MODEL_NAME,\\n    trust_remote_code=True,\\n    quantization_config=bnb_config,\\n)\\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\ntokenizer.pad_token = tokenizer.eos_token\\ndef print_trainable_parameters(model):\\n    \"\"\"\\n    Prints the number of trainable parameters in the model.\\n    \"\"\"\\n    trainable_params = 0\\n    all_param = 0\\n    for _, param in model.named_parameters():\\n        all_param += param.numel()\\n        if param.requires_grad:\\n            trainable_params += param.numel()\\n    print(\\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\\n    )\\n```\\nThis code snippet enables gradient checkpointing to reduce memory usage during training and then preparing the model for quantization\\npython\\nmodel.gradient_checkpointing_enable()\\nmodel = prepare_model_for_kbit_training(model)\\nLoraConfig\\xa0allows you to control how LoRA is applied to the base model through the following parameters:\\n```python\\nfrom peft import LoraConfig, get_peft_model\\nlora_alpha = 16\\nlora_dropout = 0.1\\nlora_r = 64\\nconfig = LoraConfig(\\n    lora_alpha=lora_alpha,\\n    lora_dropout=lora_dropout,\\n    r=lora_r,\\n    bias=\"none\",\\n    task_type=\"CAUSAL_LM\"\\n)\\nmodel = get_peft_model(model, config)\\nprint_trainable_parameters(model)\\n```\\nbelow are the hyperparameters we can choose; you can always do the experiments & select the best\\npython\\ngeneration_config = model.generation_config\\ngeneration_config.max_new_tokens = 80   # maxium no of token in output will get\\ngeneration_config.temperature = 0.7\\ngeneration_config.top_p = 0.7\\ngeneration_config.num_return_sequences = 1\\ngeneration_config.pad_token_id = tokenizer.eos_token_id\\ngeneration_config.eos_token_id = tokenizer.eos_token_id\\n```python\\n%%time\\nSpecify the target device for model execution, typically a GPU.\\ndevice = \"cuda:0\"\\nTokenize the input prompt and move it to the specified device.\\nencoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\\nRun model inference in evaluation mode (inference_mode) for efficiency.\\nwith torch.inference_mode():\\n    outputs = model.generate(\\n        input_ids=encoding.input_ids,\\n        attention_mask=encoding.attention_mask,\\n        generation_config=generation_config,\\n    )\\nDecode the generated output and print it, excluding special tokens.\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n```\\nNow we can prepare prompts for text-generation tasks\\n```python\\ndef generate_prompt(data_point):\\n    return f\"\"\"\\n: {data_point[\"question\"]}\\n: {data_point[\"answer\"]}\\n\"\"\".strip()\\ndef generate_and_tokenize_prompt(data_point):\\n    full_prompt = generate_prompt(data_point)\\n    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\\n    return tokenized_full_prompt\\ndata = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\\n```\\ncreate an output folder for saving all experiments\\npython\\nOUTPUT_DIR = \"experiments\"\\nwe are using TensorBoard for tracking our experiments.\\npython\\n%load_ext tensorboard\\n%tensorboard --logdir experiments/runs\\nBelow are several training parameters. To explore all of them, please refer to the\\xa0BitsAndBytesConfig\\n```python\\ntraining_args = transformers.TrainingArguments(\\n    per_device_train_batch_size=1,\\n    gradient_accumulation_steps=4,\\n    num_train_epochs=1,\\n    learning_rate=2e-4,\\n    fp16=True,\\n    save_total_limit=3,\\n    logging_steps=1,\\n    output_dir=OUTPUT_DIR,\\n    max_steps=80,\\n    optim=\"paged_adamw_8bit\",\\n    lr_scheduler_type=\"cosine\",\\n    warmup_ratio=0.05,\\n    report_to=\"tensorboard\",\\n)\\ntrainer = transformers.Trainer(\\n    model=model,\\n    train_dataset=data,\\n    args=training_args,\\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\\n)\\nmodel.config.use_cache = False\\ntrainer.train()\\n```\\n\\nTrained model for a few epochs. now save the trained model\\npython\\nmodel.save_pretrained(\"trained-lama_model\")\\nnow we are pushing our weights to Huggingface hub, so later we can use these weights for fine-tuning use your own directory name to push it into your HF repo.\\npython\\nmodel.push_to_hub(\\n    \"akashAD/Llama2-7b-qlora-chat-support-bot-faq\", use_auth_token=True\\n)\\nload the trained model\\n```python\\nPEFT_MODEL = \"akashAD/Llama2-7b-qlora-chat-support-bot-faq\"\\nconfig = PeftConfig.from_pretrained(PEFT_MODEL)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    config.base_model_name_or_path,\\n    return_dict=True,\\n    quantization_config=bnb_config,\\n    device_map=\"auto\",\\n    trust_remote_code=True,\\n)\\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\\ntokenizer.pad_token = tokenizer.eos_token\\nmodel = PeftModel.from_pretrained(model, PEFT_MODEL)\\n```\\nbelow are some experimental hyperparameters you can play with it to get the best results\\npython\\ngeneration_config = model.generation_config\\ngeneration_config.max_new_tokens = 50\\ngeneration_config.temperature = 0.3\\ngeneration_config.top_p = 0.7\\ngeneration_config.num_return_sequences = 1\\ngeneration_config.pad_token_id = tokenizer.eos_token_id\\ngeneration_config.eos_token_id = tokenizer.eos_token_id\\n```python\\n%%time\\nprompt = f\"\"\"\\n: How can I create an account?\\n:\\n\"\"\".strip()\\nencoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\\nwith torch.inference_mode():\\n    outputs = model.generate(\\n        input_ids=encoding.input_ids,\\n        attention_mask=encoding.attention_mask,\\n        generation_config=generation_config,\\n)\\n\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n```\\nThe below code utilizes our model to generate text responses from user questions, streamlining conversational AI interactions\\n```python\\ndef generate_response(question: str) -> str:\\n    prompt = f\"\"\"\\n: {question}\\n:\\n\"\"\".strip()\\n    encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\\n    with torch.inference_mode():\\n        outputs = model.generate(\\n            input_ids=encoding.input_ids,\\n            attention_mask=encoding.attention_mask,\\n            generation_config=generation_config,\\n        )\\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\\nassistant_start = \":\"\\nresponse_start = response.find(assistant_start)\\nreturn response[response_start + len(assistant_start) :].strip()\\n\\n```\\nnow let us ask questions to our model\\npython\\nprompt = \"Can I return a product if it was a clearance or final sale item?\"\\nprint(generate_response(prompt))\\npython\\nprompt = \"What happens when I return a clearance item?\"\\nprint(generate_response(prompt))\\nThat\\'s it; you can try to play with these hyperparameters to achieve better results.\\nFeel free to explore the power of LLMs on your own data with this\\xa0Colab notebook\\nSummary\\nThis blog explores instruction fine-tuning and mitigating catastrophic forgetting in large language models (LLMs). It covers how instruction fine-tuning improves LLMs for precision tasks but may face challenges with smaller models and resource constraints.\\nTo tackle catastrophic forgetting, strategies like task-specific multitask fine-tuning and parameter-efficient fine-tuning (PEFT) are discussed, with a focus on PEFT’s memory efficiency.\\nThe blog introduces LoRA (Low-rank Adaptation), a parameter-efficient fine-tuning technique, and QLoRA (Quantized Low-rank Adaptation), which enhances LoRA. It explains their benefits and differences.\\nA hands-on QLoRA implementation is demonstrated using Transformers and Bits & Bytes libraries, including model selection and training. Lastly, we covered saving and sharing trained models on the Hugging Face Hub and provided instructions for model loading and text generation tasks.\\nStay tuned for upcoming blogs as we delve deeper into the world of Large Language Models (LLMs). Your support is greatly appreciated — leave a like if you found our exploration enlightening!\\nFor a deeper dive into cutting-edge technology, explore the\\xa0vector-recipes\\xa0repository, brimming with real-world examples, use cases, and recipes to ignite your next project. We trust you found this journey informative and inspiring. Cheers!\\nTags\\nBlog\\nShare with friends\\n\\n\\n\\n\\nLink copied\\n\\n\\nPosted by Akash Desai\\nYou might also like\\n\\nLate interaction & efficient Multi-modal retrievers need more than a vector index ---------------------------------------------------------------------------------\\nBlog 10 min read\\n\\nImprove (almost) every retriever with LanceDB hybrid search and Reranking -------------------------------------------------------------------------\\nBlog 7 min read\\n\\nMy summer internship experience at LanceDB ------------------------------------------\\nBlog 6 min read\\n\\nDevelopers, Ditch the Black Box: Welcome to Continue ----------------------------------------------------\\nCase Study 3 min read\\n\\nBenchmarking Cohere Rerankers with LanceDB ------------------------------------------\\nBlog 6 min read\\n\\nTokens Per Second is NOT All You Need -------------------------------------\\nBlog 3 min read\\n\\nFeatured\\n\\nLance v2 is now in Beta -----------------------\\nEngineering 5 min read\\n\\nColumnar File Readers in Depth: Scheduling vs Decoding ------------------------------------------------------\\nEngineering 9 min read\\n\\nNew Funding and A New Foundation for Multimodal AI Data -------------------------------------------------------\\nNews 3 min read\\n\\nColumnar File Readers in Depth: Parallelism without Row Groups --------------------------------------------------------------\\nEngineering 7 min read\\n\\n\\nSign up\\nDocumentation\\nJoin Discord Community\\nYouTube\\n\\nLanceDB'),\n",
              "  Document(metadata={}, page_content='Efficient Fine-Tuning with LoRA for LLMs | Databricks Blog\\nSkip to main content\\n\\nLogin\\n\\n\\n\\nWhy Databricks\\n\\n\\n\\n\\nDiscover\\n\\n\\nFor Executives\\n\\n\\nFor Startups\\n\\n\\nLakehouse Architecture\\n\\n\\nMosaic Research\\n\\n\\n\\n\\nCustomers\\n\\n\\nFeatured Stories\\n\\n\\nSee All Customers\\n\\n\\n\\n\\nPartners\\n\\n\\nCloud Providers Databricks on AWS, Azure, and GCP\\n\\n\\nConsulting & System Integrators Experts to build, deploy and migrate to Databricks\\n\\n\\nTechnology Partners Connect your existing tools to your Lakehouse\\n\\n\\nC&SI Partner Program Build, deploy or migrate to the Lakehouse\\n\\n\\nData Partners Access the ecosystem of data consumers\\n\\n\\nPartner Solutions Find custom industry and migration solutions\\n\\n\\nBuilt on Databricks Build, market and grow your business\\n\\n\\n\\n\\n\\n\\n\\n\\nProduct\\n\\n\\n\\n\\nDatabricks Platform\\n\\n\\nPlatform Overview A unified platform for data, analytics and AI\\n\\n\\nData Management Data reliability, security and performance\\n\\n\\nSharing An open, secure, zero-copy sharing for all data\\n\\n\\nData Warehousing Serverless data warehouse for SQL analytics\\n\\n\\nGovernance Unified governance for all data, analytics and AI assets\\n\\n\\nReal-Time Analytics Real-time analytics, AI and applications made simple\\n\\n\\nArtificial Intelligence Build and deploy ML and GenAI applications\\n\\n\\nData Engineering ETL and orchestration for batch and streaming data\\n\\n\\nBusiness Intelligence Intelligent analytics for real-world data\\n\\n\\nData Science Collaborative data science at scale\\n\\n\\n\\n\\nIntegrations and Data\\n\\n\\nMarketplace Open marketplace for data, analytics and AI\\n\\n\\nIDE Integrations Build on the Lakehouse in your favorite IDE\\n\\n\\nPartner Connect Discover and integrate with the Databricks ecosystem\\n\\n\\n\\n\\nPricing\\n\\n\\nDatabricks Pricing Explore product pricing, DBUs and more\\n\\n\\nCost Calculator Estimate your compute costs on any cloud\\n\\n\\n\\n\\nOpen Source\\n\\nOpen Source Technologies Learn more about the innovations behind the platform\\n\\n\\n\\n\\n\\n\\n\\nSolutions\\n\\n\\n\\n\\nDatabricks for Industries\\n\\n\\nCommunications\\n\\n\\nMedia and Entertainment\\n\\n\\nFinancial Services\\n\\n\\nPublic Sector\\n\\n\\nHealthcare & Life Sciences\\n\\n\\nRetail\\n\\n\\nManufacturing\\n\\n\\nSee All Industries\\n\\n\\n\\n\\nCross Industry Solutions\\n\\n\\nCustomer Data Platform\\n\\n\\nCyber Security\\n\\n\\n\\n\\nMigration & Deployment\\n\\n\\nData Migration\\n\\n\\nProfessional Services\\n\\n\\n\\n\\nSolution Accelerators\\n\\nExplore Accelerators Move faster toward outcomes that matter\\n\\n\\n\\n\\n\\n\\n\\nResources\\n\\n\\n\\n\\nTraining and Certification\\n\\n\\nLearning Overview Hub for training, certification, events and more\\n\\n\\nTraining Overview Discover curriculum tailored to your needs\\n\\n\\nDatabricks Academy Sign in to the Databricks learning platform\\n\\n\\nCertification Gain recognition and differentiation\\n\\n\\nUniversity Alliance Want to teach Databricks? See how.\\n\\n\\n\\n\\nEvents\\n\\n\\nData + AI Summit\\n\\n\\nData + AI World Tour\\n\\n\\nData Intelligence Days\\n\\n\\nEvent Calendar\\n\\n\\n\\n\\nBlog and Podcasts\\n\\n\\nDatabricks Blog Explore news, product announcements, and more\\n\\n\\nDatabricks Mosaic Research Blog Discover the latest in our Gen AI research\\n\\n\\nData Brew Podcast Let’s talk data!\\n\\n\\nChampions of Data + AI Podcast Insights from data leaders powering innovation\\n\\n\\n\\n\\nGet Help\\n\\n\\nCustomer Support\\n\\n\\nDocumentation\\n\\n\\nCommunity\\n\\n\\n\\n\\nDive Deep\\n\\n\\nResource Center\\n\\n\\nDemo Center\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout\\n\\n\\n\\n\\nCompany\\n\\n\\nWho We Are\\n\\n\\nOur Team\\n\\n\\nDatabricks Ventures\\n\\n\\nContact Us\\n\\n\\n\\n\\nCareers\\n\\n\\nWorking at Databricks\\n\\n\\nOpen Jobs\\n\\n\\n\\n\\nPress\\n\\n\\nAwards and Recognition\\n\\n\\nNewsroom\\n\\n\\n\\n\\nSecurity and Trust\\n\\nSecurity and Trust\\n\\n\\n\\n\\n\\n\\n\\nReady to get started?\\n\\n\\nGet a Demo\\n\\n\\nLogin\\n\\nContact Us\\nTry Databricks\\n\\nCategories\\n\\nAll blog posts\\nCompany\\nCulture\\nCustomers\\nEvents\\nNews\\n\\n\\nPlatform\\nAnnouncements\\nPartners\\nProduct\\nSolutions\\nSecurity and Trust\\n\\n\\nGenerative AI\\nMosaic Research\\n\\n\\nEngineering\\nData Science and ML\\nOpen Source\\nSolutions Accelerators\\nData Engineering\\nTutorials\\nData Streaming\\nData Warehousing\\n\\n\\nData Strategy\\nBest Practices\\nData Leader\\nInsights\\n\\n\\nIndustries\\nEnergy\\nFinancial Services\\nHealthcare and Life Sciences\\nMedia and Entertainment\\nRetail and Consumer Goods\\nManufacturing\\nPublic Sector\\n\\n\\n\\n\\nEfficient Fine-Tuning with LoRA: A Guide to Optimal Parameter Selection for Large Language Models\\n\\nby Avinash Sooriyarachchi\\nAugust 30, 2023 in Engineering\\nShare this post\\n\\nWith the rapid advancement of neural network-based techniques and Large Language Model (LLM) research, businesses are increasingly interested in AI applications for value generation. They employ various machine learning approaches, both generative and non-generative, to address text-related challenges such as classification, summarization, sequence-to-sequence tasks, and controlled text generation. Organizations can opt for third-party APIs, but fine-tuning models with proprietary data offers domain-specific and pertinent results, enabling cost-effective and independent solutions deployable across different environments in a secure manner.\\nEnsuring efficient resource utilization and cost-effectiveness is crucial when choosing a strategy for fine-tuning. This blog explores arguably the most popular and effective variant of such parameter efficient methods, Low Rank Adaptation (LoRA), with a particular emphasis on QLoRA (an even more efficient variant of LoRA). The approach here will be to take an open large language model and fine-tune it to generate fictitious product descriptions when prompted with a product name and a category. The model chosen for this exercise is\\xa0OpenLLaMA-3b-v2, an open large language model with a permissive license (Apache 2.0), and the dataset chosen is\\xa0Red Dot Design Award Product Descriptions, both of which can be downloaded from the HuggingFace Hub at the links provided.\\nFine-Tuning, LoRA and QLoRA\\nIn the realm of language models, fine tuning an existing language model to perform a specific task on specific data is a common practice. This involves adding a task-specific head, if necessary, and updating the weights of the neural network through backpropagation during the training process. It is important to note the distinction between this finetuning process and training from scratch. In the latter scenario, the model\\'s weights are randomly initialized, while in finetuning, the weights are already optimized to a certain extent during the pre-training phase. The decision of which weights to optimize or update, and which ones to keep frozen, depends on the chosen technique.\\nFull finetuning involves optimizing or training all layers of the neural network. While this approach typically yields the best results, it is also the most resource-intensive and time-consuming.\\nFortunately, there exist parameter-efficient approaches for fine-tuning that have proven to be effective. Although most such approaches have yielded less performance, Low Rank Adaptation (LoRA) has bucked this trend by even outperforming full finetuning in some cases, as a consequence of avoiding catastrophic forgetting (a phenomenon which occurs when the knowledge of the pretrained model is lost during the fine-tuning process).\\nLoRA\\xa0is an improved finetuning method where instead of finetuning all the weights that constitute the weight matrix of the pre-trained large language model, two smaller matrices that approximate this larger matrix are fine-tuned. These matrices constitute the LoRA adapter. This fine-tuned adapter is then loaded to the pretrained model and used for inference.\\nQLoRA\\xa0is an even more memory efficient version of LoRA where the pretrained model is loaded to GPU memory as quantized 4-bit weights (compared to 8-bits in the case of LoRA), while preserving similar effectiveness to LoRA. Probing this method, comparing the two methods when necessary, and figuring out the best combination of QLoRA hyperparameters to achieve optimal performance with the quickest training time will be the focus here.\\nLoRA is implemented in the Hugging Face Parameter Efficient Fine-Tuning (PEFT) library, offering ease of use and QLoRA can be leveraged by using\\xa0bitsandbytes\\xa0and\\xa0PEFT\\xa0together. HuggingFace\\xa0Transformer Reinforcement Learning (TRL)\\xa0library offers a convenient trainer for supervised finetuning with seamless integration for LoRA. These three libraries will provide the necessary tools to finetune the chosen pretrained model to generate coherent and convincing product descriptions once prompted with an instruction indicating the desired attributes.\\nPrepping the data for supervised fine-tuning\\nTo probe the effectiveness of QLoRA for fine tuning a model for instruction following, it is essential to transform the data to a format suited for supervised fine-tuning. Supervised fine-tuning in essence, further trains a pretrained model to generate text conditioned on a provided prompt. It is supervised in that the model is finetuned on a dataset that has prompt-response pairs formatted in a consistent manner.\\nAn example observation from our chosen dataset from the Hugging Face hub looks as follows:\\nproductcategorydescriptiontext\"Biamp Rack Products\"\"Digital Audio Processors\"\"“High recognition value, uniform aesthetics and practical scalability – this has been impressively achieved with the Biamp brand language …\"\"Product Name: Biamp Rack Products; Product Category: Digital Audio Processors; Product Description: “High recognition value, uniform aesthetics and practical scalability – this has been impressively achieved with the Biamp brand language …\\xa0\\nAs useful as this dataset is, this is not well formatted for fine-tuning of a language model for instruction following in the manner described above.\\nThe following code snippet loads the dataset from the Hugging Face hub into memory, transforms the necessary fields into a consistently formatted string representing the prompt, and inserts the response( i.e. the description), immediately afterwards. This format is known as the ‘Alpaca format’ in large language model research circles as it was the format used to finetune the original LlaMA model from Meta to result in the Alpaca model, one of the first widely distributed instruction-following large language models (although not licensed for commercial use).\\nThe resulting prompts are then loaded into a hugging face dataset for supervised finetuning. Each such prompt has the following format.\\nTo facilitate quick experimentation, each fine-tuning exercise will be done on a 5000 observation subset of this data.\\nTesting model performance before\\xa0fine-tuning\\nBefore any fine-tuning, it’s a good idea to check how the model performs without any fine-tuning to get a baseline for pre-trained model performance.\\nThe model can be loaded in 8-bit as follows and prompted with the format specified in the\\xa0model card on Hugging Face.\\nThe output obtained is not quite what we want.\\nThe first part of the result is actually satisfactory, but the rest of it is more of a rambling mess.\\nSimilarly, if the model is prompted with the input text in the ‘Alpaca format’ as discussed before, the output is expected to be just as sub-optimal:\\nAnd sure enough, it is:\\nThe model performs what it was trained to do, predicts the next most probable token. The point of supervised fine-tuning in this context is to generate the desired text in a controllable manner. Please note that in the subsequent experiments, while QLoRA leverages a model loaded in 4-bit with the weights frozen, the inference process to examine output quality is done once the model has been loaded in 8-bit as shown above for consistency.\\nThe Turnable Knobs\\nWhen using PEFT to train a model with LoRA or QLoRA (note that, as mentioned before, the primary difference between the two is that in the latter, the pretrained models are frozen in 4-bit during the fine-tuning process), the hyperparameters of the low rank adaptation process can be defined in a LoRA config as shown below:\\nTwo of these hyperparameters, r and target_modules are empirically shown to affect adaptation quality significantly and will be the focus of the tests that follow. The other hyperparameters are kept constant at the values indicated above for simplicity.\\nr\\xa0represents the rank of the low rank matrices learned during the finetuning process. As this value is increased, the number of parameters needed to be updated during the low-rank adaptation increases. Intuitively, a lower\\xa0r\\xa0may lead to a quicker, less computationally intensive training process, but may affect the quality of the model thus produced. However, increasing r beyond a certain value may not yield any discernible increase in quality of model output. How the value of r affects adaptation (fine-tuning) quality will be put to the test shortly.\\nWhen fine-tuning with LoRA, it is possible to target specific modules in the model architecture. The adaptation process will target these modules and apply the update matrices to them.\\xa0Similar to the situation with \"r,\" targeting more modules during LoRA adaptation results in increased training time and greater demand for compute resources.\\xa0Thus, it is a common practice to only target the attention blocks of the transformer. However, recent work as shown in the\\xa0QLoRA paper\\xa0by Dettmers\\xa0et al.\\xa0suggests that targeting all linear layers results in better adaptation quality. This will be explored here as well.\\nNames of the linear layers of the model can be conveniently appended to a list with the following code snippet:\\nTuning the finetuning with LoRA\\nThe developer experience of fine tuning large language models in general have improved dramatically over the past year or so. The latest high level abstraction from Hugging Face is the SFTTrainer class in the TRL library. To perform QLoRA, all that is needed is the following:\\n1. \\xa0Load the model to GPU memory in 4-bit (bitsandbytes enables this process).\\n2. \\xa0Define the LoRA configuration as discussed above.\\n3. \\xa0Define the train and test splits of the prepped instruction following data into Hugging Face Dataset objects.\\n4. Define training arguments. These include the number of epochs, batch size and other training hyperparameters which will be kept constant during this exercise.\\n5. Pass these arguments into an instance of SFTTrainer.\\nThese steps are clearly indicated in the source file in the\\xa0repository\\xa0associated with this blog.\\nThe actual training logic is abstracted away nicely as follows:\\nIf MLFlow autologging is enabled in the Databricks workspace, which is highly recommended, all the training parameters and metrics are automatically tracked and logged with the MLFlow tracking server. This functionality is invaluable in monitoring long-running training tasks. Needless to say, the fine-tuning process is performed using a compute cluster (in this case, a single node with a single A100 GPU) created using the latest Databricks Machine runtime with GPU support.\\n\\nHyperparameter Combination #1: QLoRA with r=8 and targeting “q_proj”, “v_proj”\\nThe first combination of QLoRA hyperparameters attempted is\\xa0r=8\\xa0and targets only the attention blocks, namely\\xa0“q_proj”\\xa0and\\xa0“v_proj”\\xa0for adaptation.\\nThe following code snippets gives the number of trainable parameters:\\nThese choices result in 2,662,400 parameters being updated during the fine-tuning process (~2.6 million) from a total of ~3.2 billion parameters the model consists of. This is less than 0.1% of the model parameters. The entire finetuning process on a single Nvidia A100 with 80 GBs of GPU for 3 epochs only takes roughly 12 minutes. The GPU utilization metrics can be conveniently viewed at the metrics tab of the cluster configurations.\\n\\nAt the end of the training process, the fine-tuned model is obtained by loading the adapter weights to the pre-trained model as follows:\\nThis model can now be used for inference as any other model.\\nQualitative Evaluation\\nA couple of example prompt-response pairs are listed below\\nPrompt\\xa0(passed to the model in the Alpaca format, not shown for conciseness here):\\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\nResponse:\\nPrompt:\\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless Vacuum Cleaner\\nResponse:\\nThe model has clearly been adapted for generating more consistent descriptions. However the response to the first prompt about the optical mouse is quite short and the following phrase “The vacuum cleaner is equipped with a dust container that can be emptied via a dust container”\\xa0is logically flawed.\\nHyperparameter Combination #2: QLoRA with r=16 and targeting all linear layers\\nSurely, things can be improved here. It is worth exploring increasing the rank of low rank matrices learned during adaptation to 16, i.e. double the value of r to 16 and keep all else \\xa0the same. This doubles the number of trainable parameters to 5,324,800 (~5.3 million).\\nQualitative Evaluation\\nThe quality of output, however, remains unchanged for the same exact prompts.\\nPrompt:\\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\nResponse:\\nPrompt:\\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless Vacuum Cleaner\\nResponse:\\nThe same lack of detail and logical flaws in detail where details are available persists. If this fine tuned model is used for product description generation in a real-world scenario, this is not acceptable output.\\nHyperparameter Combination #3: QLoRA with r=8 and targeting all linear layers\\nGiven that doubling r does not seemingly result in any perceivable increase in output quality, it is worth changing the other important knob. i.e. targeting all linear layers instead of just the attention blocks. Here, the LoRA hyperparameters are\\xa0r=8\\xa0and target_layers are \\xa0\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'o_proj\\',\\'gate_proj\\',\\'down_proj\\',\\'up_proj\\'\\xa0and\\xa0\\'lm_head\\'. This increases the number of parameters updated to 12,994,560 and increases the training time to roughly 15.5 minutes.\\nQualitative Evaluation\\nPrompting the model with the same prompts yield the following:\\nPrompt:\\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\nResponse:\\nPrompt:\\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless Vacuum Cleaner\\nResponse:\\nNow it is possible to see a somewhat longer coherent description of the fictitious optical mouse and there are no logical flaws in the description of the vacuum cleaner. The product descriptions are not only logical, but relevant. Just as a reminder, these relatively high-quality results are obtained by fine-tuning less than a 1% of the model’s weights with a total dataset of 5000 such prompt-description pairs formatted in a consistent manner.\\nHyperparameter Combination #4: LoRA with \\xa0r=8 and targeting all linear transformer layers\\nIt\\xa0is also worth exploring whether the quality of output from the model improves if the pretrained model is\\xa0frozen in 8-bit instead of 4-bit. In other words, replicating the exact finetuning process using LoRA instead of QLoRA. Here, the LoRA hyperparameters are kept the same as before, in the new-found optimal configuration, i.e.\\xa0r=8\\xa0and targeting\\xa0all linear transformer layers\\xa0during the adaptation process.\\nQualitative Evaluation\\nThe results for the two prompts used throughout the article are as given below:\\nPrompt:\\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\nResponse:\\nPrompt:\\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless Vacuum Cleaner\\nResponse:\\nAgain, there isn’t much of an improvement in the quality of the output text.\\nKey Observations\\nBased on the above set of trials, and further evidence detailed in the excellent publication presenting QLoRA, it can be deduced that the value of r (the rank of matrices updated during adaptation) does not improve adaptation quality beyond a certain point. The biggest improvement is observed in targeting all linear layers in the adaptation process, as opposed to just the attention blocks, as commonly documented in technical literature detailing LoRA and QLoRA. The trials executed above and other empirical evidence suggest that QLoRA does not indeed suffer from any discernible reduction in quality of text generated, compared to LoRA.\\nFurther Considerations for using LoRA adapters in deployment\\nIt\\'s important to optimize the usage of adapters and understand the limitations of the technique. The size of the LoRA adapter obtained through finetuning is typically just a few megabytes, while the pretrained base model can be several gigabytes in memory and on disk. During inference, both the adapter and the pretrained LLM need to be loaded, so the memory requirement remains similar.\\nFurthermore, if the weights of the pre-trained LLM and the adapter aren’t merged, there will be a slight increase in inference latency. Fortunately, with the PEFT library, the process of merging the weights with the adapter can be done with a single line of code as shown here:\\nThe figure below outlines the process from fine-tuning an adapter to model deployment.\\n\\nWhile the adapter pattern offers significant benefits, merging adapters is not a universal solution. One advantage of the adapter pattern is the ability to deploy a single large pretrained model with task-specific adapters. This allows for efficient inference by utilizing the pretrained model as a backbone for different tasks. However, merging weights makes this approach impossible. The decision to merge weights depends on the specific use case and acceptable inference latency. Nonetheless, LoRA/ QLoRA continues to be a highly effective method for parameter efficient fine-tuning and is widely used.\\nConclusion\\nLow Rank Adaptation is a powerful fine-tuning technique that can yield great results if used with the right configuration. Choosing the correct value of rank and the layers of the neural network architecture to target during adaptation could decide the quality of the output from the fine-tuned model. QLoRA results in further memory savings while preserving the adaptation quality. Even when the fine-tuning is performed, \\xa0there are several important engineering considerations to ensure the adapted model is deployed in the correct manner.\\nIn summary, a concise table indicating the different combinations of LoRA parameters attempted, text quality output and number of parameters updated when fine-tuning OpenLLaMA-3b-v2 for 3 epochs on 5000 observations on a single A100 is shown below.\\nrtarget_modulesBase model weightsQuality of outputNumber of parameters updated (in millions)8Attention blocks4low2.66216Attention blocks4low5.3248All linear layers4high12.9958All linear layers8high12.995\\nTry this on Databricks! Clone the\\xa0GitHub repository\\xa0associated with the blog\\xa0into a Databricks\\xa0Repo\\xa0to get\\xa0started. More thoroughly documented examples to finetune models on Databricks are available\\xa0here.\\nTry Databricks for free\\nGet Started\\nRelated posts\\n\\nUsing MLflow AI Gateway and Llama 2 to Build Generative AI Apps\\nAugust 23, 2023 by Kasey Uhlenhuth, Xiangrui Meng, Hagay Lupesko, Sean Owen, Corey Zumar, Liang Zhang, Ina Koleva, Vladimir Kolovski and Arpit Jasapara in Data Science and ML\\nTo build customer support bots, internal knowledge graphs, or Q&A systems, customers often use Retrieval Augmented Generation (RAG) applications which leverage pre-trained models...\\n\\nDatabricks + MosaicML\\nJuly 19, 2023 by Matei Zaharia, Patrick Wendell, Reynold Xin and Ali Ghodsi in Company\\nToday, we’re excited to share that we’ve completed our acquisition of MosaicML, a leading platform for creating and customizing generative AI models for...\\nSee all Engineering posts\\n\\nWhy Databricks\\nDiscover\\n\\nFor Executives\\nFor Startups\\nLakehouse Architecture\\nMosaic Research\\n\\nCustomers\\n\\nFeatured\\nSee All\\n\\nPartners\\n\\nCloud Providers\\nTechnology Partners\\nData Partners\\nBuilt on Databricks\\nConsulting & System Integrators\\nC&SI Partner Program\\nPartner Solutions\\n\\nWhy Databricks\\nDiscover\\n\\nFor Executives\\nFor Startups\\nLakehouse Architecture\\nMosaic Research\\n\\nCustomers\\n\\nFeatured\\nSee All\\n\\nPartners\\n\\nCloud Providers\\nTechnology Partners\\nData Partners\\nBuilt on Databricks\\nConsulting & System Integrators\\nC&SI Partner Program\\nPartner Solutions\\n\\nProduct\\nDatabricks Platform\\n\\nPlatform Overview\\nSharing\\nGovernance\\nArtificial Intelligence\\nBusiness Intelligence\\nData Management\\nData Warehousing\\nReal-Time Analytics\\nData Engineering\\nData Science\\n\\nPricing\\n\\nPricing Overview\\nPricing Calculator\\n\\nOpen Source\\nIntegrations and Data\\n\\nMarketplace\\nIDE Integrations\\nPartner Connect\\n\\nProduct\\nDatabricks Platform\\n\\nPlatform Overview\\nSharing\\nGovernance\\nArtificial Intelligence\\nBusiness Intelligence\\nData Management\\nData Warehousing\\nReal-Time Analytics\\nData Engineering\\nData Science\\n\\nPricing\\n\\nPricing Overview\\nPricing Calculator\\n\\nOpen Source\\nIntegrations and Data\\n\\nMarketplace\\nIDE Integrations\\nPartner Connect\\n\\nSolutions\\nDatabricks For Industries\\n\\nCommunications\\nFinancial Services\\nHealthcare and Life Sciences\\nManufacturing\\nMedia and Entertainment\\nPublic Sector\\nRetail\\nView All\\n\\nCross Industry Solutions\\n\\nCustomer Data Platform\\nCyber Security\\n\\nData Migration\\nProfessional Services\\nSolution Accelerators\\nSolutions\\nDatabricks For Industries\\n\\nCommunications\\nFinancial Services\\nHealthcare and Life Sciences\\nManufacturing\\nMedia and Entertainment\\nPublic Sector\\nRetail\\nView All\\n\\nCross Industry Solutions\\n\\nCustomer Data Platform\\nCyber Security\\n\\nData Migration\\nProfessional Services\\nSolution Accelerators\\nResources\\nDocumentation\\nCustomer Support\\nCommunity\\nTraining and Certification\\n\\nLearning Overview\\nTraining Overview\\nCertification\\nUniversity Alliance\\nDatabricks Academy Login\\n\\nEvents\\n\\nData + AI Summit\\nData + AI World Tour\\nData Intelligence Days\\nFull Calendar\\n\\nBlog and Podcasts\\n\\nDatabricks Blog\\nDatabricks Mosaic Research Blog\\nData Brew Podcast\\nChampions of Data & AI Podcast\\n\\nResources\\nDocumentation\\nCustomer Support\\nCommunity\\nTraining and Certification\\n\\nLearning Overview\\nTraining Overview\\nCertification\\nUniversity Alliance\\nDatabricks Academy Login\\n\\nEvents\\n\\nData + AI Summit\\nData + AI World Tour\\nData Intelligence Days\\nFull Calendar\\n\\nBlog and Podcasts\\n\\nDatabricks Blog\\nDatabricks Mosaic Research Blog\\nData Brew Podcast\\nChampions of Data & AI Podcast\\n\\nAbout\\nCompany\\n\\nWho We Are\\nOur Team\\nDatabricks Ventures\\nContact Us\\n\\nCareers\\n\\nOpen Jobs\\nWorking at Databricks\\n\\nPress\\n\\nAwards and Recognition\\nNewsroom\\n\\nSecurity and Trust\\nAbout\\nCompany\\n\\nWho We Are\\nOur Team\\nDatabricks Ventures\\nContact Us\\n\\nCareers\\n\\nOpen Jobs\\nWorking at Databricks\\n\\nPress\\n\\nAwards and Recognition\\nNewsroom\\n\\nSecurity and Trust\\n\\nDatabricks Inc.\\n160 Spear Street, 15th Floor\\nSan Francisco, CA 94105\\n1-866-330-0121\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSee Careers at Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n© Databricks 2025. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the\\xa0Apache Software Foundation.\\n\\nPrivacy Notice\\n|Terms of Use\\n|Modern Slavery Statement\\n|California Privacy\\n|Your Privacy Choices\\n\\n'),\n",
              "  Document(metadata={}, page_content='In-depth guide to fine-tuning LLMs with LoRA and QLoRA\\n\\nServicesBlogContact\\n\\n\\nIn-depth guide to fine-tuning LLMs with LoRA and QLoRA\\nPranav Patel\\nPEFT Finetuning\\nWhat is PEFT Finetuning?\\nWhy use PEFT Finetuning?\\nLoRA Finetuning\\nHow LoRA works?\\nLoRA finetuning with HuggingFace\\nQLoRA Finetuning\\nHow does QLoRA work?\\nHow is QLoRA different from LoRA?\\nQLoRA finetuning with HuggingFace\\nQLoRA vs Standard FinetuningOther variants of LoRA finetuning\\nQA LoRA\\nLongLoRA\\nHow finetuning improves model performance for your business\\nLanguage Models like GPT-4 have become the de facto standard in the NLP industry for building products and applications. These models are capable of performing a plethora of tasks and can easily adapt to new tasks using Prompt Engineering Techniques. But these models also present a massive challenge around training. Massive models like GPT-4 cost millions of dollars to train, hence we use smaller models in production settings.\\n\\u200d\\nBut smaller models on the other hand cannot generalize to multiple tasks, and we end up having multiple models for multiple tasks of multiple users. This is where PEFT techniques like LoRA come in, these techniques allow you to train large models much more efficiently compared to fully finetuning them. In this blog, we will walk through LoRA, QLoRA, and other popular techniques that emerged specifically from LoRA.\\nWhat is PEFT Finetuning?\\nPEFT Finetuning is Parameter Efficient Fine Tuning, a set of fine-tuning techniques that allows you to fine-tune and train models much more efficiently than normal training. PEFT techniques usually work by reducing the number of trainable parameters in a neural network. The most famous and in-use PEFT techniques are Prefix Tuning, P-tuning, LoRA, etc. LoRA is perhaps the most used one. LoRA also has many variants like QLoRA and LongLoRA, which have their own applications.\\nWhy use PEFT Finetuning?\\nThere are many reasons to use PEFT techniques, they have become the go-to way to finetune LLMs and other models. But here are some reasons why even enterprises and large businesses like to use these approaches.\\nSaves Time\\nAs the number of trainable parameters decreases, you have to spend less time on training. But that’s only one part of it. With less trainable parameters, you can train models much faster, and as a result test models much faster too. With more time on your hands, you can spend it on testing different models, different datasets different techniques, and whatnot.\\n\\u200d\\nAlso, with more time you can train your models for much longer periods of time leading to a much lower loss, along with an increased batch size as PEFT techniques are heavily optimized for memory usage.\\nSaves Money\\nThis goes without saying but PEFT can save you a ton of money on computing costs. As mentioned, because of heavy memory optimizations, you don’t need to rent instances with high amounts of VRAM, as you can fit bigger batches in smaller VRAM. This saves money on the compute and allows you to train on much bigger datasets leveraging the advantage of fitting in larger batch sizes.\\n\\u200d\\nEasily build Multi-Tenancy architecture services\\nAs said, because LLMs have become so large and complicated to serve and handle, it’s almost impossible to train specialized models for users. If you have multiple users, you can either take on the complicated task of finetuning a new model every time a new user comes in, or you can just finetune the same model on the new data for the new user. Both approaches have their own set of issues, if you finetune a new model for every user, it will lead to better accuracy but then you have to handle massive models, load them into memory, store them, process them, etc, it’s an architecture hell and can cause big issues if you make a small mistake. Training the same model for all users is much easier, but then the model accuracy drops significantly.\\n\\u200d\\n\\n\\u200d\\nPEFT finetuning on the other hand takes the best of both worlds and lets you build small adapters that you can pair with models and get customized results. These adapters can be finetuned for specific datasets or specific users. These adapters are very small, 6MB-8MB, and you only need to apply these adapters to the large model, which is much faster to do in a production environment.\\n\\u200d\\n\\nLoRA Finetuning\\nLoRA is the most popular and perhaps the most used PEFT technique, but was released back in 2021 in this paper. LoRA is more of an adapter approach, where it introduces new parameters into the model to train the model through these new parameters. The trick is in how the new params are introduced and merged back into the model, without increasing the total number of params in the model.\\nHow LoRA works?\\nAs mentioned before, LoRA is an adapter-based approach, but new parameters are added only for the training step, they are not introduced as a part of the model. This keeps the model size completely the same and still offers the flexibility of parameter-efficient finetuning. Here’s a more detailed explanation of how it works.\\n\\u200d\\nLoRA works by breaking down the weight update matrix into smaller matrices and using them to train the model. Take a look at the diagram below, the ΔWAxB is the weight update matrix, the matrix of learned changes from backpropagation, this is the same size as the number of parameters we need to update to finetune our model. This matrix, or any matrix, can be represented as a set of smaller matrices, presented here as A and B with r as their rank. The r parameter controls the size of the smaller matrices.\\n\\u200d\\nThese smaller matrices can then be used to train the model using normal backpropagation but updating the parameters in the smaller matrices rather than updating directly in the model. We basically learn the ΔW through the smaller matrices. These smaller matrices can then be multiplied together to get back the original matrix. As these matrices are much smaller, this process uses fewer parameters and as a result much fewer computation resources. This also results in smaller checkpoints as you don’t have to store the whole model, but just the smaller matrices.\\n\\u200d\\n\\n\\u200d\\nLoRA finetuning with HuggingFace\\nTo implement LoRA finetuning with HuggingFace, you need to use the PEFT library to inject the LoRA adapters into the model and use them as the update matrices.\\n\\u200d\\n```py\\nfrom transformers import AutoModelForCausalLM\\nfrom peft import get_peft_config, get_peft_model, LoraConfig, TaskType\\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", trust_remote_code=True) # load the model\\npeft_config = LoraConfig(\\n    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=32, lora_alpha=16, lora_dropout=0.1,\\n    target_modules=[\\'query_key_value\\'] # optional, you can target specific layers using this\\n) # create LoRA config for the finetuning\\nmodel = get_peft_model(model, peft_config) # create a model ready for LoRA finetuning\\nmodel.print_trainable_parameters() \\ntrainable params: 9,437,184 || all params: 6,931,162,432 || trainable%: 0.13615586263611604\\n```\\n\\u200d\\nOnce this is done, you can train the model as you would normally do. But this time it will take much less time and compute resources as it normally does.\\nQLoRA Finetuning\\nQLoRA is a finetuning technique that combines a high-precision computing technique with a low-precision storage method. This helps keep the model size small while still making sure the model is still highly performant and accurate.\\nHow does QLoRA work?\\n\\nQLoRA works by introducing 3 new concepts that help to reduce memory while keeping the same quality performance. These are 4-bit Normal Float, Double Quantization, and Paged Optimizers. Let’s talk about these 3 very important concepts in detail.\\n4-Bit Normal Float\\n4-bit NormalFloat or NF is a new information-theoretically optimal data type built on top of Quantile Quantization techniques. 4-Bit NF works by estimating the 2k + 1 (k is the number of bits) quantiles in a 0 to 1 distribution, then normalizing its values into the [-1, 1] range. Once we have that, we can also normalize our neural network weights into the [-1, 1] range and then quantize into the quantiles we got from step 2.\\n\\u200d\\nHere’s an example of what quantile quantization looks like\\n\\u200d\\n\\nYou can see that there are “buckets” or “bins” of data where the data is quantized. Both the numbers 2 and 3 fall into the same quantile, 2. This quantization process allows you to use fewer numbers by “rounding off” to the nearest quantile.\\nDouble Dequantization\\nDouble quantization is the process of quantizing the quantization constants used during the quantization process in the 4-bit NF quantization. This is not important, but can save 0.5 bits per parameter on average, as mentioned in the paper. This helps with the process because QLoRA uses Block-wise k-bit Quantization, meaning instead of quantizing all the weights together, we create multiple chunks or blocks of weights which are then quantized independently.\\n\\u200d\\nThis block-wise method leads to multiple quantization constants being created, which then once again can be quantized to save additional space. This is okay because the number of quantization constants is low and hence not a lot of computing or storage is required.\\nError Reduction with LoRA Tuning\\nAs shown before, quantile quantization creates buckets or bins for a large range of numbers. This process leads to placing multiple different numbers into the same buckets, for example, two numbers 2 and 3 can be converted into 3 during the quantization process. This leads to an error of 1 being introduced during the dequantization of weights.\\n\\u200d\\nHere is a graphical representation of these errors on a larger distribution of weights of a neural network.\\n\\u200d\\n\\n\\u200d\\nThis error is why QLoRA is more of a finetuning mechanism than a standalone quantization strategy. Although it can be used for 4-bit inference. When fine-tuning with QLoRA we use the LoRA tuning mechanism of creating 2 smaller weight update matrices and then using them to update the weights of the neural network. Here, we keep the LoRA matrices in a higher precision format, like brain float 16 or float 16 and during the backpropagation and forward pass the weights of the network are also de-quantized. So the actual training is still happening in higher precision formats, but the storage is still in lower precision. This causes quantization errors to emerge, but the model training itself is able to compensate for these inefficiencies in the quantization process.\\n\\u200d\\nHence, the LoRA training with higher precision helps the model learn about and reduce the quantization errors.\\nHow is QLoRA different from LoRA?\\nQLoRA and LoRA both are finetuning techniques, but QLoRA uses LoRA as an accessory to fix the errors introduced during the quantization errors. LoRA in itself is more of a standalone finetuning technique.\\nQLoRA finetuning with HuggingFace\\nTo do QLoRA finetuning with HuggingFace, you need to install both the BitsandBytes library and the PEFT library. The BitsandBytes library takes care of the 4-bit quantization and the whole low-precision storage and high-precision compute part. The PEFT library will be used for the LoRA finetuning part.\\n\\u200d\\n```py\\nimport torch\\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\nmodel_id = \"EleutherAI/gpt-neox-20b\"\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_use_double_quant=True,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_compute_dtype=torch.bfloat16\\n) # setup bits and bytes config\\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\\nmodel.gradient_checkpointing_enable()\\nmodel = prepare_model_for_kbit_training(model) # prepares the whole model for kbit training\\nconfig = LoraConfig(\\n    r=8, \\n    lora_alpha=32, \\n    target_modules=[\"query_key_value\"], \\n    lora_dropout=0.05, \\n    bias=\"none\", \\n    task_type=\"CAUSAL_LM\"\\n)\\nmodel = get_peft_model(model, config) # Now you get a model ready for QLoRA training\\n```\\n\\u200d\\nAnd then once again you can move to normal training using the HF trainer. Check out this colab notebook as a guide for QLoRA training.\\nQLoRA vs Standard Finetuning\\nIn the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.\\n\\u200d\\n\\nAs you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don’t see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.\\n\\u200d\\nEven for the much bigger language models, performance remains the same:\\n\\u200d\\n\\nHence the authors trained a model on top of the base LLaMA models, Guanaco. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved 99.3% performance relative to ChatGPT. Even though other models have lesser models like Vicuna 13B, and Guanaco33B because of the use of 4-bit precision used less memory than the 13B model.\\n\\u200d\\nOther variants of LoRA finetuning\\nQA LoRA\\nQA LoRA is another fine-tuning technique built on top of QLoRA, introduced in this paper. QALoRA was mainly released for finetuning diffusion models, but can easily be generalized for training any type of models, just like LoRA.\\n\\u200d\\nThe difference between QLoRA and QALoRA is that QALoRA is quantization aware meaning the weights of the LoRA adapters are also quantized along with the weights of the model during the finetuning process. This helps in more efficient training as there is no need for the conversion step to update the models during the backpropagation process.\\n\\n\\u200d\\nLongLoRA\\n\\n\\u200d\\nLongLoRA is yet another variation of the LoRA finetuning technique, but this technique is specifically for training longer context models. LongLoRA works by using something called SHIFT SHORT ATTENTION. This method creates chunks or groups of the tokens, in which the attention is calculated independently. This method allows LongLoRA to scale to a much longer context because of the distributed workload.\\n\\u200d\\nAlong with this, the authors show the need to use the LoRA on the normalization and the embedding layers too for this method to work properly.\\n\\u200d\\nHow finetuning improves model performance for your business\\nIf you are considering finetuning a model for your business, you are probably correct. But data becomes an important part of the process, take a look at our synthetic data generation guide. However, there are many ways fine-tuned models can improve the performance of your business by providing customization. Finetuning the models helps you customize the model to your specific needs and knowledge. You can use an RAG pipeline to customize the model, but sometimes the knowledge is so vast that embedding and similarity searches are not enough, that’s where customization via finetuning comes in.\\n\\u200d\\nIf you need finetuned models for your business, reach out, don’t be shy. We have a ton of experience in finetuning all kinds of models, from small T5 models to massive models like Falcon180B. We have seen it all.\\n\\u200d\\nSubscribe to stay informed\\nSubscribe to our newsletter to stay updated on all things AI!\\nSubscribe\\nAwesome, you subscribed!\\nError! Please try again.\\n\\nLinkedInBlogResearchContact UsServices\\nTemplate pages\\nSubscribe to our newsletter!\\nSubscribe\\nAwesome, you subscribed!\\nError! Please try again.\\n© 2023, Mercity\\n')]}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEmGpcxo4T3a"
      },
      "source": [
        "### Test Web Search Route"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwqZoy1c4UJh",
        "outputId": "7f57a58e-20d2-479e-ce52-4a810b00b806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---ROUTE USER QUESTION---\n",
            "---ROUTING QUESTION TO WEB SEARCH WORKFLOW---\n",
            "---REWRITING USER QUERY FOR OPTIMAL RETRIEVAL FOR WEB SEARCH---\n",
            "---WEB SEARCH---\n",
            "---GENERATE ANSWER---\n",
            "---STANDARD RAG FLOW - GENERATING ANSWER---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---HALLUCINATION CHECK: SUCCESSFUL - GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---ASSESS HALLUCINATION GRADE---\n",
            "---DECISION: STOP AGENT---\n"
          ]
        }
      ],
      "source": [
        "query = \"Give a detailed explanation of the various design patterns for Agentic AI Systems\"\n",
        "response = adaptive_rag.invoke({\"orig_question\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "QEw4lR5c4W9h",
        "outputId": "a2cb3597-f23c-411b-9d6f-ae74cd414de3"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The context provided outlines four key design patterns for Agentic AI systems, which are designed to make AI models, particularly large language models (LLMs), more autonomous and capable of handling complex tasks. These patterns are:\n",
              "\n",
              "1. **Reflection Pattern**: This pattern enhances an AI's ability to self-evaluate and refine its outputs. It involves the AI reviewing its own work, identifying errors or areas for improvement, and iteratively refining its responses. This is particularly useful for tasks requiring precision, such as content creation or code generation. An example of this is the SELF-RAG framework, which integrates retrieval and self-reflection to improve the quality and factual accuracy of language models.\n",
              "\n",
              "2. **Tool Use Pattern**: This pattern allows AI systems to interact with external tools and resources, expanding their problem-solving capabilities beyond their internal knowledge. For instance, an AI might use web searches, databases, or programming languages like Python to gather information or perform complex computations. This pattern is powerful for tackling tasks that require external data or specialized computations.\n",
              "\n",
              "3. **Planning Pattern**: This pattern enables AI to break down complex tasks into smaller, manageable steps, creating a strategic roadmap for task completion. It involves outlining objectives, analyzing resources, and executing plans step-by-step. Variations like ReAct (Reasoning and Acting) and ReWOO (Reasoning With Open Ontology) incorporate decision-making and adaptive strategies, allowing AI to dynamically adjust plans based on new information.\n",
              "\n",
              "4. **Multi-Agent Pattern**: This pattern involves orchestrating multiple AI agents to work collaboratively on complex tasks. Each agent may specialize in different aspects of a task, and they communicate and coordinate to achieve a unified outcome. This pattern is akin to project management in human teams, where different team members handle specific roles but work towards a common goal.\n",
              "\n",
              "These design patterns collectively enhance the autonomy and efficiency of AI systems, enabling them to perform tasks, make decisions, and interact with other systems in a more human-like manner. They provide a foundation for building intelligent, autonomous AI systems capable of addressing real-world challenges."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response['generation']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYwr-U4L5Kkf"
      },
      "source": [
        "### Test Hallucination Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyhvIRob4eyO"
      },
      "source": [
        "Now we will simulate a hallucination occurence by uncommenting a part of the `generate_answer` node function and hard-code a wrong answer to the below question\n",
        "\n",
        "![](https://i.imgur.com/LkC8oOB.png)\n",
        "\n",
        "Remember to rerun the agent graph compilation code in 'Build the Agent Graph' section before running the below code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjcdWCxn5b5r",
        "outputId": "e99c7508-3ade-4b6e-d1e8-11bfcad9a161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---ROUTE USER QUESTION---\n",
            "---ROUTING QUESTION TO WEB SEARCH WORKFLOW---\n",
            "---REWRITING USER QUERY FOR OPTIMAL RETRIEVAL FOR WEB SEARCH---\n",
            "---WEB SEARCH---\n",
            "---GENERATE ANSWER---\n",
            "---STANDARD RAG FLOW - GENERATING ANSWER---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---HALLUCINATION CHECK: UNSUCCESSFUL - GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRYING RESPONSE GENERATION---\n",
            "---ASSESS HALLUCINATION GRADE---\n",
            "---DECISION: HALLUCINATION EXISTS RERUNNING generate_answer NODE with hallucination_feedback---\n",
            "---GENERATE ANSWER---\n",
            "Hallucination Check Feedback: The LLM generated response mentions the observer and factory pattern as the main design patterns, which is not supported by the context documents. The context documents focus on Agentic AI Design Patterns, specifically Reflection, Tool Use, Planning, and Multi-Agent Collaboration, and do not mention the observer or factory patterns.\n",
            "---REFLECTING ON HALLUCINATED FEEDBACK - GENERATING ANSWER---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---HALLUCINATION CHECK: SUCCESSFUL - GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---ASSESS HALLUCINATION GRADE---\n",
            "---DECISION: STOP AGENT---\n"
          ]
        }
      ],
      "source": [
        "query = \"Give a detailed explanation of the various design patterns for Agentic AI Systems\"\n",
        "response = adaptive_rag.invoke({\"orig_question\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "Nv0cSYOa6cE1",
        "outputId": "4116773b-bdda-47e4-d4ec-9d6e7b888f8f"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Agentic AI systems are designed to operate autonomously, mimicking human-like problem-solving and decision-making processes. These systems are structured using specific design patterns that enhance their autonomy and efficiency. The key Agentic AI design patterns include:\n",
              "\n",
              "1. **Reflection Pattern**: This pattern focuses on the AI's ability to self-evaluate and refine its outputs. The AI system reviews its actions or decisions, identifies errors or areas for improvement, and iteratively refines its approach. This is akin to a human reviewing their work to improve accuracy and reliability. For example, a trading bot might analyze its daily performance, identify underperforming strategies, and adjust its algorithms for better results.\n",
              "\n",
              "2. **Tool Use Pattern**: This pattern allows AI systems to extend their capabilities by interacting with external tools and resources. Instead of relying solely on internal computations, the AI can access databases, perform web searches, or execute complex functions using programming languages like Python. This pattern is powerful for tasks that require information or computations beyond the AI's pre-existing data, making the system more versatile and applicable to real-world scenarios.\n",
              "\n",
              "3. **Planning Pattern**: The Planning Pattern enables AI systems to break down complex tasks into smaller, manageable steps. The AI creates a roadmap of subtasks, determining the most efficient path to achieve a goal. This pattern incorporates strategic thinking, allowing the AI to dynamically adapt its workflows based on goals and constraints. Variations like ReAct (Reasoning and Acting) and ReWOO (Reasoning With Open Ontology) further enhance this pattern by integrating decision-making and contextual reasoning.\n",
              "\n",
              "4. **Multi-Agent Pattern**: This pattern involves orchestrating multiple AI agents to work collaboratively on complex tasks. Each agent is assigned specific roles or functions, and they communicate and collaborate to achieve a unified outcome. This pattern mirrors human teamwork, where different specialists contribute their expertise to solve a problem. It is particularly useful for large-scale or complex problems that require diverse skill sets.\n",
              "\n",
              "These design patterns collectively enable Agentic AI systems to operate more independently and effectively, pushing the boundaries of what AI can achieve by encouraging self-evaluation, tool integration, strategic planning, and collaboration. By mastering these patterns, developers can create more intelligent, autonomous AI systems capable of addressing real-world challenges."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response['generation']))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
