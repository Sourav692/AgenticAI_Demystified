{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Prompt Engineering with Open-Source Large Language Models (LLMs) using HuggingFace Serverless APIs</h1> </center>\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "\n",
    "_____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to run any open-source LLMs via HugginFace Inference APIs using this colab notebook. You can run this notebook in your local server also without worrying about having enough infrastructure to run these models!\n",
    "\n",
    "Thankfully HuggingFace has made its [__Inference API__](https://huggingface.co/docs/api-inference/quicktour) free to use with some basic rate limits etc. in place so you don't end up making unlimited requests on it's servers.\n",
    "\n",
    "The best part is you can access 150,000+ deep learning models without worrying about your infrastructure.\n",
    "\n",
    "The models we will be trying here include:\n",
    "\n",
    "- __[Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)__ model which is a 7B parameters transformer LLM built by the French young company [MistralAI](https://mistral.ai/company/)  is a instruct fine-tuned version of the [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) which is based on their first [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) generative text model using a variety of publicly available conversation datasets.\n",
    "\n",
    "- __[gemma-2b-it ](https://huggingface.co/google/gemma-1.1-2b-it)__ is a part of Google's gemma series, a 2 billion parameter transformer model fine-tuned for instruction-following tasks, enabling it to handle a wide array of complex language processing activities.\n",
    "\n",
    "\n",
    "\n",
    "__You just need an internet connection and a HuggingFace Account and API Key to use these models.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import requests\n",
    "import json\n",
    "\n",
    "API_KEY = getpass(\"Enter HuggingFace API Key: \")\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer \"+API_KEY}\n",
    "\n",
    "def query(payload, MODEL_API_URL):\n",
    "  response = requests.post(MODEL_API_URL, headers=headers, json=payload)\n",
    "  print('API Response:', response)\n",
    "  \n",
    "  # Check if the response is successful\n",
    "  if response.status_code == 200:\n",
    "    return response.json()\n",
    "  else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated to use Mistral-7B-Instruct-v0.3 (v0.2 is deprecated and not available)\n",
    "MISTRAL7B_API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "mistral_params = {\n",
    "                  \"wait_for_model\": True,\n",
    "                  \"do_sample\": False,\n",
    "                  \"return_full_text\": False,\n",
    "                  \"max_new_tokens\": 1000,\n",
    "                }\n",
    "\n",
    "# Alternative: If v0.3 doesn't work, try these alternatives:\n",
    "# Option 1: Use a smaller Mistral model\n",
    "MISTRAL7B_ALTERNATIVE_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Option 2: Use Microsoft's version\n",
    "MISTRAL7B_MICROSOFT_URL = \"https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium\"\n",
    "\n",
    "GEMMA2B_IT_API_URL = \"https://api-inference.huggingface.co/models/google/gemma-1.1-2b-it\"\n",
    "gemma_params = {\n",
    "                    \"wait_for_model\": True,\n",
    "                    \"do_sample\": False,\n",
    "                    \"return_full_text\": False,\n",
    "                    \"max_new_tokens\": 1000,\n",
    "                  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Basic Q&A with proper error handling\n",
    "prompt = \"\"\"Can you explain what is quantum computing to a 5th grader?\"\"\"\n",
    "print(prompt)\n",
    "\n",
    "# FIXED: Added proper error handling for the output\n",
    "output = query(payload={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": mistral_params\n",
    "                },\n",
    "                MODEL_API_URL=MISTRAL7B_API_URL)\n",
    "\n",
    "# Check if output is valid before accessing it\n",
    "if output and len(output) > 0 and 'generated_text' in output[0]:\n",
    "    print(\"✅ Mistral-7B-v0.3 Response:\")\n",
    "    print(output[0]['generated_text'])\n",
    "else:\n",
    "    print(\"❌ Mistral-7B-v0.3 model is not available. Trying alternative...\")\n",
    "    # Try alternative model\n",
    "    output = query(payload={\n",
    "                    \"inputs\": prompt,\n",
    "                    \"parameters\": mistral_params\n",
    "                    },\n",
    "                    MODEL_API_URL=MISTRAL7B_ALTERNATIVE_URL)\n",
    "    \n",
    "    if output and len(output) > 0 and 'generated_text' in output[0]:\n",
    "        print(\"✅ Using alternative Mistral-7B-v0.1 model:\")\n",
    "        print(output[0]['generated_text'])\n",
    "    else:\n",
    "        print(\"❌ All Mistral models are unavailable. Please try Gemma model below.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Gemma model with proper error handling\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing Gemma-2B model:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "output = query(payload={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": gemma_params\n",
    "                },\n",
    "                MODEL_API_URL=GEMMA2B_IT_API_URL)\n",
    "\n",
    "if output and len(output) > 0 and 'generated_text' in output[0]:\n",
    "    response = output[0]['generated_text']\n",
    "    print(\"✅ Gemma-2B Response:\")\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"❌ Gemma model is also not available. Please check your API key and internet connection.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUwrTGNs4kSN"
   },
   "source": [
    "<center> <h1> Prompt Engineering with Open-Source Large Language Models (LLMs) using HuggingFace Serverless APIs</h1> </center>\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGwBgKICV_DJ"
   },
   "source": [
    "\n",
    "In this notebook we will learn how to run any open-source LLMs via HugginFace Inference APIs using this colab notebook. You can run this notebook in your local server also without worrying about having enough infrastructure to run these models!\n",
    "\n",
    "Thankfully HuggingFace has made its [__Inference API__](https://huggingface.co/docs/api-inference/quicktour) free to use with some basic rate limits etc. in place so you don't end up making unlimited requests on it's servers.\n",
    "\n",
    "The best part is you can access 150,000+ deep learning models without worrying about your infrastructure.\n",
    "\n",
    "The models we will be trying here include:\n",
    "\n",
    "- __[Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)__ model which is a 7B parameters transformer LLM built by the French young company [MistralAI](https://mistral.ai/company/)  is a instruct fine-tuned version of the [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) which is based on their first [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) generative text model using a variety of publicly available conversation datasets.\n",
    "\n",
    "- __[gemma-2b-it ](https://huggingface.co/google/gemma-1.1-2b-it)__ is a part of Google's gemma series, a 2 billion parameter transformer model fine-tuned for instruction-following tasks, enabling it to handle a wide array of complex language processing activities.\n",
    "\n",
    "\n",
    "\n",
    "__You just need an internet connection and a HuggingFace Account and API Key to use these models.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "Successfully installed huggingface_hub-0.34.4\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w81pr0vzhNaJ"
   },
   "source": [
    "## Get your API Key\n",
    "\n",
    "Remember to go to your [HuggingFace Account Settings](https://huggingface.co/settings/account) and generate an API key by creating a new token from the [Access Tokens](https://huggingface.co/settings/tokens) section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGSuxLmAiCGP"
   },
   "source": [
    "## Load HuggingFace API Credentials\n",
    "\n",
    "Enter your key from [here](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_YjkI6sVG7H",
    "outputId": "0926504d-d4e0-4351-9ff9-8f5a47dc79e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter HuggingFace API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "API_KEY = getpass(\"Enter HuggingFace API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgZzsDCZiN3k"
   },
   "source": [
    "### Create LLM API Access Function\n",
    "\n",
    "Here we create a basic function which can access any LLM API endpoint available on HuggingFace.\n",
    "\n",
    "For more details refer to the [detailed documentation](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task) as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-TRno7hBB_qX"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer \"+API_KEY}\n",
    "\n",
    "def query(payload, MODEL_API_URL):\n",
    "  response = requests.post(MODEL_API_URL, headers=headers, json=payload)\n",
    "  print('API Response:', response)\n",
    "  return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BPGezswkYNZ"
   },
   "source": [
    "## Create LLM API Access Config\n",
    "\n",
    "Here we decide which LLMs we will access by getting their inference API endpoints.\n",
    "\n",
    "We also set some general configuration settings. You can find the [detailed documentation](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task) here.\n",
    "\n",
    "Some useful config settings include:\n",
    "\n",
    "- max_new_tokens: The amount of new tokens to be generated in the response\n",
    "- do_sample: Whether or not to use sampling. False means use greedy decoding i.e temperature=0\n",
    "- temperature: Between 0 - 1, The value used to module the next token probabilities. Higher temperature means the results may vary and be more creative\n",
    "- return_full_text: If set to False, does not return your input prompt to the model\n",
    "- wait_for_model:  If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done\n",
    "- repetition_penalty: The more a token is used within generation the more it is penalized to not be picked in successive generation passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WtpZr9DlXHPZ"
   },
   "outputs": [],
   "source": [
    "MISTRAL7B_API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "mistral_params = {\n",
    "                  \"wait_for_model\": True,\n",
    "                  \"do_sample\": False,\n",
    "                  \"return_full_text\": False,\n",
    "                  \"max_new_tokens\": 1000,\n",
    "                }\n",
    "\n",
    "GEMMA2B_IT_API_URL = \"https://api-inference.huggingface.co/models/google/gemma-1.1-2b-it\"\n",
    "gemma_params = {\n",
    "                    \"wait_for_model\": True,\n",
    "                    \"do_sample\": False,\n",
    "                    \"return_full_text\": False,\n",
    "                    \"max_new_tokens\": 1000,\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "repJ_4TCllJO"
   },
   "source": [
    "## Prompting with Open-Source LLM APIs\n",
    "\n",
    "Now we will use HugginFace LLM APIs and try some tasks with prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjD6KmrBluQE"
   },
   "source": [
    "### 1. Basic Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JIyiS1TtYfMi",
    "outputId": "7239137a-9783-4b41-db92-47be248df2af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you explain what is quantum computing to a 5th grader?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Can you explain what is quantum computing to a 5th grader?\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "fjo2tQmVjoKF",
    "outputId": "a761608d-5b9f-4925-eb6c-dba5791594b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MISTRAL7B_API_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Relativity is a theory proposed by Albert Einstein that describes the fundamental laws of physics, particularly how objects move and interact, in a way that is consistent with the principles of special and general relativity.\n",
      "\n",
      "1. Special Relativity: This theory, published in 1905, deals with objects moving at constant speeds, specifically those moving at or near the speed of light (approximately 299,792 kilometers per second). It introduces two key concepts:\n",
      "\n",
      "   a. Time Dilation: Time can appear to move slower for an object in motion compared to an object at rest. This means that a clock on a fast-moving spaceship would appear to run slower than a clock on Earth.\n",
      "\n",
      "   b. Length Contraction: Objects in motion can appear shorter in the direction of motion. This means that a spaceship moving at high speeds would appear shorter to an observer on Earth.\n",
      "\n",
      "2. General Relativity: Published in 1915, this theory extends special relativity to include gravity. It states that massive objects cause a distortion in space-time, which is felt as gravity. This means that a planet like Earth bends the space around it, and objects moving near it follow this bent path, which we perceive as gravity.\n",
      "\n",
      "In simple terms, relativity tells us that the laws of physics are the same for all observers, regardless of their motion or the gravity they are in. It also tells us that time and space are interconnected and can be warped by mass and energy. This has profound implications for our understanding of the universe, including the behavior of black holes and the expansion of the universe.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient()\n",
    "# Correct format: messages should be a list of dictionaries\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain relativity simply.\"}\n",
    "    ],\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9y8uhKXOB7B",
    "outputId": "82746635-a155-48aa-9b4f-a1e3f5bf03fc"
   },
   "outputs": [],
   "source": [
    "# output = query(payload={\n",
    "#                 \"inputs\": prompt,\n",
    "#                 \"parameters\": mistral_params\n",
    "#                 },\n",
    "#                 MODEL_API_URL=MISTRAL7B_API_URL)\n",
    "\n",
    "# print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3cJytUzYc4I",
    "outputId": "f5b5fcb7-9faf-4e25-85be-c2fd150f8776"
   },
   "outputs": [],
   "source": [
    "# # with Gemma\n",
    "# output = query(payload={\n",
    "#                 \"inputs\": prompt,\n",
    "#                 \"parameters\": gemma_params\n",
    "#                 },\n",
    "#                 MODEL_API_URL=GEMMA2B_IT_API_URL)\n",
    "# response = output[0]['generated_text']\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Relativity is a theory proposed by Albert Einstein that describes the fundamental laws of physics, particularly how objects move and interact, in a way that is consistent with the principles of special and general relativity.\n",
       "\n",
       "1. Special Relativity: This theory, published in 1905, deals with objects moving at constant speeds, specifically those moving at or near the speed of light (approximately 299,792 kilometers per second). It introduces two key concepts:\n",
       "\n",
       "   a. Time Dilation: Time can appear to move slower for an object in motion compared to an object at rest. This means that a clock on a fast-moving spaceship would appear to run slower than a clock on Earth.\n",
       "\n",
       "   b. Length Contraction: Objects in motion can appear shorter in the direction of motion. This means that a spaceship moving at high speeds would appear shorter to an observer on Earth.\n",
       "\n",
       "2. General Relativity: Published in 1915, this theory extends special relativity to include gravity. It states that massive objects cause a distortion in space-time, which is felt as gravity. This means that a planet like Earth bends the space around it, and objects moving near it follow this bent path, which we perceive as gravity.\n",
       "\n",
       "In simple terms, relativity tells us that the laws of physics are the same for all observers, regardless of their motion or the gravity they are in. It also tells us that time and space are interconnected and can be warped by mass and energy. This has profound implications for our understanding of the universe, including the behavior of black holes and the expansion of the universe."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lD2_UpD_l5CA"
   },
   "source": [
    "### 2. Report Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r88x6sQWCheK",
    "outputId": "7815782d-566f-4840-8f00-3babdfd96990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following report delimited by triple backticks on Generative AI in max 5 lines\n",
      "\n",
      "Report:\n",
      "```\n",
      "Generative AI is a type of artificial intelligence technology that can produce various types of content, including text, imagery, audio and synthetic data. The recent buzz around generative AI has been driven by the simplicity of new user interfaces for creating high-quality text, graphics and videos in a matter of seconds.\n",
      "The technology, it should be noted, is not brand-new. Generative AI was introduced in the 1960s in chatbots. But it was not until 2014, with the introduction of generative adversarial networks, or GANs -- a type of machine learning algorithm -- that generative AI could create convincingly authentic images, videos and audio of real people.\n",
      "On the one hand, this newfound capability has opened up opportunities that include better movie dubbing and rich educational content. It also unlocked concerns about deepfakes -- digitally forged images or videos -- and harmful cybersecurity attacks on businesses, including nefarious requests that realistically mimic an employee's boss.\n",
      "Two additional recent advances that will be discussed in more detail below have played a critical part in generative AI going mainstream: transformers and the breakthrough language models they enabled. Transformers are a type of machine learning that made it possible for researchers to train ever-larger models without having to label all of the data in advance. New models could thus be trained on billions of pages of text, resulting in answers with more depth. In addition, transformers unlocked a new notion called attention that enabled models to track the connections between words across pages, chapters and books rather than just in individual sentences. And not just words: Transformers could also use their ability to track connections to analyze code, proteins, chemicals and DNA.\n",
      "The rapid advances in so-called large language models (LLMs) -- i.e., models with billions or even trillions of parameters -- have opened a new era in which generative AI models can write engaging text, paint photorealistic images and even create somewhat entertaining sitcoms on the fly. Moreover, innovations in multimodal AI enable teams to generate content across multiple types of media, including text, graphics and video. This is the basis for tools like Dall-E that automatically create images from a text description or generate text captions from images.\n",
      "These breakthroughs notwithstanding, we are still in the early days of using generative AI to create readable text and photorealistic stylized graphics. Early implementations have had issues with accuracy and bias, as well as being prone to hallucinations and spitting back weird answers. Still, progress thus far indicates that the inherent capabilities of this generative AI could fundamentally change enterprise technology how businesses operate. Going forward, this technology could help write code, design new drugs, develop products, redesign business processes and transform supply chains.\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = \"\"\"\n",
    "Generative AI is a type of artificial intelligence technology that can produce various types of content, including text, imagery, audio and synthetic data. The recent buzz around generative AI has been driven by the simplicity of new user interfaces for creating high-quality text, graphics and videos in a matter of seconds.\n",
    "The technology, it should be noted, is not brand-new. Generative AI was introduced in the 1960s in chatbots. But it was not until 2014, with the introduction of generative adversarial networks, or GANs -- a type of machine learning algorithm -- that generative AI could create convincingly authentic images, videos and audio of real people.\n",
    "On the one hand, this newfound capability has opened up opportunities that include better movie dubbing and rich educational content. It also unlocked concerns about deepfakes -- digitally forged images or videos -- and harmful cybersecurity attacks on businesses, including nefarious requests that realistically mimic an employee's boss.\n",
    "Two additional recent advances that will be discussed in more detail below have played a critical part in generative AI going mainstream: transformers and the breakthrough language models they enabled. Transformers are a type of machine learning that made it possible for researchers to train ever-larger models without having to label all of the data in advance. New models could thus be trained on billions of pages of text, resulting in answers with more depth. In addition, transformers unlocked a new notion called attention that enabled models to track the connections between words across pages, chapters and books rather than just in individual sentences. And not just words: Transformers could also use their ability to track connections to analyze code, proteins, chemicals and DNA.\n",
    "The rapid advances in so-called large language models (LLMs) -- i.e., models with billions or even trillions of parameters -- have opened a new era in which generative AI models can write engaging text, paint photorealistic images and even create somewhat entertaining sitcoms on the fly. Moreover, innovations in multimodal AI enable teams to generate content across multiple types of media, including text, graphics and video. This is the basis for tools like Dall-E that automatically create images from a text description or generate text captions from images.\n",
    "These breakthroughs notwithstanding, we are still in the early days of using generative AI to create readable text and photorealistic stylized graphics. Early implementations have had issues with accuracy and bias, as well as being prone to hallucinations and spitting back weird answers. Still, progress thus far indicates that the inherent capabilities of this generative AI could fundamentally change enterprise technology how businesses operate. Going forward, this technology could help write code, design new drugs, develop products, redesign business processes and transform supply chains.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following report delimited by triple backticks on Generative AI in max 5 lines\n",
    "\n",
    "Report:\n",
    "```{report}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generative AI is a technology that creates various content types, including text, images, audio, and synthetic data. Introduced in the 1960s, it gained prominence in 2014 with the advent of Generative Adversarial Networks (GANs). Recent advancements like transformers and large language models have made it possible for AI to generate more detailed and accurate content, opening opportunities in areas like movie dubbing and education. However, concerns about deepfakes and cybersecurity attacks persist. These advancements have led to tools like Dall-E, which can generate images from text descriptions or vice versa. Despite early issues with accuracy, bias, and hallucinations, the potential for generative AI to revolutionize enterprise technology by writing code, designing drugs, developing products, and transforming supply chains is significant.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient()\n",
    "# Correct format: messages should be a list of dictionaries\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pi9uzJ8dObA",
    "outputId": "80750f8c-1278-4355-da17-5f6b8c1f7f70"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Generative AI is a technology that creates various content types, such as text, images, audio, and synthetic data. It gained recent prominence due to user-friendly interfaces producing high-quality content quickly. First introduced in chatbots in the 1960s, significant advancements came with the introduction of Generative Adversarial Networks (GANs) in 2014, enabling realistic images, videos, and audio.\n",
       "\n",
       "Recent advancements in transformers and large language models have made generative AI more powerful, allowing for the creation of engaging text, photorealistic images, and even multimedia content. However, early implementations still face issues with accuracy, bias, and hallucinations. Despite these challenges, generative AI has the potential to revolutionize enterprise technology by writing code, designing drugs, developing products, redesigning business processes, and transforming supply chains."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "client = InferenceClient()\n",
    "# Correct format: messages should be a list of dictionaries\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    ")\n",
    "\n",
    "response = response.choices[0].message.content\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9QqGVIQdTMD",
    "outputId": "de0ef3c0-a09c-457a-f805-ec666b80397e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: <Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Generative AI is a rapidly evolving field with the potential to revolutionize various industries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with Gemma\n",
    "output = query(payload={\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": gemma_params\n",
    "                },\n",
    "                MODEL_API_URL=GEMMA2B_IT_API_URL)\n",
    "response = output[0]['generated_text']\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJTyaa37lvz9"
   },
   "source": [
    "### 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4ZKsd_3HaOw_"
   },
   "outputs": [],
   "source": [
    "review = \"\"\"I recently worked with this real estate company to purchase my first home,\n",
    "    and the experience was outstanding. The agent was knowledgeable, patient, and incredibly responsive.\n",
    "    They guided me through every step of the process, making what could have been a stressful\n",
    "    experience very smooth and enjoyable.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zA6_Ayr7aR3R",
    "outputId": "3c35337b-fbe5-41ae-f989-532ff7410297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: <Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sentiment: Positive\n",
       "\n",
       "Key Topics or Phrases:\n",
       "1. Outstanding experience\n",
       "2. Knowledgeable agent\n",
       "3. Patient and incredibly responsive\n",
       "4. Smooth and enjoyable process\n",
       "5. Guided through every step."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Act as a customer review analyst, given the following customer review text,\n",
    "do the following tasks:\n",
    "- Find the sentiment (positive, negative or neutral)\n",
    "- Extract max 5 key topics or phrases of the good or bad in the review\n",
    "Review Text:\n",
    "{review}\n",
    "\"\"\"\n",
    "\n",
    "mistral_output = query(payload={\n",
    "              \"inputs\": prompt,\n",
    "              \"parameters\": mistral_params\n",
    "              },\n",
    "              MODEL_API_URL=MISTRAL7B_API_URL)\n",
    "\n",
    "response = mistral_output[0]['generated_text']\n",
    "display(Markdown(response))\n",
    "# print(mistral_output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0ID9ysnFKEz",
    "outputId": "dd2b6bab-5849-4d40-e1f4-b73f6dac57e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response: <Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "    The only downside was the high price of the property.\n",
       "\n",
       "Overall, I would rate my experience with this real estate company as 4 out of 5 stars.\n",
       "\n",
       "**Sentiment:**\n",
       "The sentiment of the review is positive. The customer is expressing satisfaction with the service provided by the real estate agent and the overall experience.\n",
       "\n",
       "**Key Topics:**\n",
       "1. Excellent agent knowledge and patience\n",
       "2. Smooth and enjoyable process\n",
       "3. High price of the property\n",
       "4. Responsive agent\n",
       "5. Overall positive experience"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with Gemma\n",
    "gemma_output = query(payload={\n",
    "              \"inputs\": prompt,\n",
    "              \"parameters\": gemma_params\n",
    "              },\n",
    "              MODEL_API_URL=GEMMA2B_IT_API_URL)\n",
    "\n",
    "response = gemma_output[0]['generated_text']\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yx-K95jwGcBD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
